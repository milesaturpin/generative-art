Springer Texts in Statistics
Series Editors:
G. Casella
S. Fienberg
I. Olkin

For other titles published in this series, go to
http://www.springer.com/series/417

Peter D. Hoff

A First Course in Bayesian
Statistical Methods

123

Peter D. Hoff
Department of Statistics
University of Washington
Seattle WA 98195-4322
USA
hoff@stat.washington.edu

ISSN 1431-875X
ISBN 978-0-387-92299-7
e-ISBN 978-0-387-92407-6
DOI 10.1007/978-0-387-92407-6
Springer Dordrecht Heidelberg London New York
Library of Congress Control Number: 2009929120
c Springer Science+Business Media, LLC 2009

All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York,
NY10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in
connection with any form of information storage and retrieval, electronic adaptation, computer software,
or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are
not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to
proprietary rights.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface

This book originated from a set of lecture notes for a one-quarter graduatelevel course taught at the University of Washington. The purpose of the course
is to familiarize the students with the basic concepts of Bayesian theory and
to quickly get them performing their own data analyses using Bayesian computational tools. The audience for this course includes non-statistics graduate
students who did well in their department‚Äôs graduate-level introductory statistics courses and who also have an interest in statistics. Additionally, first- and
second-year statistics graduate students have found this course to be a useful
introduction to statistical modeling. Like the course, this book is intended to
be a self-contained and compact introduction to the main concepts of Bayesian
theory and practice. By the end of the text, readers should have the ability to
understand and implement the basic tools of Bayesian statistical methods for
their own data analysis purposes. The text is not intended as a comprehensive handbook for advanced statistical researchers, although it is hoped that
this latter category of readers could use this book as a quick introduction to
Bayesian methods and as a preparation for more comprehensive and detailed
studies.
Computing
Monte Carlo summaries of posterior distributions play an important role in
the way data analyses are presented in this text. My experience has been
that once a student understands the basic idea of posterior sampling, their
data analyses quickly become more creative and meaningful, using relevant
posterior predictive distributions and interesting functions of parameters. The
open-source R statistical computing environment provides sufficient functionality to make Monte Carlo estimation very easy for a large number of statistical models, and example R-code is provided throughout the text. Much of
the example code can be run ‚Äúas is‚Äù in R, and essentially all of it can be run
after downloading the relevant datasets from the companion website for this
book.

VI

Preface

Acknowledgments
The presentation of material in this book, and my teaching style in general,
have been heavily influenced by the diverse set of students taking CSSS-STAT
564 at the University of Washington. My thanks to them for improving my
teaching. I also thank Chris Hoffman, Vladimir Minin, Xiaoyue Niu and Marc
Suchard for their extensive comments, suggestions and corrections for this
book, and to Adrian Raftery for bibliographic suggestions. Finally, I thank
my wife Jen for her patience and support.

Seattle, WA

Peter Hoff
March 2009

Contents

1

Introduction and examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Why Bayes? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.1 Estimating the probability of a rare event . . . . . . . . . . . . 3
1.2.2 Building a predictive model . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.3 Where we are going . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.4 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2

Belief, probability and exchangeability . . . . . . . . . . . . . . . . . . . . .
2.1 Belief functions and probabilities . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Events, partitions and Bayes‚Äô rule . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1 Discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.2 Continuous random variables . . . . . . . . . . . . . . . . . . . . . . .
2.4.3 Descriptions of distributions . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Joint distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6 Independent random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 Exchangeability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8 de Finetti‚Äôs theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . .

13
13
14
17
17
18
19
21
23
26
27
29
30

3

One-parameter models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1 The binomial model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Inference for exchangeable binary data . . . . . . . . . . . . . . .
3.1.2 Confidence regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 The Poisson model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Posterior inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 Example: Birth rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Exponential families and conjugate priors . . . . . . . . . . . . . . . . . . .
3.4 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . .

31
31
35
41
43
45
48
51
52

VIII

Contents

4

Monte Carlo approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1 The Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Posterior inference for arbitrary functions . . . . . . . . . . . . . . . . . . .
4.3 Sampling from predictive distributions . . . . . . . . . . . . . . . . . . . . .
4.4 Posterior predictive model checking . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . .

53
53
57
60
62
65

5

The normal model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1 The normal model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Inference for the mean, conditional on the variance . . . . . . . . . .
5.3 Joint inference for the mean and variance . . . . . . . . . . . . . . . . . . .
5.4 Bias, variance and mean squared error . . . . . . . . . . . . . . . . . . . . .
5.5 Prior specification based on expectations . . . . . . . . . . . . . . . . . . .
5.6 The normal model for non-normal data . . . . . . . . . . . . . . . . . . . . .
5.7 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . .

67
67
69
73
79
83
84
86

6

Posterior approximation with the Gibbs sampler . . . . . . . . . . 89
6.1 A semiconjugate prior distribution . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.2 Discrete approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.3 Sampling from the conditional distributions . . . . . . . . . . . . . . . . . 92
6.4 Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.5 General properties of the Gibbs sampler . . . . . . . . . . . . . . . . . . . . 96
6.6 Introduction to MCMC diagnostics . . . . . . . . . . . . . . . . . . . . . . . . 98
6.7 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 104

7

The multivariate normal model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7.1 The multivariate normal density . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7.2 A semiconjugate prior distribution for the mean . . . . . . . . . . . . . 107
7.3 The inverse-Wishart distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.4 Gibbs sampling of the mean and covariance . . . . . . . . . . . . . . . . . 112
7.5 Missing data and imputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.6 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 123

8

Group comparisons and hierarchical modeling . . . . . . . . . . . . . 125
8.1 Comparing two groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.2 Comparing multiple groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
8.2.1 Exchangeability and hierarchical models . . . . . . . . . . . . . . 131
8.3 The hierarchical normal model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
8.3.1 Posterior inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
8.4 Example: Math scores in U.S. public schools . . . . . . . . . . . . . . . . 135
8.4.1 Prior distributions and posterior approximation . . . . . . . 137
8.4.2 Posterior summaries and shrinkage . . . . . . . . . . . . . . . . . . 140
8.5 Hierarchical modeling of means and variances . . . . . . . . . . . . . . . 143
8.5.1 Analysis of math score data . . . . . . . . . . . . . . . . . . . . . . . . 145
8.6 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 146

Contents

9

IX

Linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
9.1 The linear regression model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
9.1.1 Least squares estimation for the oxygen uptake data . . . 153
9.2 Bayesian estimation for a regression model . . . . . . . . . . . . . . . . . . 154
9.2.1 A semiconjugate prior distribution . . . . . . . . . . . . . . . . . . . 154
9.2.2 Default and weakly informative prior distributions . . . . . 155
9.3 Model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
9.3.1 Bayesian model comparison . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.3.2 Gibbs sampling and model averaging . . . . . . . . . . . . . . . . . 167
9.4 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 170

10 Nonconjugate priors and Metropolis-Hastings algorithms . . 171
10.1 Generalized linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
10.2 The Metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
10.3 The Metropolis algorithm for Poisson regression . . . . . . . . . . . . . 179
10.4 Metropolis, Metropolis-Hastings and Gibbs . . . . . . . . . . . . . . . . . 181
10.4.1 The Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . 182
10.4.2 Why does the Metropolis-Hastings algorithm work? . . . . 184
10.5 Combining the Metropolis and Gibbs algorithms . . . . . . . . . . . . 187
10.5.1 A regression model with correlated errors . . . . . . . . . . . . . 188
10.5.2 Analysis of the ice core data . . . . . . . . . . . . . . . . . . . . . . . . 191
10.6 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 192
11 Linear and generalized linear mixed effects models . . . . . . . . . 195
11.1 A hierarchical regression model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
11.2 Full conditional distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
11.3 Posterior analysis of the math score data . . . . . . . . . . . . . . . . . . . 200
11.4 Generalized linear mixed effects models . . . . . . . . . . . . . . . . . . . . 201
11.4.1 A Metropolis-Gibbs algorithm for posterior
approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
11.4.2 Analysis of tumor location data . . . . . . . . . . . . . . . . . . . . . 203
11.5 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 207
12 Latent variable methods for ordinal data . . . . . . . . . . . . . . . . . . 209
12.1 Ordered probit regression and the rank likelihood . . . . . . . . . . . . 209
12.1.1 Probit regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
12.1.2 Transformation models and the rank likelihood . . . . . . . . 214
12.2 The Gaussian copula model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
12.2.1 Rank likelihood for copula estimation . . . . . . . . . . . . . . . . 218
12.3 Discussion and further references . . . . . . . . . . . . . . . . . . . . . . . . . . 223
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
Common distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267

1
Introduction and examples

1.1 Introduction
We often use probabilities informally to express our information and beliefs
about unknown quantities. However, the use of probabilities to express information can be made formal: In a precise mathematical sense, it can be shown
that probabilities can numerically represent a set of rational beliefs, that there
is a relationship between probability and information, and that Bayes‚Äô rule
provides a rational method for updating beliefs in light of new information.
The process of inductive learning via Bayes‚Äô rule is referred to as Bayesian
inference.
More generally, Bayesian methods are data analysis tools that are derived
from the principles of Bayesian inference. In addition to their formal interpretation as a means of induction, Bayesian methods provide:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

parameter estimates with good statistical properties;
parsimonious descriptions of observed data;
predictions for missing data and forecasts of future data;
a computational framework for model estimation, selection and validation.

Thus the uses of Bayesian methods go beyond the formal task of induction
for which the methods are derived. Throughout this book we will explore
the broad uses of Bayesian methods for a variety of inferential and statistical
tasks. We begin in this chapter with an introduction to the basic ingredients
of Bayesian learning, followed by some examples of the different ways in which
Bayesian methods are used in practice.
Bayesian learning
Statistical induction is the process of learning about the general characteristics
of a population from a subset of members of that population. Numerical values
of population characteristics are typically expressed in terms of a parameter Œ∏,
and numerical descriptions of the subset make up a dataset y. Before a dataset
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 1,
c Springer Science+Business Media, LLC 2009


2

1 Introduction and examples

is obtained, the numerical values of both the population characteristics and the
dataset are uncertain. After a dataset y is obtained, the information it contains
can be used to decrease our uncertainty about the population characteristics.
Quantifying this change in uncertainty is the purpose of Bayesian inference.
The sample space Y is the set of all possible datasets, from which a single
dataset y will result. The parameter space Œò is the set of possible parameter
values, from which we hope to identify the value that best represents the true
population characteristics. The idealized form of Bayesian learning begins with
a numerical formulation of joint beliefs about y and Œ∏, expressed in terms of
probability distributions over Y and Œò.
1. For each numerical value Œ∏ ‚àà Œò, our prior distribution p(Œ∏) describes our
belief that Œ∏ represents the true population characteristics.
2. For each Œ∏ ‚àà Œò and y ‚àà Y, our sampling model p(y|Œ∏) describes our belief
that y would be the outcome of our study if we knew Œ∏ to be true.
Once we obtain the data y, the last step is to update our beliefs about Œ∏:
3. For each numerical value of Œ∏ ‚àà Œò, our posterior distribution p(Œ∏|y) describes our belief that Œ∏ is the true value, having observed dataset y.
The posterior distribution is obtained from the prior distribution and sampling
model via Bayes‚Äô rule:
p(Œ∏|y) = R

p(y|Œ∏)p(Œ∏)
.
Àú Œ∏)
Àú dŒ∏Àú
p(y|
Œ∏)p(
Œò

It is important to note that Bayes‚Äô rule does not tell us what our beliefs should
be, it tells us how they should change after seeing new information.

1.2 Why Bayes?
Mathematical results of Cox (1946, 1961) and Savage (1954, 1972) prove that
if p(Œ∏) and p(y|Œ∏) represent a rational person‚Äôs beliefs, then Bayes‚Äô rule is an
optimal method of updating this person‚Äôs beliefs about Œ∏ given new information y. These results give a strong theoretical justification for the use of
Bayes‚Äô rule as a method of quantitative learning. However, in practical data
analysis situations it can be hard to precisely mathematically formulate what
our prior beliefs are, and so p(Œ∏) is often chosen in a somewhat ad hoc manner
or for reasons of computational convenience. What then is the justification of
Bayesian data analysis?
A famous quote about sampling models is that ‚Äúall models are wrong,
but some are useful‚Äù (Box and Draper, 1987, pg. 424). Similarly, p(Œ∏) might
be viewed as ‚Äúwrong‚Äù if it does not accurately represent our prior beliefs.
However, this does not mean that p(Œ∏|y) is not useful. If p(Œ∏) approximates our
beliefs, then the fact that p(Œ∏|y) is optimal under p(Œ∏) means that it will also

1.2 Why Bayes?

3

generally serve as a good approximation to what our posterior beliefs should
be. In other situations it may not be our beliefs that are of interest. Rather,
we may want to use Bayes‚Äô rule to explore how the data would update the
beliefs of a variety of people with differing prior opinions. Of particular interest
might be the posterior beliefs of someone with weak prior information. This
has motivated the use of ‚Äúdiffuse‚Äù prior distributions, which assign probability
more or less evenly over large regions of the parameter space.
Finally, in many complicated statistical problems there are no obvious
non-Bayesian methods of estimation or inference. In these situations, Bayes‚Äô
rule can be used to generate estimation procedures, and the performance of
these procedures can be evaluated using non-Bayesian criteria. In many cases
it has been shown that Bayesian or approximately Bayesian procedures work
very well, even for non-Bayesian purposes.
The next two examples are intended to show how Bayesian inference, using prior distributions that may only roughly represent our or someone else‚Äôs
prior beliefs, can be broadly useful for statistical inference. Most of the mathematical details of the calculations are left for later chapters.
1.2.1 Estimating the probability of a rare event
Suppose we are interested in the prevalence of an infectious disease in a small
city. The higher the prevalence, the more public health precautions we would
recommend be put into place. A small random sample of 20 individuals from
the city will be checked for infection.
Parameter and sample spaces
Interest is in Œ∏, the fraction of infected individuals in the city. Roughly speaking, the parameter space includes all numbers between zero and one. The
data y records the total number of people in the sample who are infected.
The parameter and sample spaces are then as follows:
Œò = [0, 1]

Y = {0, 1, . . . , 20} .

Sampling model
Before the sample is obtained the number of infected individuals in the sample
is unknown. We let the variable Y denote this to-be-determined value. If
the value of Œ∏ were known, a reasonable sampling model for Y would be a
binomial(20, Œ∏) probability distribution:
Y |Œ∏ ‚àº binomial(20, Œ∏) .
The first panel of Figure 1.1 plots the binomial(20, Œ∏) distribution for Œ∏ equal
to 0.05, 0.10 and 0.20. If, for example, the true infection rate is 0.05, then the
probability that there will be zero infected individuals in the sample (Y = 0)
is 36%. If the true rate is 0.10 or 0.20, then the probabilities that Y = 0 are
12% and 1%, respectively.

1 Introduction and examples

15

0.4

4

p(Œ∏
Œ∏)
p(Œ∏
Œ∏|y)
10
0

0.0

0.1

5

probability
0.2
0.3

Œ∏=0.05
Œ∏=0.10
Œ∏=0.20

0

5
10
15
number infected in the sample

20

0.0
0.2
0.4
0.6
0.8
1.0
percentage infected in the population

Fig. 1.1. Sampling model, prior and posterior distributions for the infection rate
example. The plot on the left-hand side gives binomial(20, Œ∏) distributions for three
values of Œ∏. The right-hand side gives prior (gray) and posterior (black) densities of
Œ∏.

Prior distribution
Other studies from various parts of the country indicate that the infection rate
in comparable cities ranges from about 0.05 to 0.20, with an average prevalence
of 0.10. This prior information suggests that we use a prior distribution p(Œ∏)
that assigns a substantial amount of probability to the interval (0.05, 0.20),
and that the expected value of Œ∏ under p(Œ∏) is close to 0.10. However, there are
infinitely many probability distributions that satisfy these conditions, and it
is not clear that we can discriminate among them with our limited amount of
prior information. We will therefore use a prior distribution p(Œ∏) that has the
characteristics described above, but whose particular mathematical form is
chosen for reasons of computational convenience. Specifically, we will encode
the prior information using a member of the family of beta distributions. A
beta distribution has two parameters which we denote as a and b. If Œ∏ has a
beta(a, b) distribution, then the expectation of Œ∏ is a/(a + b) and the most
probable value of Œ∏ is (a ‚àí 1)/(a ‚àí 1 + b ‚àí 1). For our problem where Œ∏ is
the infection rate, we will represent our prior information about Œ∏ with a
beta(2,20) probability distribution. Symbolically, we write
Œ∏ ‚àº beta(2, 20).
This distribution is shown in the gray line in the second panel of Figure 1.1.
The expected value of Œ∏ for this prior distribution is 0.09. The curve of the
prior distribution is highest at Œ∏ = 0.05 and about two-thirds of the area
under the curve occurs between 0.05 and 0.20. The prior probability that the
infection rate is below 0.10 is 64%.

1.2 Why Bayes?

5

E[Œ∏] = 0.09
mode[Œ∏] = 0.05
Pr(Œ∏ < 0.10) = 0.64
Pr(0.05 < Œ∏ < 0.20) = 0.66 .
Posterior distribution
As we will see in Chapter 3, if Y |Œ∏ ‚àº binomial(n, Œ∏) and Œ∏ ‚àº beta(a, b),
then if we observe a numeric value y of Y , the posterior distribution is a
beta(a + y, b + n ‚àí y) distribution. Suppose that for our study a value of Y = 0
is observed, i.e. none of the sample individuals are infected. The posterior
distribution of Œ∏ is then a beta(2, 40) distribution.
Œ∏|{Y = 0} ‚àº beta(2, 40)
The density of this distribution is given by the black line in the second panel
of Figure 1.1. This density is further to the left than the prior distribution,
and more peaked as well. It is to the left of p(Œ∏) because the observation
that Y = 0 provides evidence of a low value of Œ∏. It is more peaked than p(Œ∏)
because it combines information from the data and the prior distribution, and
thus contains more information than in p(Œ∏) alone. The peak of this curve is
at 0.025 and the posterior expectation of Œ∏ is 0.048. The posterior probability
that Œ∏ < 0.10 is 93%.
E[Œ∏|Y = 0] = 0.048
mode[Œ∏|Y = 0] = 0.025
Pr(Œ∏ < 0.10|Y = 0) = 0.93.
The posterior distribution p(Œ∏|Y = 0) provides us with a model for learning
about the city-wide infection rate Œ∏. From a theoretical perspective, a rational individual whose prior beliefs about Œ∏ were represented by a beta(2,20)
distribution now has beliefs that are represented by a beta(2,40) distribution.
As a practical matter, if we accept the beta(2,20) distribution as a reasonable
measure of prior information, then we accept the beta(2,40) distribution as a
reasonable measure of posterior information.
Sensitivity analysis
Suppose we are to discuss the results of the survey with a group of city health
officials. A discussion of the implications of our study among a diverse group of
people might benefit from a description of the posterior beliefs corresponding
to a variety of prior distributions. Suppose we were to consider beliefs represented by beta(a, b) distributions for values of (a, b) other than (2,20). As
mentioned above, if Œ∏ ‚àº beta(a, b), then given Y = y the posterior distribution
of Œ∏ is beta(a + y, b + n ‚àí y). The posterior expectation is

6

1 Introduction and examples

a+y
a+b+n
n
y
a+b
a
=
+
a+b+nn a+b+na+b
n
w
=
y¬Ø +
Œ∏0 ,
w+n
w+n

E[Œ∏|Y = y] =

6

0.4

0.24
0.22

0.5

0.2

0.4

0.5

where Œ∏0 = a/(a + b) is the prior expectation of Œ∏ and w = a + b. From
this formula we see that the posterior expectation is a weighted average of
the sample mean y¬Ø and the prior expectation Œ∏0 . In terms of estimating Œ∏,
Œ∏0 represents our prior guess at the true value of Œ∏ and w represents our
confidence in this guess, expressed on the same scale as the sample size. If

0.2

0.16

0.1

0.3

0.3

0.18

0.2

0.12
0.1

0.3

0.2

Œ∏0

0.14

0.5

0.7

0.1

0.06
0.04

0.1

0.08

0.9
0.975

0.0

0.0

0.02

5

10

15
w

20

25

5

10

15

20

25

w

Fig. 1.2. Posterior quantities under different beta prior distributions. The left- and
right-hand panels give contours of E[Œ∏|Y = 0] and Pr(Œ∏ < 0.10|Y = 0), respectively,
for a range of prior expectations and levels of confidence.

someone provides us with a prior guess Œ∏0 and a degree of confidence w, then
we can approximate their prior beliefs about Œ∏ with a beta distribution having
parameters a = wŒ∏0 and b = w(1‚àíŒ∏0 ). Their approximate posterior beliefs are
then represented with a beta(wŒ∏0 + y, w(1 ‚àí Œ∏0 ) + n ‚àí y) distribution. We can
compute such a posterior distribution for a wide range of Œ∏0 and w values to
perform a sensitivity analysis, an exploration of how posterior information is
affected by differences in prior opinion. Figure 1.2 explores the effects of Œ∏0 and
w on the posterior distribution via contour plots of two posterior quantities.
The first plot gives contours of the posterior expectation E[Œ∏|Y = 0], and
the second gives the posterior probabilities Pr(Œ∏ < 0.10|Y = 0). This latter
plot may be of use if, for instance, the city officials would like to recommend
a vaccine to the general public unless they were reasonably sure that the
current infection rate was less than 0.10. The plot indicates, for example, that

1.2 Why Bayes?

7

people with weak prior beliefs (low values of w) or low prior expectations are
generally 90% or more certain that the infection rate is below 0.10. However,
a high degree of certainty (say 97.5%) is only achieved by people who already
thought the infection rate was lower than the average of the other cities.
Comparison to non-Bayesian methods
A standard estimate of a population proportion Œ∏ is the sample mean y¬Ø =
y/n, the fraction of infected people in the sample. For our sample in which
y = 0 this of course gives an estimate of zero, and so by using y¬Ø we would be
estimating that zero people in the city are infected. If we were to report this
estimate to a group of doctors or health officials we would probably want to
include the caveat that this estimate is subject to sampling uncertainty. One
way to describe the sampling uncertainty of an estimate is with a confidence
interval. A popular 95% confidence interval for a population proportion Œ∏ is
the Wald interval , given by
p
y¬Ø ¬± 1.96 y¬Ø(1 ‚àí y¬Ø)/n.
This interval has correct asymptotic frequentist coverage, meaning that if n
is large, then with probability approximately equal to 95%, Y will take on
a value y such that the above interval contains Œ∏. Unfortunately this does
not hold for small n: For an n of around 20 the probability that the interval
contains Œ∏ is only about 80% (Agresti and Coull, 1998). Regardless, for our
sample in which y¬Ø = 0 the Wald confidence interval comes out to be just a
single point: zero. In fact, the 99.99% Wald interval also comes out to be zero.
Certainly we would not want to conclude from the survey that we are 99.99%
certain that no one in the city is infected.
People have suggested a variety of alternatives to the Wald interval in
hopes of avoiding this type of behavior. One type of confidence interval that
performs well by non-Bayesian criteria is the ‚Äúadjusted‚Äù Wald interval suggested by Agresti and Coull (1998), which is given by
q
ÀÜ ‚àí Œ∏)/n
ÀÜ
Œ∏ÀÜ ¬± 1.96 Œ∏(1
, where
n
4 1
Œ∏ÀÜ =
y¬Ø +
.
n+4
n+42
While not originally motivated as such, this interval is clearly related to
Bayesian inference: The value of Œ∏ÀÜ here is equivalent to the posterior mean for
Œ∏ under a beta(2,2) prior distribution, which represents weak prior information
centered around Œ∏ = 1/2.
General estimation of a population mean
Given a random sample of n observations from a population, a standard estimate of the population mean Œ∏ is the sample mean y¬Ø. While y¬Ø is generally

8

1 Introduction and examples

a reliable estimate for large sample sizes, as we saw in the example it can be
statistically unreliable for small n, in which case it serves more as a summary
of the sample data than as a precise estimate of Œ∏.
If our interest lies more in obtaining an estimate of Œ∏ than in summarizing
our sample data, we may want to consider estimators of the form
n
w
Œ∏ÀÜ =
y¬Ø +
Œ∏0 ,
n+w
n+w
where Œ∏0 represents a ‚Äúbest guess‚Äù at the true value of Œ∏ and w represents a
degree of confidence in the guess. If the sample size is large, then y¬Ø is a reliable
estimate of Œ∏. The estimator Œ∏ÀÜ takes advantage of this by having its weights
on y¬Ø and Œ∏0 go to one and zero, respectively, as n increases. As a result, the
statistical properties of y¬Ø and Œ∏ÀÜ are essentially the same for large n. However,
for small n the variability of y¬Ø might be more than our uncertainty about Œ∏0 .
In this case, using Œ∏ÀÜ allows us to combine the data with prior information to
stabilize our estimation of Œ∏.
These properties of Œ∏ÀÜ for both large and small n suggest that it is a useful
estimate of Œ∏ for a broad range of n. In Section 5.4 we will confirm this by
showing that, under some conditions, Œ∏ÀÜ outperforms y¬Ø as an estimator of Œ∏ for
all values of n. As we saw in the infection rate example and will see again in
later chapters, Œ∏ÀÜ can be interpreted as a Bayesian estimator using a certain
class of prior distributions. Even if a particular prior distribution p(Œ∏) does not
exactly reflect our prior information, the corresponding posterior distribution
p(Œ∏|y) can still be a useful means of providing stable inference and estimation
for situations in which the sample size is low.
1.2.2 Building a predictive model
In Chapter 9 we will discuss an example in which our task is to build a predictive model of diabetes progression as a function of 64 baseline explanatory
variables such as age, sex and body mass index. Here we give a brief synopsis of
that example. We will first estimate the parameters in a regression model using a ‚Äútraining‚Äù dataset consisting of measurements from 342 patients. We will
then evaluate the predictive performance of the estimated regression model
using a separate ‚Äútest‚Äù dataset of 100 patients.
Sampling model and parameter space
Letting Yi be the diabetes progression of subject i and xi = (xi,1 , . . . , xi,64 )
be the explanatory variables, we will consider linear regression models of the
form
Yi = Œ≤1 xi,1 + Œ≤2 xi,2 + ¬∑ ¬∑ ¬∑ + Œ≤64 xi,64 + œÉi .
The sixty-five unknown parameters in this model are the vector of regression
coefficients Œ≤ = (Œ≤1 , . . . , Œ≤64 ) as well as œÉ, the standard deviation of the error
term. The parameter space is 64-dimensional Euclidean space for Œ≤ and the
positive real line for œÉ.

1.2 Why Bayes?

9

Prior distribution
In most situations, defining a joint prior probability distribution for 65 parameters that accurately represents prior beliefs is a near-impossible task. As
an alternative, we will use a prior distribution that only represents some aspects of our prior beliefs. The main belief that we would like to represent is
that most of the 64 explanatory variables have little to no effect on diabetes
progression, i.e. most of the regression coefficients are zero. In Chapter 9 we
will discuss a prior distribution on Œ≤ that roughly represents this belief, in
that each regression coefficient has a 50% prior probability of being equal to
zero.
Posterior distribution

0.2

Pr(Œ≤
Œ≤j ‚â† 0|y,X)
0.4
0.6
0.8

1.0

Given data y = (y1 , . . . , y342 ) and X = (x1 , . . . , x342 ), the posterior distribution p(Œ≤|y, X) can be computed and used to obtain Pr(Œ≤j 6= 0|y, X) for each
regression coefficient j. These probabilities are plotted in the first panel of
Figure 1.3. Even though each of the sixty-four coefficients started out with a
50-50 chance of being non-zero in the prior distribution, there are only six Œ≤j ‚Äôs
for which Pr(Œ≤j 6= 0|y, X) ‚â• 0.5. The vast majority of the remaining coefficients have high posterior probabilities of being zero. This dramatic increase
in the expected number of zero coefficients is a result of the information in the
data, although it is the prior distribution that allows for such zero coefficients
in the first place.

0

10

20

30
40
regressor index

50

60

Fig. 1.3. Posterior probabilities that each coefficient is non-zero.

10

1 Introduction and examples

Predictive performance and comparison to non-Bayesian methods
We can evaluate how well this model performs by using it to predict the test
ÀÜ
data: Let Œ≤
Bayes = E[Œ≤|y, X] be the posterior expectation of Œ≤, and let Xtest
be the 100 √ó 64 matrix giving the data for the 100 patients in the test dataset.
We can compute a predicted value for each of the 100 observations in the test
ÀÜ
ÀÜ test = XŒ≤
set using the equation y
Bayes . These predicted values can then be
ÀÜ test appears
compared to the actual observations y test . A plot of y test versus y
ÀÜ
in the first panel of Figure 1.4, and indicates how well Œ≤
is
able
to predict
Bayes
diabetes progression from the baseline variables.
How does this Bayesian estimate of Œ≤ compare to a non-Bayesian approach? The most commonly used estimate of a vector of regression coefficients is the ordinary least squares (OLS) estimate, provided in most if not all
ÀÜ of
statistical software packages. The OLS regression estimate is the value Œ≤
ols
Œ≤ that minimizes the sum of squares of the residuals (SSR) for the observed
data,
n
X
SSR(Œ≤) =
(yi ‚àí Œ≤ T xi )2 ,
i=1
T
‚àí1 T
ÀÜ
and is given by the formula Œ≤
X y. Predictions for the test
ols = (X X)
ÀÜ and are plotted against the
data based on this estimate are given by XŒ≤
ols
ÀÜ
observed values in the second panel of Figure 1.4. Notice that using Œ≤
ols
gives a weaker relationship between observed and predicted values than using
ÀÜ
Œ≤
by computing the average squared
Bayes . This can be
Pquantified numerically
prediction error,
(ytest,i ‚àí yÀÜtest,i )2 /100, for both sets of predictions. The
prediction error for OLS is 0.67, about 50% higher than the value of 0.45 we
obtain using the Bayesian estimate. In this problem, even though our ad hoc
prior distribution for Œ≤ only captures the basic structure of our prior beliefs
(namely, that many of the coefficients are likely to be zero), this is enough to
provide a large improvement in predictive performance over the OLS estimate.
The poor performance of the OLS method is due to its inability to recognize when the sample size is too small to accurately estimate the regression
coefficients. In such situations, the linear relationship between the values of
ÀÜ , is often an inaccurate represeny and X in the dataset, quantified by Œ≤
ols
tation of the relationship in the entire population. The standard remedy to
this problem is to fit a ‚Äúsparse‚Äù regression model, in which some or many
of the regression coefficients are set to zero. One method of choosing which
coefficients to set to zero is the Bayesian approach described above. Another
popular method is the ‚Äúlasso,‚Äù introduced by Tibshirani (1996) and studied
ÀÜ
extensively by many others. The lasso estimate is the value Œ≤
lasso of Œ≤ that
minimizes SSR(Œ≤ : Œª), a modified version of the sum of squared residuals:

SSR(Œ≤ : Œª) =

n
X
i=1

(yi ‚àí xTi Œ≤)2 + Œª

p
X
j=1

|Œ≤j | .

1.3 Where we are going

11

2

2

‚óè

‚óè

‚óè

‚óè

0

1

‚óè‚óè
‚óè

1
y^test

‚óè
‚óè

2

y test

‚óè

‚óè

‚óè

0

0

‚àí1

‚óè

‚óè
‚óè
‚óè
‚óè

‚àí1

1
y^test

‚óè
‚óè‚óè ‚óè‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè

‚àí1

‚óè

‚óè

‚óè

‚óè
‚óè ‚óè

‚óè

‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè

‚àí1

0

1

‚óè‚óè
‚óè

‚óè
‚óè‚óè

‚óè

‚óè

‚óè

2

y test

Fig. 1.4. Observed versus predicted diabetes progression values using the Bayes
estimate (left panel) and the OLS estimate (right panel).

In other words, the lasso procedure penalizes large values of |Œ≤j |. Depending
ÀÜ
on the size of Œª, this penalty can make some elements of Œ≤
lasso equal to
zero. Although the lasso procedure has been motivated by and studied in
a non-Bayesian context, in fact it corresponds to a Bayesian estimate using
a particular prior distribution: The lasso estimate is equal to the posterior
mode of Œ≤ in which the prior distribution for each Œ≤j is a double-exponential
distribution, a probability distribution that has a sharp peak at Œ≤j = 0.

1.3 Where we are going
As the above examples indicate, the uses of Bayesian methods are quite broad.
We have seen how the Bayesian approach provides
‚Ä¢ models for rational, quantitative learning;
‚Ä¢ estimators that work for small and large sample sizes;
‚Ä¢ methods for generating statistical procedures in complicated problems.
An understanding of the benefits and limits of Bayesian methods comes with
experience. In the chapters that follow, we will become familiar with these
methods by applying them to a large number of statistical models and data
analysis examples. After a review of probability in Chapter 2, we will learn
the basics of Bayesian data analysis and computation in the context of some
simple one-parameter statistical models in Chapters 3 and 4. Chapters 5, 6
and 7 discuss Bayesian inference with the normal and multivariate normal
models. While important in their own right, normal models also provide the

12

1 Introduction and examples

building blocks of more complicated modern statistical methods, such as hierarchical modeling, regression, variable selection and mixed effects models.
These advanced topics and others are covered in Chapters 8 through 12.

1.4 Discussion and further references
The idea of probability as a measure of uncertainty about unknown but deterministic quantities is an old one. Important historical works include Bayes‚Äô
‚ÄúAn essay towards solving a Problem in the Doctrine of Chances‚Äù (Bayes,
1763) and Laplace‚Äôs ‚ÄúA Philosophical Essay on Probabilities,‚Äù published in
1814 and currently published by Dover (Laplace, 1995).
The role of prior opinion in statistical inference was debated for much of
the 20th century. Most published articles on this debate take up one side or another, and include mischaracterizations of the other side. More informative are
discussions among statisticians of different viewpoints: Savage (1962) includes
a short introduction by Savage, followed by a discussion among Bartlett,
Barnard, Cox, Pearson and Smith, among others. Little (2006) considers the
strengths and weaknesses of Bayesian and frequentist statistical criteria. Efron
(2005) briefly discusses the role of different statistical philosophies in the last
two centuries, and speculates on the interplay between Bayesian and nonBayesian methods in the future of statistical science.

2
Belief, probability and exchangeability

We first discuss what properties a reasonable belief function should have, and
show that probabilities have these properties. Then, we review the basic machinery of discrete and continuous random variables and probability distributions. Finally, we explore the link between independence and exchangeability.

2.1 Belief functions and probabilities
At the beginning of the last chapter we claimed that probabilities are a way
to numerically express rational beliefs. We do not prove this claim here (see
Chapter 2 of Jaynes (2003) or Chapters 2 and 3 of Savage (1972) for details),
but we do show that several properties we would want our numerical beliefs
to have are also properties of probabilities.
Belief functions
Let F , G, and H be three possibly overlapping statements about the world.
For example:
F = { a person votes for a left-of-center candidate }
G = { a person‚Äôs income is in the lowest 10% of the population }
H = { a person lives in a large city }
Let Be() be a belief function, that is, a function that assigns numbers to
statements such that the larger the number, the higher the degree of belief.
Some philosophers have tried to make this more concrete by relating beliefs
to preferences over bets:
‚Ä¢ Be(F ) > Be(G) means we would prefer to bet F is true than G is true.
We also want Be() to describe our beliefs under certain conditions:
‚Ä¢ Be(F |H) > Be(G|H) means that if we knew that H were true, then we
would prefer to bet that F is also true than bet G is also true.
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 2,
c Springer Science+Business Media, LLC 2009


14

2 Belief, probability and exchangeability

‚Ä¢ Be(F |G) > Be(F |H) means that if we were forced to bet on F , we would
prefer to do it under the condition that G is true rather than H is true.
Axioms of beliefs
It has been argued by many that any function that is to numerically represent
our beliefs should have the following properties:
B1 Be(not H|H) ‚â§ Be(F |H) ‚â§ Be(H|H)
B2 Be(F or G|H) ‚â• max{Be(F |H), Be(G|H)}
B3 Be(F and G|H) can be derived from Be(G|H) and Be(F |G and H)
How should we interpret these properties? Are they reasonable?
B1 says that the number we assign to Be(F |H), our conditional belief in F
given H, is bounded below and above by the numbers we assign to complete
disbelief (Be(not H|H)) and complete belief (Be(H|H)).
B2 says that our belief that the truth lies in a given set of possibilities should
not decrease as we add to the set of possibilities.
B3 is a bit trickier. To see why it makes sense, imagine you have to decide
whether or not F and G are true, knowing that H is true. You could do this
by first deciding whether or not G is true given H, and if so, then deciding
whether or not F is true given G and H.
Axioms of probability
Now let‚Äôs compare B1, B2 and B3 to the standard axioms of probability.
Recall that F ‚à™ G means ‚ÄúF or G,‚Äù F ‚à© G means ‚ÄúF and G‚Äù and ‚àÖ is the
empty set.
P1 0 = Pr(not H|H) ‚â§ Pr(F |H) ‚â§ Pr(H|H) = 1
P2 Pr(F ‚à™ G|H) = Pr(F |H) + Pr(G|H) if F ‚à© G = ‚àÖ
P3 Pr(F ‚à© G|H) = Pr(G|H) Pr(F |G ‚à© H)
You should convince yourself that a probability function, satisfying P1, P2
and P3, also satisfies B1, B2 and B3. Therefore if we use a probability
function to describe our beliefs, we have satisfied the axioms of belief.

2.2 Events, partitions and Bayes‚Äô rule
Definition 1 (Partition) A collection of sets {H1 , . . . , HK } is a partition
of another set H if
1. the events are disjoint, which we write as Hi ‚à© Hj = ‚àÖ for i 6= j;
2. the union of the sets is H, which we write as ‚à™K
k=1 Hk = H.
In the context of identifying which of several statements is true, if H is the
set of all possible truths and {H1 , . . . , HK } is a partition of H, then exactly
one out of {H1 , . . . , HK } contains the truth.

2.2 Events, partitions and Bayes‚Äô rule

15

Examples
‚Ä¢ Let H be someone‚Äôs religious orientation. Partitions include
‚Äì {Protestant, Catholic, Jewish, other, none};
‚Äì {Christian, non-Christian};
‚Äì {atheist, monotheist, multitheist}.
‚Ä¢ Let H be someone‚Äôs number of children. Partitions include
‚Äì {0, 1, 2, 3 or more};
‚Äì {0, 1, 2, 3, 4, 5, 6, . . . }.
‚Ä¢ Let H be the relationship between smoking and hypertension in a given
population. Partitions include
‚Äì {some relationship, no relationship};
‚Äì {negative correlation, zero correlation, positive correlation}.
Partitions and probability
Suppose {H1 , . . . , HK } is a partition of H, Pr(H) = 1, and E is some specific
event. The axioms of probability imply the following:
Rule of total probability :

K
X

Pr(Hk ) = 1

k=1

Rule of marginal probability :

Pr(E) =

K
X

Pr(E ‚à© Hk )

k=1

=

K
X

Pr(E|Hk ) Pr(Hk )

k=1

Bayes‚Äô rule :

Pr(E|Hj ) Pr(Hj )
Pr(E)
Pr(E|Hj ) Pr(Hj )
= PK
k=1 Pr(E|Hk ) Pr(Hk )

Pr(Hj |E) =

Example
A subset of the 1996 General Social Survey includes data on the education level
and income for a sample of males over 30 years of age. Let {H1 , H2 , H3 , H4 } be
the events that a randomly selected person in this sample is in, respectively,
the lower 25th percentile, the second 25th percentile, the third 25th percentile
and the upper 25th percentile in terms of income. By definition,
{Pr(H1 ), Pr(H2 ), Pr(H3 ), Pr(H4 )} = {.25, .25, .25, .25}.

16

2 Belief, probability and exchangeability

Note that {H1 , H2 , H3 , H4 } is a partition and so these probabilities sum to 1.
Let E be the event that a randomly sampled person from the survey has a
college education. From the survey data, we have
{Pr(E|H1 ), Pr(E|H2 ), Pr(E|H3 ), Pr(E|H4 )} = {.11, .19, .31, .53}.
These probabilities do not sum to 1 - they represent the proportions of people
with college degrees in the four different income subpopulations H1 , H2 , H3
and H4 . Now let‚Äôs consider the income distribution of the college-educated
population. Using Bayes‚Äô rule we can obtain
{Pr(H1 |E), Pr(H2 |E), Pr(H3 |E), Pr(H4 |E)} = {.09, .17, .27, .47} ,
and we see that the income distribution for people in the college-educated
population differs markedly from {.25, .25, .25, .25}, the distribution for the
general population. Note that these probabilities do sum to 1 - they are the
conditional probabilities of the events in the partition, given E.
In Bayesian inference, {H1 , . . . , HK } often refer to disjoint hypotheses or
states of nature and E refers to the outcome of a survey, study or experiment.
To compare hypotheses post-experimentally, we often calculate the following
ratio:
Pr(Hi |E)
Pr(E|Hi ) Pr(Hi )/ Pr(E)
=
Pr(Hj |E)
Pr(E|Hj ) Pr(Hj )/ Pr(E)
Pr(E|Hi ) Pr(Hi )
=
Pr(E|Hj ) Pr(Hj )
Pr(E|Hi )
Pr(Hi )
=
√ó
Pr(E|Hj ) Pr(Hj )
= ‚ÄúBayes factor‚Äù √ó ‚Äúprior beliefs‚Äù .
This calculation reminds us that Bayes‚Äô rule does not determine what our
beliefs should be after seeing the data, it only tells us how they should change
after seeing the data.
Example
Suppose we are interested in the rate of support for a particular candidate for
public office. Let
H = { all possible rates of support for candidate A };
H1 = { more than half the voters support candidate A };
H2 = { less than or equal to half the voters support candidate A };
E = { 54 out of 100 people surveyed said they support candidate A }.
Then {H1 , H2 } is a partition of H. Of interest is Pr(H1 |E), or Pr(H1 |E)/ Pr(H2 |E).
We will learn how to obtain these quantities in the next chapter.

2.4 Random variables

17

2.3 Independence
Definition 2 (Independence) Two events F and G are conditionally independent given H if Pr(F ‚à© G|H) = Pr(F |H) Pr(G|H).
How do we interpret conditional independence? By Axiom P3, the following
is always true:
Pr(F ‚à© G|H) = Pr(G|H) Pr(F |H ‚à© G).
If F and G are conditionally independent given H, then we must have
always

independence

Pr(G|H) Pr(F |H ‚à© G) = Pr(F ‚à© G|H)
=
Pr(F |H) Pr(G|H)
Pr(G|H) Pr(F |H ‚à© G)
=
Pr(F |H) Pr(G|H)
Pr(F |H ‚à© G)
=
Pr(F |H).
Conditional independence therefore implies that Pr(F |H ‚à© G) = Pr(F |H). In
other words, if we know H is true and F and G are conditionally independent
given H, then knowing G does not change our belief about F .
Examples
Let‚Äôs consider the conditional dependence of F and G when H is assumed to
be true in the following two situations:
F = { a hospital patient is a smoker }
G = { a hospital patient has lung cancer }
H = { smoking causes lung cancer}
F = { you are thinking of the jack of hearts }
G = { a mind reader claims you are thinking of the jack of hearts }
H = { the mind reader has extrasensory perception }
In both of these situations, H being true implies a relationship between F
and G. What about when H is not true?

2.4 Random variables
In Bayesian inference a random variable is defined as an unknown numerical quantity about which we make probability statements. For example, the
quantitative outcome of a survey, experiment or study is a random variable
before the study is performed. Additionally, a fixed but unknown population
parameter is also a random variable.

18

2 Belief, probability and exchangeability

2.4.1 Discrete random variables
Let Y be a random variable and let Y be the set of all possible values of Y .
We say that Y is discrete if the set of possible outcomes is countable, meaning
that Y can be expressed as Y = {y1 , y2 , . . .}.
Examples
‚Ä¢ Y = number of churchgoers in a random sample from a population
‚Ä¢ Y = number of children of a randomly sampled person
‚Ä¢ Y = number of years of education of a randomly sampled person
Probability distributions and densities
The event that the outcome Y of our survey has the value y is expressed as
{Y = y}. For each y ‚àà Y, our shorthand notation for Pr(Y = y) will be p(y).
This function of y is called the probability density function (pdf) of Y , and it
has the following properties:
1. 0P‚â§ p(y) ‚â§ 1 for all y ‚àà Y;
2.
y‚ààY p(y) = 1.
General probability statements
about Y can be derived from the pdf. For
P
example, Pr(Y ‚àà A) = y‚ààA p(y). If A and B are disjoint subsets of Y, then
Pr(Y ‚àà A or Y ‚àà B) ‚â° Pr(Y ‚àà A ‚à™ B) = Pr(Y ‚àà A) + Pr(Y ‚àà B)
X
X
=
p(y) +
p(y).
y‚ààA

y‚ààB

Example: Binomial distribution
Let Y = {0, 1, 2, . . . , n} for some positive integer n. The uncertain quantity
Y ‚àà Y has a binomial distribution with probability Œ∏ if
 
n y
Pr(Y = y|Œ∏) = dbinom(y, n, Œ∏) =
Œ∏ (1 ‚àí Œ∏)n‚àíy .
y
For example, if Œ∏ = .25 and n = 4, we have:
 
4
Pr(Y = 0|Œ∏ = .25) =
(.25)0 (.75)4
0
 
4
Pr(Y = 1|Œ∏ = .25) =
(.25)1 (.75)3
1
 
4
Pr(Y = 2|Œ∏ = .25) =
(.25)2 (.75)2
2
 
4
Pr(Y = 3|Œ∏ = .25) =
(.25)3 (.75)1
3
 
4
Pr(Y = 4|Œ∏ = .25) =
(.25)4 (.75)0
4

= .316
= .422
= .211
= .047
= .004 .

2.4 Random variables

19

Example: Poisson distribution
Let Y = {0, 1, 2, . . .}. The uncertain quantity Y ‚àà Y has a Poisson distribution
with mean Œ∏ if
Pr(Y = y|Œ∏) = dpois(y, Œ∏) = Œ∏y e‚àíŒ∏ /y!.
For example, if Œ∏ = 2.1 (the 2006 U.S. fertility rate),
= 0|Œ∏
= 1|Œ∏
= 2|Œ∏
= 3|Œ∏
..
.

= 2.1) = (2.1)0 e‚àí2.1 /(0!) =
= 2.1) = (2.1)1 e‚àí2.1 /(1!) =
= 2.1) = (2.1)2 e‚àí2.1 /(2!) =
= 2.1) = (2.1)3 e‚àí2.1 /(3!) =
..
.

6

8

.12
.26
.27
.19
..
.

0.00

0.00

0.05

0.02

p(y|Œ∏
Œ∏ = 21)
0.04
0.06

p(y|Œ∏
Œ∏ = 2.1)
0.10 0.15 0.20

0.08

0.25

Pr(Y
Pr(Y
Pr(Y
Pr(Y

0

2

4

10

y

0

20

40

60

80

100

y

Fig. 2.1. Poisson distributions with means of 2.1 and 21.

2.4.2 Continuous random variables
Suppose that the sample space Y is roughly equal to
PR, the set of all real
numbers. We cannot define Pr(Y ‚â§ 5) as equal to
y‚â§5 p(y) because the
sum does not make sense (the set of real numbers less than or equal to 5 is
‚Äúuncountable‚Äù). So instead of defining probabilities of events in terms of a pdf
p(y), courses in mathematical statistics often define probability distributions
for random variables in terms of something called a cumulative distribution
function, or cdf:
F (y) = Pr(Y ‚â§ y) .
Note that F (‚àû) = 1, F (‚àí‚àû) = 0, and F (b) ‚â§ F (a) if b < a. Probabilities of
various events can be derived from the cdf:

20

2 Belief, probability and exchangeability

‚Ä¢ Pr(Y > a) = 1 ‚àí F (a)
‚Ä¢ Pr(a < Y ‚â§ b) = F (b) ‚àí F (a)
If F is continuous (i.e. lacking any ‚Äújumps‚Äù), we say that Y is a continuous
random variable. A theorem from mathematics says that for every continuous
cdf F there exists a positive function p(y) such that
Z a
F (a) =
p(y) dy.
‚àí‚àû

This function is called the probability density function of Y , and its properties
are similar to those of a pdf for a discrete random variable:
1. 0R ‚â§ p(y) for all y ‚àà Y;
2. y‚ààR p(y) dy = 1.
As in the discrete case,R probability statements about Y can be derived from
the pdf: Pr(Y ‚àà A) = y‚ààA p(y) dy, and if A and B are disjoint subsets of Y,
then
Pr(Y ‚àà A or Y ‚àà B) ‚â° Pr(Y ‚àà A ‚à™ B) = Pr(Y ‚àà A) + Pr(Y ‚àà B)
Z
Z
=
p(y) dy +
p(y) dy.
y‚ààA

y‚ààB

Comparing these properties to the analogous properties in the discrete case,
we see that integration for continuous distributions behaves similarly to summation for discrete distributions. In fact, integration can be thought of as a
generalization of summation for situations in which the sample space is not
countable. However, unlike a pdf in the discrete case, the pdf for a continuous
random variable is not necessarily less than 1, and p(y) is not ‚Äúthe probability
that Y = y.‚Äù However, if p(y1 ) > p(y2 ) we will sometimes informally say that
y1 ‚Äúhas a higher probability‚Äù than y2 .
Example: Normal distribution
Suppose we are sampling from a population on Y = (‚àí‚àû, ‚àû), and we know
that the mean of the population is ¬µ and the variance is œÉ 2 . Among all probability distributions having a mean of ¬µ and a variance of œÉ 2 , the one that is
the most ‚Äúspread out‚Äù or ‚Äúdiffuse‚Äù (in terms of a measure called entropy), is
the normal(¬µ, œÉ 2 ) distribution, having a cdf given by
(

2 )
Z y
1 y‚àí¬µ
1
2
‚àö
exp ‚àí
dy.
Pr(Y ‚â§ y|¬µ, œÉ ) = F (y) =
2
œÉ
2œÄœÉ
‚àí‚àû
Evidently,
(

2 )
1
1 y‚àí¬µ
p(y|¬µ, œÉ ) = dnorm(y, ¬µ, œÉ) = ‚àö
exp ‚àí
.
2
œÉ
2œÄœÉ
2

2.4 Random variables

21

0.0

0.0

0.2

0.1

F(y)
0.4
0.6

p(y)
0.2
0.3

0.8

0.4

1.0

0.5

Letting ¬µ = 10.75 and œÉ = .8 (œÉ 2 = .64) gives the cdf and density in Figure
2.2. This mean and standard deviation make the median value of eY equal
to about 46,630, which is about the median U.S. household income in 2005.
Additionally, Pr(eY > 100000) = Pr(Y > log 100000) = 0.17, which roughly
matches the fraction of households in 2005 with incomes exceeding $100,000.

8

9

10

11
y

12

13

14

8

9

10

11
y

12

13

14

Fig. 2.2. Normal distribution with mean 10.75 and standard deviation 0.8.

2.4.3 Descriptions of distributions
The mean or expectation of an unknown quantity Y is given by
P
E[Y ] = y‚ààY yp(y) if Y is discrete;
R
E[Y ] = y‚ààY yp(y) dy if Y is continuous.
The mean is the center of mass of the distribution. However, it is not in general
equal to either of
the mode: ‚Äúthe most probable value of Y ,‚Äù or
the median: ‚Äúthe value of Y in the middle of the distribution.‚Äù
In particular, for skewed distributions (like income distributions) the mean
can be far from a ‚Äútypical‚Äù sample value: see, for example, Figure 2.3. Still,
the mean is a very popular description of the location of a distribution. Some
justifications for reporting and studying the mean include the following:
1. The mean of {Y1 , . . . , Yn } is a scaled version of the total, and the total is
often a quantity of interest.
2. Suppose you are forced to guess what the value of Y is, and you are
penalized by an amount (Y ‚àí yguess )2 . Then guessing E[Y ] minimizes your
expected penalty.

22

2 Belief, probability and exchangeability

mode
median
mean

0.0

0.0

0.1

0.5

105p(y)

p(y)
0.2
0.3

1.0

0.4

0.5

1.5

3. In some simple models that we shall see shortly, the sample mean contains
all of the information about the population that can be obtained from the
data.

8

9

10

11
y

12

13

0

50000

150000
y

250000

Fig. 2.3. Mode, median and mean of the normal and lognormal distributions, with
parameters ¬µ = 10.75 and œÉ = 0.8.

In addition to the location of a distribution we are often interested in how
spread out it is. The most popular measure of spread is the variance of a
distribution:
Var[Y ] = E[(Y ‚àí E[Y ])2 ]
= E[Y 2 ‚àí 2Y E[Y ] + E[Y ]2 ]
= E[Y 2 ] ‚àí 2E[Y ]2 + E[Y ]2
= E[Y 2 ] ‚àí E[Y ]2 .
The variance is the average squared distance that a sample value Y will be
from the population mean E[Y ]. The standard deviation is the square root of
the variance, and is on the same scale as Y .
Alternative measures of spread are based on quantiles. For a continuous,
strictly increasing cdf F , the Œ±-quantile is the value yŒ± such that F (yŒ± ) ‚â°
Pr(Y ‚â§ yŒ± ) = Œ±. The interquartile range of a distribution is the interval
(y.25 , y.75 ), which contains 50% of the mass of the distribution. Similarly, the
interval (y.025 , y.975 ) contains 95% of the mass of the distribution.

2.5 Joint distributions

23

2.5 Joint distributions
Discrete distributions
Let
‚Ä¢ Y1 , Y2 be two countable sample spaces;
‚Ä¢ Y1 , Y2 be two random variables, taking values in Y1 , Y2 respectively.
Joint beliefs about Y1 and Y2 can be represented with probabilities. For example, for subsets A ‚äÇ Y1 and B ‚äÇ Y2 , Pr({Y1 ‚àà A} ‚à© {Y2 ‚àà B}) represents
our belief that Y1 is in A and that Y2 is in B. The joint pdf or joint density
of Y1 and Y2 is defined as
pY1 Y2 (y1 , y2 ) = Pr({Y1 = y1 } ‚à© {Y2 = y2 }), for y1 ‚àà Y1 , y2 ‚àà Y2 .
The marginal density of Y1 can be computed from the joint density:
pY1 (y1 ) ‚â° Pr(Y1 = y1 )
X
=
Pr({Y1 = y1 } ‚à© {Y2 = y2 })
y2 ‚ààY2

‚â°

X

pY1 Y2 (y1 , y2 ) .

y2 ‚ààY2

The conditional density of Y2 given {Y1 = y1 } can be computed from the joint
density and the marginal density:
Pr({Y1 = y1 } ‚à© {Y2 = y2 })
Pr(Y1 = y1 )
pY1 Y2 (y1 , y2 )
=
.
pY1 (y1 )

pY2 |Y1 (y2 |y1 ) =

You should convince yourself that
{pY1 , pY2 |Y1 } can be derived from pY1 Y2 ,
{pY2 , pY1 |Y2 } can be derived from pY1 Y2 ,
pY1 Y2 can be derived from {pY1 , pY2 |Y1 },
pY1 Y2 can be derived from {pY2 , pY1 |Y2 },
but
pY1 Y2 cannot be derived from {pY1 , pY2 }.
The subscripts of density functions are often dropped, in which case the type
of density function is determined from the function argument: p(y1 ) refers to
pY1 (y1 ), p(y1 , y2 ) refers to pY1 Y2 (y1 , y2 ), p(y1 |y2 ) refers to pY1 |Y2 (y1 |y2 ), etc.

24

2 Belief, probability and exchangeability

Example: Social mobility
Logan (1983) reports the following joint distribution of occupational categories
of fathers and sons:
son‚Äôs occupation
father‚Äôs occupation farm operatives craftsmen sales professional
farm
0.018 0.035
0.031 0.008
0.018
operatives
0.002 0.112
0.064 0.032
0.069
craftsmen
0.001 0.066
0.094 0.032
0.084
sales
0.001 0.018
0.019 0.010
0.051
professional
0.001 0.029
0.032 0.043
0.130
Suppose we are to sample a father-son pair from this population. Let Y1 be
the father‚Äôs occupation and Y2 the son‚Äôs occupation. Then
Pr(Y2 = professional ‚à© Y1 = farm)
Pr(Y1 = farm)
.018
=
.018 + .035 + .031 + .008 + .018
= .164 .

Pr(Y2 = professional|Y1 = farm) =

Continuous joint distributions
If Y1 and Y2 are continuous we start with a cumulative distribution function.
Given a continuous joint cdf FY1 Y2 (a, b) ‚â° Pr({Y1 ‚â§ a} ‚à© {Y2 ‚â§ b}), there is
a function pY1 Y2 such that
Z

a

Z

b

FY1 Y2 (a, b) =

pY1 Y2 (y1 , y2 ) dy2 dy1 .
‚àí‚àû

‚àí‚àû

The function pY1 Y2 is the joint density of Y1 and Y2 . As in the discrete case,
we have
R‚àû
‚Ä¢ pY1 (y1 ) = ‚àí‚àû pY1 Y2 (y2 , y2 ) dy2 ;
‚Ä¢ pY2 |Y1 (y2 |y1 ) = pY1 Y2 (y1 , y2 )/pY1 (y1 ) .
You should convince yourself that pY2 |Y1 (y2 |y1 ) is an actual probability density, i.e. for each value of y1 it is a probability density for Y2 .
Mixed continuous and discrete variables
Let Y1 be discrete and Y2 be continuous. For example, Y1 could be occupational category and Y2 could be personal income. Suppose we define
‚Ä¢ a marginal density pY1 from our beliefs Pr(Y1 = y1 );
‚Ä¢ a conditional density pY2 |Y1 (y2 |y1 ) from Pr(Y2 ‚â§ y2 |Y1 = y1 ) ‚â° FY2 |Y1 (y2 |y1 )
as above.

2.5 Joint distributions

25

The joint density of Y1 and Y2 is then
pY1 Y2 (y1 , y2 ) = pY1 (y1 ) √ó pY2 |Y1 (y2 |y1 ),
and has the property that
Z
Pr(Y1 ‚àà A, Y2 ‚àà B) =
y2 ‚ààB

Ô£±
Ô£≤X
Ô£≥

y1 ‚ààA

Ô£º
Ô£Ω
pY1 Y2 (y1 , y2 ) dy2 .
Ô£æ

Bayes‚Äô rule and parameter estimation
Let
Œ∏ = proportion of people in a large population who have a certain characteristic.
Y = number of people in a small random sample from the population who
have the characteristic.
Then we might treat Œ∏ as continuous and Y as discrete. Bayesian estimation
of Œ∏ derives from the calculation of p(Œ∏|y), where y is the observed value of Y .
This calculation first requires that we have a joint density p(y, Œ∏) representing
our beliefs about Œ∏ and the survey outcome Y . Often it is natural to construct
this joint density from
‚Ä¢ p(Œ∏), beliefs about Œ∏;
‚Ä¢ p(y|Œ∏), beliefs about Y for each value of Œ∏.
Having observed {Y = y}, we need to compute our updated beliefs about Œ∏:
p(Œ∏|y) = p(Œ∏, y)/p(y) = p(Œ∏)p(y|Œ∏)/p(y) .
This conditional density is called the posterior density of Œ∏. Suppose Œ∏a and
Œ∏b are two possible numerical values of the true value of Œ∏. The posterior
probability (density) of Œ∏a relative to Œ∏b , conditional on Y = y, is
p(Œ∏a |y)
p(Œ∏a )p(y|Œ∏a )/p(y)
=
p(Œ∏b |y)
p(Œ∏b )p(y|Œ∏b )/p(y)
p(Œ∏a )p(y|Œ∏a )
=
.
p(Œ∏b )p(y|Œ∏b )
This means that to evaluate the relative posterior probabilities of Œ∏a and Œ∏b ,
we do not need to compute p(y). Another way to think about it is that, as a
function of Œ∏,
p(Œ∏|y) ‚àù p(Œ∏)p(y|Œ∏).
The constant of proportionality is 1/p(y), which could be computed from
Z
Z
p(y) =
p(y, Œ∏) dŒ∏ =
p(y|Œ∏)p(Œ∏) dŒ∏
Œò

Œò

26

2 Belief, probability and exchangeability

giving
p(Œ∏|y) = R

p(Œ∏)p(y|Œ∏)
.
p(Œ∏)p(y|Œ∏) dŒ∏
Œ∏

As we will see in later chapters, the numerator is the critical part.

2.6 Independent random variables
Suppose Y1 , . . . , Yn are random variables and that Œ∏ is a parameter describing
the conditions under which the random variables are generated. We say that
Y1 , . . . , Yn are conditionally independent given Œ∏ if for every collection of n
sets {A1 , . . . , An } we have
Pr(Y1 ‚àà A1 , . . . , Yn ‚àà An |Œ∏) = Pr(Y1 ‚àà A1 |Œ∏) √ó ¬∑ ¬∑ ¬∑ √ó Pr(Yn ‚àà An |Œ∏).
Notice that this definition of independent random variables is based on our
previous definition of independent events, where here each {Yj ‚àà Aj } is an
event. From our previous calculations, if independence holds, then
Pr(Yi ‚àà Ai |Œ∏, Yj ‚àà Aj ) = Pr(Yi ‚àà Ai |Œ∏),
so conditional independence can be interpreted as meaning that Yj gives no
additional information about Yi beyond that in knowing Œ∏. Furthermore, under
independence the joint density is given by
p(y1 , . . . , yn |Œ∏) = pY1 (y1 |Œ∏) √ó ¬∑ ¬∑ ¬∑ √ó pYn (yn |Œ∏) =

n
Y

pYi (yi |Œ∏),

i=1

the product of the marginal densities.
Suppose Y1 , . . . , Yn are generated in similar ways from a common process.
For example, they could all be samples from the same population, or runs
of an experiment performed under similar conditions. This suggests that the
marginal densities are all equal to some common density giving
p(y1 , . . . , yn |Œ∏) =

n
Y

p(yi |Œ∏).

i=1

In this case, we say that Y1 , . . . , Yn are conditionally independent and identically distributed (i.i.d.). Mathematical shorthand for this is
Y1 , . . . , Yn |Œ∏ ‚àº i.i.d. p(y|Œ∏).

2.7 Exchangeability

27

2.7 Exchangeability
Example: Happiness
Participants in the 1998 General Social Survey were asked whether or not
they were generally happy. Let Yi be the random variable associated with this
question, so that

1 if participant i says that they are generally happy,
Yi =
0 otherwise.
In this section we will consider the structure of our joint beliefs about
Y1 , . . . , Y10 , the outcomes of the first 10 randomly selected survey participants. As before, let p(y1 , . . . , y10 ) be our shorthand notation for Pr(Y1 =
y1 , . . . , Y10 = y10 ), where each yi is either 0 or 1.
Exchangeability
Suppose we are asked to assign probabilities to three different outcomes:
p(1, 0, 0, 1, 0, 1, 1, 0, 1, 1) = ?
p(1, 0, 1, 0, 1, 1, 0, 1, 1, 0) = ?
p(1, 1, 0, 0, 1, 1, 0, 0, 1, 1) = ?
Is there an argument for assigning them the same numerical value? Notice
that each sequence contains six ones and four zeros.
Definition 3 (Exchangeable) Let p(y1 , . . . , yn ) be the joint density of Y1 ,
. . ., Yn . If p(y1 , . . . , yn ) = p(yœÄ1 , . . . , yœÄn ) for all permutations œÄ of {1, . . . , n},
then Y1 , . . . , Yn are exchangeable.
Roughly speaking, Y1 , . . . , Yn are exchangeable if the subscript labels convey
no information about the outcomes.
Independence versus dependence
Consider the following two probability assignments:
Pr(Y10 = 1) = a
Pr(Y10 = 1|Y1 = Y2 = ¬∑ ¬∑ ¬∑ = Y8 = Y9 = 1) = b
Should we have a < b, a = b, or a > b? If a 6= b then Y10 is NOT independent
of Y1 , . . . , Y9 .

28

2 Belief, probability and exchangeability

Conditional independence
Suppose someone told you the numerical value of Œ∏, the rate of happiness
among the 1,272 respondents to the question. Do the following probability
assignments seem reasonable?
?

Pr(Y10 = 1|Œ∏) ‚âà Œ∏
?

Pr(Y10 = 1|Y1 = y1 , . . . , Y9 = y9 , Œ∏) ‚âà Œ∏
?

Pr(Y9 = 1|Y1 = y1 , . . . , Y8 = y8 , Y10 = y10 , Œ∏) ‚âà Œ∏
If these assignments are reasonable, then we can consider the Yi ‚Äôs as conditionally independent and identically distributed given Œ∏, or at least approximately
so: The population size of 1,272 is much larger than the sample size of 10, in
which case sampling without replacement is approximately the same as i.i.d.
sampling with replacement. Assuming conditional independence,
Pr(Yi = yi |Œ∏, Yj = yj , j 6= i) = Œ∏yi (1 ‚àí Œ∏)1‚àíyi
Pr(Y1 = y1 , . . . , Y10 = y10 |Œ∏) =

10
Y
i=1
P

=Œ∏

Œ∏yi (1 ‚àí Œ∏)1‚àíyi
yi

P

(1 ‚àí Œ∏)10‚àí

yi

.

If Œ∏ is uncertain to us, we describe our beliefs about it with p(Œ∏), a prior
distribution. The marginal joint distribution of Y1 , . . . , Y10 is then
Z 1
Z 1 P
P
p(y1 , . . . , y10 ) =
p(y1 , . . . , y10 |Œ∏)p(Œ∏) dŒ∏ =
Œ∏ yi (1 ‚àí Œ∏)10‚àí yi p(Œ∏) dŒ∏.
0

0

Now consider our probabilities for the three binary sequences given above:
R
p(1, 0, 0, 1, 0, 1, 1, 0, 1, 1) = R Œ∏6 (1 ‚àí Œ∏)4 p(Œ∏) dŒ∏
p(1, 0, 1, 0, 1, 1, 0, 1, 1, 0) = R Œ∏6 (1 ‚àí Œ∏)4 p(Œ∏) dŒ∏
p(1, 1, 0, 0, 1, 1, 0, 0, 1, 1) = Œ∏6 (1 ‚àí Œ∏)4 p(Œ∏) dŒ∏
It looks like Y1 , . . . , Yn are exchangeable under this model of beliefs.
Claim:
If Œ∏ ‚àº p(Œ∏) and Y1 , . . . , Yn are conditionally i.i.d. given Œ∏, then marginally
(unconditionally on Œ∏), Y1 , . . . , Yn are exchangeable.
Proof:
Suppose Y1 , . . . , Yn are conditionally i.i.d. given some unknown parameter Œ∏.
Then for any permutation œÄ of {1, . . . , n} and any set of values (y1 , . . . , yn ) ‚àà
Y n,

2.8 de Finetti‚Äôs theorem

29

Z
p(y1 , . . . , yn |Œ∏)p(Œ∏) dŒ∏
)
Z (Y
n
=
p(yi |Œ∏) p(Œ∏) dŒ∏

p(y1 , . . . , yn ) =

Z
=

i=1
( n
Y

(definition of marginal probability)
(Yi ‚Äôs are conditionally i.i.d.)

)
p(yœÄi |Œ∏) p(Œ∏) dŒ∏

(product does not depend on order)

i=1

= p(yœÄ1 , . . . yœÄn )

(definition of marginal probability) .

2.8 de Finetti‚Äôs theorem
We have seen that
Y1 , . . . , Yn |Œ∏ i.i.d
Œ∏ ‚àº p(Œ∏)


‚áí Y1 , . . . , Yn are exchangeable.

What about an arrow in the other direction? Let {Y1 , Y2 , . . .} be a potentially
infinite sequence of random variables all having a common sample space Y.
Theorem 1 (de Finetti) Let Yi ‚àà Y for all i ‚àà {1, 2, . . .}. Suppose that, for
any n, our belief model for Y1 , . . . , Yn is exchangeable:
p(y1 , . . . , yn ) = p(yœÄ1 , . . . , yœÄn )
for all permutations œÄ of {1, . . . , n}. Then our model can be written as
)
Z (Y
n
p(y1 , . . . , yn ) =
p(yi |Œ∏) p(Œ∏) dŒ∏
1

for some parameter Œ∏, some prior distribution on Œ∏ and some sampling model
p(y|Œ∏). The prior and sampling model depend on the form of the belief model
p(y1 , . . . , yn ).
The probability distribution p(Œ∏) represents our beliefs about the outcomes of
{Y1 , Y2 , . . .}, induced by our belief model p(y1 , y2 , . . .). More precisely,
P
p(Œ∏) represents our beliefs about limn‚Üí‚àû PYi /n in the binary case;
p(Œ∏) represents our beliefs about limn‚Üí‚àû (Yi ‚â§ c)/n for each c in the
general case.
The main ideas of this and the previous section can be summarized as follows:

Y1 , . . . , Yn |Œ∏ are i.i.d.
‚áî Y1 , . . . , Yn are exchangeable for all n .
Œ∏ ‚àº p(Œ∏)
When is the condition ‚ÄúY1 , . . . , Yn are exchangeable for all n‚Äù reasonable?
For this condition to hold, we must have exchangeability and repeatability.
Exchangeability will hold if the labels convey no information. Situations in
which repeatability is reasonable include the following:

30

2 Belief, probability and exchangeability

Y1 , . . . , Yn are outcomes of a repeatable experiment;
Y1 , . . . , Yn are sampled from a finite population with replacement;
Y1 , . . . , Yn are sampled from an infinite population without replacement.
If Y1 , . . . , Yn are exchangeable and sampled from a finite population of size
N >> n without replacement, then they can be modeled as approximately
being conditionally i.i.d. (Diaconis and Freedman, 1980).

2.9 Discussion and further references
The notion of subjective probability in terms of a coherent gambling strategy
was developed by de Finetti, who is of course also responsible for de Finetti‚Äôs
theorem (de Finetti, 1931, 1937). Both of these topics were studied further by
many others, including Savage (Savage, 1954; Hewitt and Savage, 1955).
The concept of exchangeability goes beyond just the concept of an infinitely exchangeable sequence considered in de Finetti‚Äôs theorem. Diaconis
and Freedman (1980) consider exchangeability for finite populations or sequences, and Diaconis (1988) surveys some other versions of exchangeability.
Chapter 4 of Bernardo and Smith (1994) provides a guide to building statistical models based on various types of exchangeability. A very comprehensive
and mathematical review of exchangeability is given in Aldous (1985), which
in particular provides an excellent survey of exchangeability as applied to
random matrices.

3
One-parameter models

A one-parameter model is a class of sampling distributions that is indexed
by a single unknown parameter. In this chapter we discuss Bayesian inference
for two one-parameter models: the binomial model and the Poisson model. In
addition to being useful statistical tools, these models also provide a simple
environment within which we can learn the basics of Bayesian data analysis,
including conjugate prior distributions, predictive distributions and confidence
regions.

3.1 The binomial model
Happiness data
Each female of age 65 or over in the 1998 General Social Survey was asked
whether or not they were generally happy. Let Yi = 1 if respondent i reported
being generally happy, and let Yi = 0 otherwise. If we lack information distinguishing these n = 129 individuals we may treat their responses as being
exchangeable. Since 129 is much smaller than the total size N of the female
senior citizen population, the results of the last chapter indicate that our joint
beliefs about Y1 , . . . , Y129 are well approximated by
PN
‚Ä¢ our beliefs about Œ∏ = i=1 Yi /N ;
‚Ä¢ the model that, conditional on Œ∏, the Yi ‚Äôs are i.i.d. binary random variables
with expectation Œ∏.
The last item says that the probability for any potential outcome {y1 , . . . , y129 },
conditional on Œ∏, is given by
p(y1 , . . . , y129 |Œ∏) = Œ∏

P129

i=1

yi

P129

(1 ‚àí Œ∏)129‚àí

i=1

What remains to be specified is our prior distribution.

P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 3,
c Springer Science+Business Media, LLC 2009


yi

.

32

3 One-parameter models

A uniform prior distribution
The parameter Œ∏ is some unknown number between 0 and 1. Suppose our
prior information is such that all subintervals of [0, 1] having the same length
also have the same probability. Symbolically,
Pr(a ‚â§ Œ∏ ‚â§ b) = Pr(a + c ‚â§ Œ∏ ‚â§ b + c) for 0 ‚â§ a < b < b + c ‚â§ 1.
This condition implies that our density for Œ∏ must be the uniform density:
p(Œ∏) = 1 for all Œ∏ ‚àà [0, 1].
For this prior distribution and the above sampling model, Bayes‚Äô rule gives
p(Œ∏|y1 , . . . , y129 ) =

p(y1 , . . . , y129 |Œ∏)p(Œ∏)
p(y1 , . . . , y129 )

= p(y1 , . . . , y129 |Œ∏) √ó

1
p(y1 , . . . , y129 )

‚àù p(y1 , . . . , y129 |Œ∏) .
The last line says that in this particular case p(Œ∏|y1 , . . . , y129 ) and p(y1 , . . .,
y129 |Œ∏) are proportional to each other as functions of Œ∏. This is because the
posterior distribution is equal to p(y1 , . . . , y129 |Œ∏) divided by something that
does not depend on Œ∏. This means that these two functions of Œ∏ have the same
shape, but not necessarily the same scale.
Data and posterior distribution
‚Ä¢ 129 individuals surveyed;
‚Ä¢ 118 individuals report being generally happy (91%);
‚Ä¢ 11 individuals do not report being generally happy (9%).
The probability of these data for a given value of Œ∏ is
p(y1 , . . . , y129 |Œ∏) = Œ∏118 (1 ‚àí Œ∏)11 .
A plot of this probability as a function of Œ∏ is shown in the first plot of Figure
3.1. Our result above about proportionality says that the posterior distribution
p(Œ∏|y1 , . . . , y129 ) will have the same shape as this function, and so we know
that the true value of Œ∏ is very likely to be near 0.91, and almost certainly
above 0.80. However, we will often want to be more precise than this, and
we will need to know the scale of p(Œ∏|y1 , . . . , yn ) as well as the shape. From
Bayes‚Äô rule, we have
p(Œ∏|y1 , . . . , y129 ) = Œ∏118 (1 ‚àí Œ∏)11 √ó p(Œ∏)/p(y1 , . . . , y129 )
= Œ∏118 (1 ‚àí Œ∏)11 √ó 1/p(y1 , . . . , y129 ).

33

1027p(y 1,...,y 129|Œ∏
Œ∏)
0 1 2 3 4

3.1 The binomial model

0.0

0.2

0.4

0.6

0.8

1.0

0.6

0.8

1.0

p(Œ∏
Œ∏|y 1,...,y 129)
0
5
10 15

Œ∏

0.0

0.2

0.4
Œ∏

Fig. 3.1. Sampling probability of the data as a function of Œ∏, along with the
posterior distribution. Note that a uniform prior distribution (plotted in gray in
the second panel) gives a posterior distribution that is proportional to the sampling
probability.

It turns out that we can calculate the scale or ‚Äúnormalizing constant‚Äù
1/p(y1 , . . . , y129 ) using the following result from calculus:
Z
0

1

Œ∏a‚àí1 (1 ‚àí Œ∏)b‚àí1 dŒ∏ =

Œì (a)Œì (b)
.
Œì (a + b)

(the value of the gamma function Œì (x) for any number x > 0 can be looked
up in a table, or with R using the gamma() function). How does the calculus
result help us compute p(Œ∏|y1 , . . . , y129 )? Let‚Äôs recall what we know about
p(Œ∏|y1 , . . . , y129 ):
R1
(a) 0 p(Œ∏|y1 , . . . , y129 ) dŒ∏ = 1, since all probability distributions integrate or
sum to 1;
(b) p(Œ∏|y1 , . . . , y129 ) = Œ∏118 (1 ‚àí Œ∏)11 /p(y1 , . . . , y129 ), from Bayes‚Äô rule.
Therefore,

34

3 One-parameter models

Z

1

1=

p(Œ∏|y1 , . . . , y129 ) dŒ∏

using (a)

Œ∏118 (1 ‚àí Œ∏)11 /p(y1 , . . . , y129 ) dŒ∏

using (b)

0

Z
1=

1

0

Z 1
1
Œ∏118 (1 ‚àí Œ∏)11 dŒ∏
p(y1 , . . . , y129 ) 0
1
Œì (119)Œì (12)
1=
using the calculus result, and so
p(y1 , . . . , y129 )
Œì (131)
1=

p(y1 , . . . , y129 ) =

Œì (119)Œì (12)
.
Œì (131)

You should convince yourself that this result holds for any sequence {y1 , . . . , y129 }
that contains 118 ones and 11 zeros. Putting everything together, we have
Œì (131)
Œ∏118 (1 ‚àí Œ∏)11 , which we will write as
Œì (119)Œì (12)
Œì (131)
=
Œ∏119‚àí1 (1 ‚àí Œ∏)12‚àí1 .
Œì (119)Œì (12)

p(Œ∏|y1 , . . . , y129 ) =

This density for Œ∏ is called a beta distribution with parameters a = 119 and
b = 12, which can be calculated, plotted and sampled from in R using the
functions dbeta() and rbeta() .
The beta distribution
An uncertain quantity Œ∏, known to be between 0 and 1, has a beta(a, b) distribution if
p(Œ∏) = dbeta(Œ∏, a, b) =

Œì (a + b) a‚àí1
Œ∏
(1 ‚àí Œ∏)b‚àí1
Œì (a)Œì (b)

for 0 ‚â§ Œ∏ ‚â§ 1.

For such a random variable,
mode[Œ∏] = (a ‚àí 1)/[(a ‚àí 1) + (b ‚àí 1)] if a > 1 and b > 1;
E[Œ∏] = a/(a + b);
Var[Œ∏] = ab/[(a + b + 1)(a + b)2 ] = E[Œ∏] √ó E[1 ‚àí Œ∏]/(a + b + 1).
For our data on happiness in which we observed (Y1 , . . . , Y129 ) = (y1 , . . . , y129 )
P129
with i=1 yi = 118,
mode[Œ∏|y1 , . . . , y129 ] = 0.915;
E[Œ∏|y1 , . . . , y129 ] = 0.908;
sd[Œ∏|y1 , . . . , y129 ] = 0.025.

3.1 The binomial model

35

3.1.1 Inference for exchangeable binary data
Posterior inference under a uniform prior
If Y1 , . . . , Yn |Œ∏ are i.i.d. binary(Œ∏), we showed that
p(Œ∏|y1 , . . . , yn ) = Œ∏

P

yi

P

(1 ‚àí Œ∏)n‚àí

yi

√ó p(Œ∏)/p(y1 , . . . , yn ).

If we compare the relative probabilities of any two Œ∏-values, say Œ∏a and Œ∏b , we
see that
P

y

P

p(Œ∏a |y1 , . . . , yn )
Œ∏a i (1 ‚àí Œ∏a )n‚àí yi √ó p(Œ∏a )/p(y1 , . . . , yn )
= Py
P
p(Œ∏b |y1 , . . . , yn )
Œ∏b i (1 ‚àí Œ∏b )n‚àí yi √ó p(Œ∏b )/p(y1 , . . . , yn )
 P y i 
n‚àíP yi
Œ∏a
1 ‚àí Œ∏a
p(Œ∏a )
=
.
Œ∏b
1 ‚àí Œ∏b
p(Œ∏b )
This shows that the probability
Pn density at Œ∏a relative to that at Œ∏b depends
on y1 , . . . , yn only through i=1 yi . From this, you can show that
!
n
n
X
X
Pr(Œ∏ ‚àà A|Y1 = y1 , . . . , Yn = yn ) = Pr Œ∏ ‚àà A|
Yi =
yi .
i=1

i=1

Pn

We interpret this as meaning that i=1 Yi P
contains all the information about
n
Œ∏ available from the data, and we say that i=1 Yi is a sufficient statistic for
Œ∏ and P
p(y1 , . . . , yn |Œ∏). The word ‚Äúsufficient‚Äù is used because it is ‚Äúsufficient‚Äù to
know
Yi in order to make inference about Œ∏. In this case where P
Y1 , . . . , Yn |Œ∏
n
are i.i.d. binary(Œ∏) random variables, the sufficient statistic Y = i=1 Yi has
a binomial distribution with parameters (n, Œ∏).
The binomial distribution
A random variable Y ‚àà {0, 1, . . . , n} has a binomial(n, Œ∏) distribution if
 
n y
Pr(Y = y|Œ∏) = dbinom(y, n, Œ∏) =
Œ∏ (1 ‚àí Œ∏)n‚àíy , y ‚àà {0, 1, . . . , n}.
y
Binomial distributions with different values of n and Œ∏ are plotted in Figures
3.2 and 3.3. For a binomial(n, Œ∏) random variable,
E[Y |Œ∏] = nŒ∏;
Var[Y |Œ∏] = nŒ∏(1 ‚àí Œ∏).

Pr(Y=y|Œ∏
Œ∏ = 0.8, n=10)
0.10
0.20
0.00

0.00

Pr(Y=y|Œ∏
Œ∏ = 0.2, n=10)
0.10
0.20

0.30

3 One-parameter models
0.30

36

0

2

4

6

8

10

0

2

4

y

6

8

10

80

100

y

0.10
Pr(Y=y|Œ∏
Œ∏ = 0.8, n=100)
0.02 0.04 0.06 0.08
0.00

0.00

Pr(Y=y|Œ∏
Œ∏ = 0.2, n=100)
0.02 0.04 0.06 0.08

0.10

Fig. 3.2. Binomial distributions with n = 10 and Œ∏ ‚àà {0.2, 0.8}.

0

20

40

60

80

100

0

20

y

40

60
y

Fig. 3.3. Binomial distributions with n = 100 and Œ∏ ‚àà {0.2, 0.8}.

Posterior inference under a uniform prior distribution
Having observed Y = y our task is to obtain the posterior distribution of Œ∏:
p(Œ∏|y) =
=

p(y|Œ∏)p(Œ∏)
p(y)

n y
n‚àíy
p(Œ∏)
y Œ∏ (1 ‚àí Œ∏)

p(y)
= c(y)Œ∏ (1 ‚àí Œ∏)n‚àíy p(Œ∏)
y

3.1 The binomial model

37

where c(y) is a function of y and not of Œ∏. For the uniform distribution with
p(Œ∏) = 1, we can find out what c(y) is using our calculus trick:
Z 1
1=
c(y)Œ∏y (1 ‚àí Œ∏)n‚àíy dŒ∏
0

Z
1 = c(y)

1

Œ∏y (1 ‚àí Œ∏)n‚àíy dŒ∏

0

1 = c(y)

Œì (y + 1)Œì (n ‚àí y + 1)
.
Œì (n + 2)

The normalizing constant c(y) is therefore equal to Œì (n + 2)/{Œì (y + 1)Œì (n ‚àí
y + 1)}, and we have
Œì (n + 2)
Œ∏y (1 ‚àí Œ∏)n‚àíy
Œì (y + 1)Œì (n ‚àí y + 1)
Œì (n + 2)
=
Œ∏(y+1)‚àí1 (1 ‚àí Œ∏)(n‚àíy+1)‚àí1
Œì (y + 1)Œì (n ‚àí y + 1)
= beta(y + 1, n ‚àí y + 1).
P
Recall the happiness example, where we observed that Y ‚â° Yi = 118:
X
n = 129, Y ‚â°
Yi = 118 ‚áí Œ∏|{Y = 118} ‚àº beta(119, 12).
p(Œ∏|y) =

This confirms the
P sufficiency result for this model and prior distribution, by
showing that if
yi = y = 118,
p(Œ∏|y1 , . . . , yn ) = p(Œ∏|y) = beta(119, 12).
In other words, the information contained in {Y1 = y1 , . P
. . , Yn = yn } is
Pthe
same as the information contained in {Y = y}, where Y = Yi and y = yi .
Posterior distributions under beta prior distributions
The uniform prior distribution has p(Œ∏) = 1 for all Œ∏ ‚àà [0, 1]. This distribution
can be thought of as a beta prior distribution with parameters a = 1, b = 1:
p(Œ∏) =

Œì (2)
1
Œ∏1‚àí1 (1 ‚àí Œ∏)1‚àí1 =
1 √ó 1 = 1.
Œì (1)Œì (1)
1√ó1

Note that Œì (x + 1) = x! = x √ó (x ‚àí 1) ¬∑ ¬∑ ¬∑ √ó 1 if x is a positive integer, and
Œì (1) = 1 by convention. In the previous paragraph, we saw that


Œ∏ ‚àº beta(1, 1) (uniform)
if
, then {Œ∏|Y = y} ‚àº beta(1 + y, 1 + n ‚àí y),
Y ‚àº binomial(n, Œ∏)
and so to get the posterior distribution when our prior distribution is beta(a =
1, b = 1), we can simply add the number of 1‚Äôs to the a parameter and the

38

3 One-parameter models

number of 0‚Äôs to the b parameter. Does this result hold for arbitrary beta priors? Let‚Äôs find out: Suppose Œ∏ ‚àº beta(a, b) and Y |Œ∏ ‚àº binomial(n, Œ∏). Having
observed Y = y,
p(Œ∏)p(y|Œ∏)
p(y)
 
1
Œì (a + b) a‚àí1
n y
=
√ó
Œ∏
(1 ‚àí Œ∏)b‚àí1 √ó
Œ∏ (1 ‚àí Œ∏)n‚àíy
p(y) Œì (a)Œì (b)
y

p(Œ∏|y) =

= c(n, y, a, b) √ó Œ∏a+y‚àí1 (1 ‚àí Œ∏)b+n‚àíy‚àí1
= dbeta(Œ∏, a + y, b + n ‚àí y) .
It is important to understand the last two lines above: The second to last line
says that p(Œ∏|y) is, as a function of Œ∏, proportional to Œ∏a+y‚àí1 √ó(1‚àíŒ∏)b+n‚àíy‚àí1 .
This means that it has the same shape as the beta density dbeta(Œ∏, a+y, b+n‚àí
y). But we also know that p(Œ∏|y) and the beta density must both integrate to 1,
and therefore they also share the same scale. These two things together mean
that p(Œ∏|y) and the beta density are in fact the same function. Throughout the
book we will use this trick to identify posterior distributions: We will recognize
that the posterior distribution is proportional to a known probability density,
and therefore must equal that density.
Conjugacy
We have shown that a beta prior distribution and a binomial sampling model
lead to a beta posterior distribution. To reflect this, we say that the class of
beta priors is conjugate for the binomial sampling model.
Definition 4 (Conjugate) A class P of prior distributions for Œ∏ is called
conjugate for a sampling model p(y|Œ∏) if
p(Œ∏) ‚àà P ‚áí p(Œ∏|y) ‚àà P.
Conjugate priors make posterior calculations easy, but might not actually
represent our prior information. However, mixtures of conjugate prior distributions are very flexible and are computationally tractable (see Exercises 3.4
and 3.5).
Combining information
If Œ∏|{Y = y} ‚àº beta(a + y, b + n ‚àí y), then
E[Œ∏|y] =

a+y
a+y‚àí1
E[Œ∏|y]E[1 ‚àí Œ∏|y]
, mode[Œ∏|y] =
, Var[Œ∏|y] =
.
a+b+n
a+b+n‚àí2
a+b+n+1

The posterior expectation E[Œ∏|y] is easily recognized as a combination of prior
and data information:

3.1 The binomial model
beta(1,1) prior, n=5

‚àë y i =1

beta(3,2) prior, n=5

p( Œ∏|y)
1.0

2.0

‚àë y i =1

0.0

0.0

p( Œ∏|y)
1.0

2.0

prior
posterior

39

0.2

0.4

0.6
Œ∏

1.0

0.0

0.2

0.4

0.6
Œ∏

‚àë y i =20

beta(3,2) prior, n=100

0.8

1.0

‚àë y i =20

0

0

2

2

p(Œ∏
Œ∏|y)
4 6

p(Œ∏
Œ∏|y)
4
6

8

8

10

beta(1,1) prior, n=100

0.8

10

0.0

0.0

0.2

0.4

0.6

0.8

Œ∏

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Œ∏

Fig. 3.4. Beta posterior distributions under two different sample sizes and two different prior distributions. Look across a row to see the effect of the prior distribution,
and down a column to see the effect of the sample size.

a+y
a+b+n
a+b
a
n
y
=
+
a+b+na+b a+b+nn
a+b
n
=
√ó prior expectation +
√ó data average .
a+b+n
a+b+n

E[Œ∏|y] =

For this model and prior distribution, the posterior expectation (also known
as the posterior mean) is a weighted average of the prior expectation and the
sample average, with weights proportional to a + b and n respectively. This
leads to the interpretation of a and b as ‚Äúprior data‚Äù:
a ‚âà ‚Äúprior number of 1‚Äôs,‚Äù
b ‚âà ‚Äúprior number of 0‚Äôs,‚Äù
a + b ‚âà ‚Äúprior sample size.‚Äù
If our sample size n is larger than our prior sample size a + b, then it seems
reasonable that a majority of our information about Œ∏ should be coming from
the data as opposed to the prior distribution. This is indeed the case: For
example, if n >> a + b, then
a+b
y
1y
y
‚âà 0 , E[Œ∏|y] ‚âà , Var[Œ∏|y] ‚âà
1‚àí
.
a+b+n
n
nn
n

40

3 One-parameter models

Prediction
An important feature of Bayesian inference is the existence of a predictive
distribution for new observations. Reverting for the moment to our notation
for binary data, let y1 , . . . , yn be the outcomes from a sample of n binary
random variables, and let YÀú ‚àà {0, 1} be an additional outcome from the same
population that has yet to be observed. The predictive distribution of YÀú is the
conditional distribution of YÀú given {Y1 = y1 , . . . , Yn = yn }. For conditionally
i.i.d. binary variables this distribution can be derived from the distribution of
YÀú given Œ∏ and the posterior distribution of Œ∏:
Z
Àú
Pr(Y = 1|y1 , . . . , yn ) = Pr(YÀú = 1, Œ∏|y1 , . . . , yn ) dŒ∏
Z
= Pr(YÀú = 1|Œ∏, y1 , . . . , yn )p(Œ∏|y1 , . . . , yn ) dŒ∏
Z
= Œ∏p(Œ∏|y1 , . . . , yn ) dŒ∏
Pn
a + i=1 yi
= E[Œ∏|y1 , . . . , yn ] =
a+b+n
Pn
b + i=1 (1 ‚àí yi )
Pr(YÀú = 0|y1 , . . . , yn ) = 1 ‚àí E[Œ∏|y1 , . . . , yn ] =
.
a+b+n
You should notice two important things about the predictive distribution:
1. The predictive distribution does not depend on any unknown quantities.
If it did, we would not be able to use it to make predictions.
2. The predictive distribution depends on our observed data. In this distribution, YÀú is not independent of Y1 , . . . , Yn (recall Section 2.7). This
is because observing Y1 , . . . , Yn gives information about Œ∏, which in turn
gives information about YÀú . It would be bad if YÀú were independent of
Y1 , . . . , Yn - it would mean that we could never infer anything about the
unsampled population from the sample cases.
Example
The uniform prior distribution, or beta(1,1) prior, can be thought of as equivalent to the information in a prior dataset consisting of a single ‚Äú1‚Äù and a
single ‚Äú0‚Äù. Under this prior distribution,
Pr(YÀú = 1|Y = y) = E[Œ∏|Y = y] =
mode(Œ∏|Y = y) =

2 1
n y
+
,
2+n2 2+nn

y
,
n

Pn
where Y = i=1 Yi . Does the discrepancy between these two posterior summaries of our information make sense? Consider the case in which Y = 0, for
which mode(Œ∏|Y = 0) = 0 but Pr(YÀú = 1|Y = 0) = 1/(2 + n).

3.1 The binomial model

41

3.1.2 Confidence regions
It is often desirable to identify regions of the parameter space that are likely
to contain the true value of the parameter. To do this, after observing the
data Y = y we can construct an interval [l(y), u(y)] such that the probability
that l(y) < Œ∏ < u(y) is large.
Definition 5 (Bayesian coverage) An interval [l(y), u(y)], based on the
observed data Y = y, has 95% Bayesian coverage for Œ∏ if
Pr(l(y) < Œ∏ < u(y)|Y = y) = .95.
The interpretation of this interval is that it describes your information about
the location of the true value of Œ∏ after you have observed Y = y. This is
different from the frequentist interpretation of coverage probability, which
describes the probability that the interval will cover the true value before the
data are observed:
Definition 6 (frequentist coverage) A random interval [l(Y ), u(Y )] has
95% frequentist coverage for Œ∏ if, before the data are gathered,
Pr(l(Y ) < Œ∏ < u(Y )|Œ∏) = .95.
In a sense, the frequentist and Bayesian notions of coverage describe pre- and
post-experimental coverage, respectively.
You may recall your introductory statistics instructor belaboring the following point: Once you observe Y = y and you plug this data into your
confidence interval formula [l(y), u(y)], then

0 if Œ∏ 6‚àà [l(y), u(y)];
Pr(l(y) < Œ∏ < u(y)|Œ∏) =
1 if Œ∏ ‚àà [l(y), u(y)].
This highlights the lack of a post-experimental interpretation of frequentist
coverage. Although this may make the frequentist interpretation seem somewhat lacking, it is still useful in many situations. Suppose you are running a
large number of unrelated experiments and are creating a confidence interval
for each one of them. If your intervals each have 95% frequentist coverage
probability, you can expect that 95% of your intervals contain the correct
parameter value.
Can a confidence interval have the same Bayesian and frequentist coverage
probability? Hartigan (1966) showed that, for the types of intervals we will
construct in this book, an interval that has 95% Bayesian coverage additionally
has the property that
Pr(l(Y ) < Œ∏ < u(Y )|Œ∏) = .95 + n
where |n | < na for some constant a. This means that a confidence interval
procedure that gives 95% Bayesian coverage will have approximately 95% frequentist coverage as well, at least asymptotically. It is important to keep in

42

3 One-parameter models

mind that most non-Bayesian methods of constructing 95% confidence intervals also only achieve this coverage rate asymptotically. For more discussion of
the similarities between intervals constructed by Bayesian and non-Bayesian
methods, see Severini (1991) and Sweeting (2001).
Quantile-based interval
Perhaps the easiest way to obtain a confidence interval is to use posterior
quantiles. To make a 100 √ó (1 ‚àí Œ±)% quantile-based confidence interval, find
numbers Œ∏Œ±/2 < Œ∏1‚àíŒ±/2 such that
1. Pr(Œ∏ < Œ∏Œ± |Y = y) = Œ±/2;
2. Pr(Œ∏ > Œ∏1‚àíŒ±/2 |Y = y) = Œ±/2.
The numbers Œ∏Œ±/2 , Œ∏1‚àíŒ±/2 are the Œ±/2 and 1 ‚àí Œ±/2 posterior quantiles of Œ∏,
and so
Pr(Œ∏ ‚àà [Œ∏Œ±/2 , Œ∏1‚àíŒ±/2 ]|Y = y) = 1 ‚àí Pr(Œ∏ 6‚àà [Œ∏Œ±/2 , Œ∏1‚àíŒ±/2 ]|Y = y)
= 1 ‚àí [Pr(Œ∏ < Œ∏Œ±/2 |Y = y) + Pr(Œ∏ > Œ∏1‚àíŒ±/2 |Y = y)]
= 1 ‚àí Œ±.
Example: Binomial sampling and uniform prior
Suppose out of n = 10 conditionally independent draws of a binary random
variable we observe Y = 2 ones. Using a uniform prior distribution for Œ∏,
the posterior distribution is Œ∏|{Y = 2} ‚àº beta(1 + 2, 1 + 8). A 95% posterior
confidence interval can be obtained from the .025 and .975 quantiles of this
beta distribution. These quantiles are 0.06 and 0.52 respectively, and so the
posterior probability that Œ∏ ‚àà [0.06, 0.52] is 95%.
> a<‚àí1 ; b<‚àí1
> n<‚àí10 ; y<‚àí2

#p r i o r
#data

> q b e t a ( c ( . 0 2 5 , . 9 7 5 ) , a+y , b+n‚àíy )
[ 1 ] 0.06021773 0.51775585

Highest posterior density (HPD) region
Figure 3.5 shows the posterior distribution and a 95% confidence interval for Œ∏
from the previous example. Notice that there are Œ∏-values outside the quantilebased interval that have higher probability (density) than some points inside
the interval. This suggests a more restrictive type of interval:
Definition 7 (HPD region) A 100 √ó (1 ‚àí Œ±)% HPD region consists of a
subset of the parameter space, s(y) ‚äÇ Œò such that
1. Pr(Œ∏ ‚àà s(y)|Y = y) = 1 ‚àí Œ± ;

43

0.0

0.5

1.0

p( Œ∏|y)
1.5 2.0

2.5

3.0

3.2 The Poisson model

0.0

0.2

0.4

0.6

0.8

1.0

Œ∏

Fig. 3.5. A beta posterior distribution, with vertical bars indicating a 95% quantilebased confidence interval.

2. If Œ∏a ‚àà s(y), and Œ∏b 6‚àà s(y), then p(Œ∏a |Y = y) > p(Œ∏b |Y = y).
All points in an HPD region have a higher posterior density than points outside the region. However, an HPD region might not be an interval if the
posterior density is multimodal (having multiple peaks). Figure 3.6 gives the
basic idea behind the construction of an HPD region: Gradually move a horizontal line down across the density, including in the HPD region all Œ∏-values
having a density above the horizontal line. Stop moving the line down when
the posterior probability of the Œ∏-values in the region reaches (1 ‚àí Œ±). For the
binomial example above, the 95% HPD region is [0.04, 0.048], which is narrower (more precise) than the quantile-based interval, yet both contain 95%
of the posterior probability.

3.2 The Poisson model
Some measurements, such as a person‚Äôs number of children or number of
friends, have values that are whole numbers. In these cases our sample space
is Y = {0, 1, 2, . . .}. Perhaps the simplest probability model on Y is the Poisson
model.
Poisson distribution
Recall from Chapter 2 that a random variable Y has a Poisson distribution
with mean Œ∏ if
Pr(Y = y|Œ∏) = dpois(y, Œ∏) = Œ∏y e‚àíŒ∏ /y! for y ‚àà {0, 1, 2, . . .} .
For such a random variable,

3 One-parameter models

2.5

3.0

44

0.0

0.5

1.0

p( Œ∏|y)
1.5 2.0

50% HPD
75% HPD
95% HPD
95% quantile‚àíbased

0.0

0.2

0.4

0.6

0.8

1.0

Œ∏

Fig. 3.6. Highest posterior density regions of varying probability content. The
dashed line is the 95% quantile-based interval.

‚Ä¢ E[Y |Œ∏] = Œ∏;
‚Ä¢ Var[Y |Œ∏] = Œ∏.

Pr(‚àë Y i = y|Œ∏
Œ∏ = 1.83)
0.02 0.04 0.06 0.08

0.30

People sometimes say that the Poisson family of distributions has a ‚Äúmeanvariance relationship‚Äù because if one Poisson distribution has a larger mean
than another, it will have a larger variance as well.

0.00

0.00

Pr(Y i = y i )
0.10
0.20

Poisson model
empirical distribution

0

2
4
6
number of children

8

0

10

20
30
40
number of children

50

Fig. 3.7. Poisson distributions. The first panel shows a Poisson distribution with
mean of 1.83, along with the empirical distribution of the number of children of
women of age 40 from the GSS during the 1990s. The second panel shows the
distribution of the sum of 10 i.i.d. Poisson random variables with mean 1.83. This
is the same as a Poisson distribution with mean 18.3

3.2 The Poisson model

45

3.2.1 Posterior inference
If we model Y1 , . . . , Yn as i.i.d. Poisson with mean Œ∏, then the joint pdf of our
sample data is as follows:
Pr(Y1 = y1 , . . . , Yn = yn |Œ∏) =
=

n
Y

p(yi |Œ∏)

i=1
n
Y

1 yi ‚àíŒ∏
Œ∏ e
y!
i=1 i

= c(y1 , . . . , yn )Œ∏

P

yi ‚àínŒ∏

e

.

Comparing two values of Œ∏ a posteriori, we have
P

y

p(Œ∏a |y1 , . . . , yn )
c(y1 , . . . , yn ) e‚àínŒ∏a Œ∏a i p(Œ∏a )
P
=
p(Œ∏b |y1 , . . . , yn )
c(y1 , . . . , yn ) e‚àínŒ∏b Œ∏ yi p(Œ∏b )
b
P

y

e‚àínŒ∏a Œ∏a i p(Œ∏a )
= ‚àínŒ∏ P y
.
e b Œ∏ i p(Œ∏b )
b
Pn
As in the case of the i.i.d. binary model, i=1 Yi contains all theP
information
n
about Œ∏ that is available in the data,
and
again
we
say
that
i=1 Yi is a
Pn
sufficient statistic. Furthermore, { i=1 Yi |Œ∏} ‚àº Poisson(nŒ∏).
Conjugate prior
For now we will work with a class of conjugate prior distributions that will
make posterior calculations simple. Recall that a class of prior densities is
conjugate for a sampling model p(y1 , . . . , yn |Œ∏) if the posterior distribution is
also in the class. For the Poisson sampling model, our posterior distribution
for Œ∏ has the following form:
p(Œ∏|y1 , . . . , yn ) ‚àù p(Œ∏) √ó p(y1 , . . . , yn |Œ∏)
P
‚àù p(Œ∏) √ó Œ∏ yi e‚àínŒ∏ .
This means that whatever our conjugate class of densities is, it will have
to include terms like Œ∏c1 e‚àíc2 Œ∏ for numbers c1 and c2 . The simplest class of
such densities includes only these terms, and their corresponding probability
distributions are known as the family of gamma distributions.
Gamma distribution
An uncertain positive quantity Œ∏ has a gamma(a, b) distribution if
p(Œ∏) = dgamma(Œ∏, a, b) =
For such a random variable,

ba a‚àí1 ‚àíbŒ∏
Œ∏
e ,
Œì (a)

for Œ∏, a, b > 0.

46

3 One-parameter models

‚Ä¢ E[Œ∏] = a/b;
2
‚Ä¢ Var[Œ∏] = a/b
;
(a ‚àí 1)/b if a > 1
‚Ä¢ mode[Œ∏] =
.
0
if a ‚â§ 1

a=4 b=4
0.8

a=2 b=2

4

6

8

10

p( Œ∏)
0.4
0.6
0.2
4

6

8

10

0

2

4

6
Œ∏

a=2 b=1

a=8 b=4

a=32 b=16

4

6

8

10

0

Œ∏

8

10

8

10

p( Œ∏)
0.0 0.2 0.4 0.6 0.8 1.0

p( Œ∏)
0.2
0.1

2

2

Œ∏

0.0
0

0

Œ∏
p( Œ∏)
0.0 0.1 0.2 0.3 0.4 0.5 0.6

2

0.3

0

0.0

0.0

0.0

0.2

0.2

p( Œ∏)
0.4

p( Œ∏)
0.4 0.6

0.8

0.6

1.0

a=1 b=1

2

4

6

8

10

0

2

4

Œ∏

6
Œ∏

Fig. 3.8. Gamma densities.

Posterior distribution of Œ∏
Suppose Y1 , . . . , Yn |Œ∏ ‚àº i.i.d. Poisson(Œ∏) and p(Œ∏)=dgamma(Œ∏, a, b). Then
p(Œ∏|y1 , . . . , yn ) = p(Œ∏) √ó p(y1 , . . . , yn |Œ∏)/p(y1 , . . . , yn )
o

	 n P
= Œ∏a‚àí1 e‚àíbŒ∏ √ó Œ∏ yi e‚àínŒ∏ √ó c(y1 , . . . , yn , a, b)
n
o
P
= Œ∏a+ yi ‚àí1 e‚àí(b+n)Œ∏ √ó c(y1 , . . . , yn , a, b).
This is evidently a gamma distribution, and we have confirmed the conjugacy
of the gamma family for the Poisson sampling model:

n
X
Œ∏ ‚àº gamma(a, b)
‚áí {Œ∏|Y1 , . . . , Yn } ‚àº gamma(a +
Yi , b + n) .
Y1 , . . . , Yn |Œ∏ ‚àº Poisson(Œ∏)
i=1

Estimation and prediction proceed in a manner similar to that in the binomial
model. The posterior expectation of Œ∏ is a convex combination of the prior
expectation and the sample average:

3.2 The Poisson model

47

P

a + yi
b+n
P
b a
n
yi
=
+
b+n b
b+n n

E[Œ∏|y1 , . . . , yn ] =

‚Ä¢ b is interpreted as the number of prior observations;
‚Ä¢ a is interpreted as the sum of counts from b prior observations.
For large n, the information from the data dominates the prior information:
n >> b ‚áí E[Œ∏|y1 , . . . , yn ] ‚âà y¬Ø, Var[Œ∏|y1 , . . . , yn ] ‚âà y¬Ø/n .
Predictions about additional data can be obtained with the posterior predictive distribution:
Z ‚àû
p(Àú
y |y1 , . . . , yn ) =
p(Àú
y |Œ∏, y1 , . . . , yn )p(Œ∏|y1 , . . . , yn ) dŒ∏
0
Z
= p(Àú
y |Œ∏)p(Œ∏|y1 , . . . , yn ) dŒ∏
Z
X
= dpois(Àú
y , Œ∏)dgamma(Œ∏, a +
yi , b + n) dŒ∏
P


Z 
1 yÀú ‚àíŒ∏
(b + n)a+ yi a+P yi ‚àí1 ‚àí(b+n)Œ∏
P
=
Œ∏ e
Œ∏
e
dŒ∏
yÀú!
Œì (a + yi )
P
Z
‚àû
P
(b + n)a+ yi
P
=
Œ∏a+ yi +Àúy‚àí1 e‚àí(b+n+1)Œ∏ dŒ∏ .
Œì (Àú
y + 1)Œì (a + yi ) 0
Evaluation of this complicated integral looks daunting, but it turns out that
it can be done without any additional calculus. Let‚Äôs use what we know about
the gamma density:
Z ‚àû a
b
1=
Œ∏a‚àí1 e‚àíbŒ∏ dŒ∏ for any values a, b > 0 .
Œì
(a)
0
This means that
Z ‚àû
0

Œ∏a‚àí1 e‚àíbŒ∏ dŒ∏ =

Œì (a)
ba

for any values a, b > 0 .

P
Now substitute in a + yi + yÀú instead of a and b + n + 1 instead of b to get
P
Z ‚àû
P
Œì (a + yi + yÀú)
P
.
Œ∏a+ yi +Àúy‚àí1 e‚àí(b+n+1)Œ∏ dŒ∏ =
(b + n + 1)a+ yi +Àúy
0
After simplifying some of the algebra, this gives
P

a+P yi 
yÀú
Œì (a + yi + yÀú)
b+n
1
P
p(Àú
y |y1 , . . . , yn ) =
Œì (Àú
y + 1)Œì (a + yi ) b + n + 1
b+n+1

48

3 One-parameter models

for yÀú P
‚àà {0, 1, 2, . . .}. This is a negative binomial distribution with parameters
(a + yi , b + n), for which
P
a + yi
Àú
E[Y |y1 , . . . , yn ] =
= E[Œ∏|y1 , . . . , yn ];
b +Pn
a + yi b + n + 1
Var[YÀú |y1 , . . . , yn ] =
= Var[Œ∏|y1 , . . . yn ] √ó (b + n + 1)
b+n
b+n
b+n+1
= E[Œ∏|y1 , . . . , yn ] √ó
.
b+n
Let‚Äôs try to obtain a deeper understanding of this formula for the predictive
variance. Recall, the predictive variance is to some extent a measure of our
posterior uncertainty about a new sample YÀú from the population. Uncertainty
about YÀú stems from uncertainty about the population and the variability
in sampling from the population. For large n, uncertainty about Œ∏ is small
((b + n + 1)/(b + n) ‚âà 1) and uncertainty about YÀú stems primarily from
sampling variability, which for the Poisson model is equal to Œ∏. For small n,
uncertainty in YÀú also includes the uncertainty in Œ∏, and so the total uncertainty
is larger than just the sampling variability ((b + n + 1)/(b + n) > 1).
3.2.2 Example: Birth rates

Bachelor's or higher

0

0

2

10

4

n 1(y )
20

n 2(y )
6
8

10

30

12

Less than bachelor's

0

1

2

3
y

4

5

6

0

1

2
y

3

4

Fig. 3.9. Numbers of children for the two groups.

Over the course of the 1990s the General Social Survey gathered data on
the educational attainment and number of children of 155 women who were 40
years of age at the time of their participation in the survey. These women were
in their 20s during the 1970s, a period of historically low fertility rates in the

3.2 The Poisson model

49

United States. In this example we will compare the women with college degrees
to those without in terms of their numbers of children. Let Y1,1 . . . , Yn1 ,1
denote the numbers of children for the n1 women without college degrees and
Y1,2 . . . , Yn2 ,2 be the data for women with degrees. For this example, we will
use the following sampling models:
Y1,1 . . . , Yn1 ,1 |Œ∏1 ‚àº i.i.d. Poisson(Œ∏1 )
Y1,2 . . . , Yn2 ,2 |Œ∏2 ‚àº i.i.d. Poisson(Œ∏2 )
The appropriateness of the Poisson model for these data will be examined in
the next chapter.
Empirical distributions for the data are displayed in Figure 3.9, and group
sums and means are as follows:
Pn1
Less than bachelor‚Äôs: n1 = 111, P i=1
Yi,1 = 217, Y¬Ø1 = 1.95
n2
¬Ø
Bachelor‚Äôs or higher: n2 = 44,
i=1 Yi,2 = 66, Y2 = 1.50
In the case where {Œ∏1 , Œ∏2 } ‚àº i.i.d. gamma(a = 2, b = 1), we have the following
posterior distributions:
X
Œ∏1 |{n1 = 111,
Yi,1 = 217} ‚àº gamma(2 + 217, 1 + 111) = gamma(219, 112)
X
Œ∏2 |{n2 = 44,
Yi,2 = 66} ‚àº gamma(2 + 66, 1 + 44) = gamma(68, 45)
Posterior means, modes and 95% quantile-based confidence intervals for Œ∏1
and Œ∏2 can be obtained from their gamma posterior distributions:
> a<‚àí2 ; b<‚àí1
> n1<‚àí111 ; sy1 <‚àí217
> n2<‚àí44 ; sy2 <‚àí66

# p r i o r parameters
# data i n group 1
# data i n group 2

> ( a+sy1 ) / ( b+n1 )
# p o s t e r i o r mean
[ 1 ] 1.955357
> ( a+sy1 ‚àí1)/(b+n1 )
# p o s t e r i o r mode
[ 1 ] 1.946429
> qgamma( c ( . 0 2 5 , . 9 7 5 ) , a+sy1 , b+n1 )
# p o s t e r i o r 95% CI
[ 1 ] 1.704943 2.222679
> ( a+sy2 ) / ( b+n2 )
[ 1 ] 1.511111
> ( a+sy2 ‚àí1)/(b+n2 )
[ 1 ] 1.488889
> qgamma( c ( . 0 2 5 , . 9 7 5 ) , a+sy2 , b+n2 )
[ 1 ] 1.173437 1.890836

Posterior densities for the population means of the two groups are shown in
the first panel of Figure 3.10. The posterior
substantial evidence
P indicatesP
that Œ∏1 > Œ∏2 . For example, Pr(Œ∏1 > Œ∏2 | Yi,1 = 217, Yi,2 = 66) = 0.97.
Now consider two randomly sampled individuals, one from each of the two

3 One-parameter models
3.0

50

0.0

0.00

0.5

p( yn++1|y1...yn)
0.10
0.20

p( Œ∏|y1...yn)
1.0 1.5 2.0

2.5

0.30

Less than bachelor's
Bachelor's or higher

0

1

2

3
Œ∏

4

5

0

2

4

6
y n++1

8

10

12

Fig. 3.10. Posterior distributions of mean birth rates (with the common prior
distribution given by the dashed line), and posterior predictive distributions for
number of children.

populations. To what extent do we expect the one without the bachelor‚Äôs
degree to have more children than the other? We can calculate the relevant
probabilities exactly: The posterior predictive distributions for YÀú1 and YÀú2 are
both negative binomial distributions and are plotted in the second panel of
Figure 3.10.
> y<‚àí 0 : 1 0
> dnbinom ( y , s i z e =(a+sy1 ) , mu=(a+sy1 ) / ( b+n1 ) )
[ 1 ] 1 . 4 2 7 4 7 3 e ‚àí01 2 . 7 6 6 5 1 8 e ‚àí01 2 . 6 9 3 0 7 1 e ‚àí01 1 . 7 5 5 6 6 0 e ‚àí01
[ 5 ] 8 . 6 2 2 9 3 0 e ‚àí02 3 . 4 0 3 3 8 7 e ‚àí02 1 . 1 2 4 4 2 3 e ‚àí02 3 . 1 9 8 4 2 1 e ‚àí03
[ 9 ] 7 . 9 9 6 0 5 3 e ‚àí04 1 . 7 8 4 7 6 3 e ‚àí04 3 . 6 0 1 1 1 5 e ‚àí05
> dnbinom ( y , s i z e =(a+sy2 ) , mu=(a+sy2 ) / ( b+n2 ) )
[ 1 ] 2 . 2 4 3 4 6 0 e ‚àí01 3 . 3 1 6 4 2 0 e ‚àí01 2 . 4 8 7 3 1 5 e ‚àí01 1 . 2 6 1 6 8 1 e ‚àí01
[ 5 ] 4 . 8 6 8 4 4 4 e ‚àí02 1 . 5 2 4 0 3 5 e ‚àí02 4 . 0 3 0 9 6 1 e ‚àí03 9 . 2 6 3 7 0 0 e ‚àí04
[ 9 ] 1 . 8 8 7 9 8 2 e ‚àí04 3 . 4 6 5 8 6 1 e ‚àí05 5 . 8 0 1 5 5 1 e ‚àí06

Notice that there is much more overlap between these two distributions than
Àú
between
the posterior
distributions of Œ∏1 and Œ∏2 . For
P
P
P example, Pr(
P Y1 >
Àú
Àú
Àú
Y2 | Yi,1 = 217, Yi,2 = 66) = .48 and Pr(Y1 = Y2 | Yi,1 = 217, Yi,2 =
66) = .22. The distinction between the events {Œ∏1 > Œ∏2 } and {YÀú1 > YÀú2 } is
extremely important: Strong evidence of a difference between two populations
does not mean that the difference itself is large.

3.3 Exponential families and conjugate priors

51

3.3 Exponential families and conjugate priors
The binomial and Poisson models discussed in this chapter are both instances of one-parameter exponential family models. A one-parameter exponential family model is any model whose densities can be expressed as
p(y|œÜ) = h(y)c(œÜ)eœÜt(y) , where œÜ is the unknown parameter and t(y) is the
sufficient statistic. Diaconis and Ylvisaker (1979) study conjugate prior distributions for general exponential family models, and in particular prior distributions of the form p(œÜ|n0 , t0 ) = Œ∫(n0 , t0 )c(œÜ)n0 en0 t0 œÜ . Combining such
prior information with information from Y1 , . . . , Yn ‚àº i.i.d. p(y|œÜ) gives the
following posterior distribution:
p(œÜ|y1 , . . . , yn ) ‚àù p(œÜ)p(y1 , . . . , yn |œÜ)
(
"
n0 +n

‚àù c(œÜ)

exp œÜ √ó n0 t0 +

n
X

#)
t(yi )

i=1

‚àù p(œÜ|n0 + n, n0 t0 + nt¬Ø(y)),
P
where t¬Ø(y) =
t(yi )/n. The similarity between the posterior and prior distributions suggests that n0 can be interpreted as a ‚Äúprior sample size‚Äù and t0
as a ‚Äúprior guess‚Äù of t(Y ). This interpretation can be made a bit more precise:
Diaconis and Ylvisaker (1979) show that
E[t(Y )] = E[ E[t(Y )|œÜ] ]
= E[‚àíc0 (œÜ)/c(œÜ)] = t0
(see also Exercise 3.6), so t0 represents the prior expected value of t(Y ). The
parameter n0 is a measure of how informative the prior is. There are a variety
of ways of quantifying this, but perhaps the simplest is to note that, as a function of œÜ, p(œÜ|n0 , t0 ) has the same shape as a likelihood
p(Àú
y1 , . . . , yÀún0 |œÜ) based
P
on n0 ‚Äúprior observations‚Äù yÀú1 , . . . , yÀún0 for which t(Àú
yi )/n0 = t0 . In this sense
the prior distribution p(œÜ|n0 , t0 ) contains the same amount of information that
would be obtained from n0 independent samples from the population.
Example: Binomial model
The exponential family representation of the binomial(Œ∏) model can be obtained from the density function for a single binary random variable:
p(y|Œ∏) = Œ∏y (1 ‚àí Œ∏)1‚àíy
y

Œ∏
(1 ‚àí Œ∏)
=
1‚àíŒ∏
= eœÜy (1 + eœÜ )‚àí1 ,
where œÜ = log[Œ∏/(1‚àíŒ∏)] is the log-odds. The conjugate prior for œÜ is thus given
by p(œÜ|n0 , t0 ) ‚àù (1 + eœÜ )‚àín0 en0 t0 œÜ , where t0 represents the prior expectation

52

3 One-parameter models

of t(y) = y, or equivalently, t0 represents our prior probability that Y = 1. Using the change of variables formula (Exercise 3.10), this translates into a prior
distribution for Œ∏ such that p(Œ∏|n0 , t0 ) ‚àù Œ∏n0 t0 ‚àí1 (1 ‚àí Œ∏)n0 (1‚àít0 )‚àí1 , which is a
beta(n0 t0 , n0 (1‚àít0 )) distribution. A weakly informative prior distribution can
be obtained by setting t0 equal to our prior expectation and n0 = 1. If our prior
expectation is 1/2, the resulting prior is a beta(1/2,1/2) distribution, which is
equivalent to Jeffreys‚Äô prior distribution (Exercise 3.11) for the binomial sampling model. Under the weakly informative beta(t0P
, (1‚àít0 )) prior distribution,
P
the posterior would be {Œ∏|y1 , . . . , yn } ‚àº beta(t0 + yi , (1 ‚àí t0 ) + (1 ‚àí yi )).
Example: Poisson model
The Poisson(Œ∏) model can be shown to be an exponential family model with
‚Ä¢ t(y) = y;
‚Ä¢ œÜ = log Œ∏;
‚Ä¢ c(œÜ) = exp(e‚àíœÜ ).
The conjugate prior distribution for œÜ is thus p(œÜ|n0 , t0 ) = exp(n0 e‚àíœÜ )en0 t0 y
where t0 is the prior expectation of the population mean of Y . This translates
into a prior density for Œ∏ of the form p(Œ∏|n0 , t0 ) ‚àù Œ∏n0 t0 ‚àí1 e‚àín0 Œ∏ , which is
a gamma(n0 t0 , n0 ) density. A weakly informative prior distribution can be
obtained with t0 set to the prior expectation of Y and n0 = 1, giving a
gamma(t0 , 1) prior distribution. The posterior
distribution under such a prior
P
would be {Œ∏|y1 , . . . , yn } ‚àº gamma(t0 + yi , 1 + n).

3.4 Discussion and further references
The notion of conjugacy for classes of prior distributions was developed in
Raiffa and Schlaifer (1961). Important results on conjugacy for exponential
families appear in Diaconis and Ylvisaker (1979) and Diaconis and Ylvisaker
(1985). The latter shows that any prior distribution may be approximated by
a mixture of conjugate priors.
Most authors refer to intervals of high posterior probability as ‚Äúcredible
intervals‚Äù as opposed to confidence intervals. Doing so fails to recognize that
Bayesian intervals do have frequentist coverage probabilities, often being very
close to the specified Bayesian coverage level (Welch and Peers, 1963; Hartigan, 1966; Severini, 1991). Some authors suggest that accurate frequentist
coverage can be a guide for the construction of prior distributions (Tibshirani,
1989; Sweeting, 1999, 2001). See also Kass and Wasserman (1996) for a review
of formal methods for selecting prior distributions.

4
Monte Carlo approximation

In the last chapter we saw examples in which a conjugate prior distribution for
an unknown parameter Œ∏ led to a posterior distribution for which there were
simple formulae for posterior means and variances. However, often we will
want to summarize other aspects of a posterior distribution. For example, we
may want to calculate Pr(Œ∏ ‚àà A|y1 , . . . , yn ) for arbitrary sets A. Alternatively,
we may be interested in means and standard deviations of some function of Œ∏,
or the predictive distribution of missing or unobserved data. When comparing
two or more populations we may be interested in the posterior distribution
of |Œ∏1 ‚àí Œ∏2 |, Œ∏1 /Œ∏2 , or max{Œ∏1 , . . . , Œ∏m }, all of which are functions of more
than one parameter. Obtaining exact values for these posterior quantities can
be difficult or impossible, but if we can generate random sample values of
the parameters from their posterior distributions, then all of these posterior
quantities of interest can be approximated to an arbitrary degree of precision
using the Monte Carlo method.

4.1 The Monte Carlo method
In the last chapter we obtained the following posterior distributions for
birthrates of women without and with bachelor‚Äôs degrees, respectively:
p(Œ∏1 |

111
X

Yi,1 = 217) = dgamma(Œ∏1 , 219, 112)

i=1

p(Œ∏2 |

44
X

Yi,2 = 66) = dgamma(Œ∏2 , 68, 45)

i=1

Additionally, we modeled Œ∏1 and Œ∏2 as
P conditionally
P independent given the
data. It was claimed that Pr(Œ∏1 > Œ∏2 | Yi,1 = 217, Yi,2 = 66) = 0.97. How
was this probability calculated? From Chapter 2, we have
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 4,
c Springer Science+Business Media, LLC 2009


54

4 Monte Carlo approximation

Pr(Œ∏1 > Œ∏2 |y1,1 , . . . , yn2 ,2 )
Z ‚àû Z Œ∏1
=
p(Œ∏1 , Œ∏2 |y1,1 , . . . , yn2 ,2 ) dŒ∏2 dŒ∏1
0

Z

0
‚àû

Z

Œ∏1

dgamma(Œ∏1 , 219, 112) √ó dgamma(Œ∏2 , 68, 45) dŒ∏2 dŒ∏1

=
0

=

0
219

112 4568
Œì (219)Œì (68)

Z
0

‚àû

Z

Œ∏1

Œ∏1218 Œ∏267 e‚àí112Œ∏1 ‚àí45Œ∏2 dŒ∏2 dŒ∏1 .

0

There are a variety of ways to calculate this integral. It can be done with
pencil and paper using results from calculus, and it can be calculated numerically in many mathematical software packages. However, the feasibility
of these integration methods depends heavily on the particular details of this
model, prior distribution and the probability statement that we are trying to
calculate. As an alternative, in this text we will use an integration method for
which the general principles and procedures remain relatively constant across
a broad class of problems. The method, known as Monte Carlo approximation, is based on random sampling and its implementation does not require a
deep knowledge of calculus or numerical analysis.
Let Œ∏ be a parameter of interest and let y1 , . . . , yn be the numerical values
of a sample from a distribution p(y1 , . . . , yn |Œ∏). Suppose we could sample some
number S of independent, random Œ∏-values from the posterior distribution
p(Œ∏|y1 , . . . , yn ):
Œ∏(1) , . . . , Œ∏(S) ‚àº i.i.d p(Œ∏|y1 , . . . , yn ).
Then the empirical distribution of the samples {Œ∏(1) , . . . , Œ∏(S) } would approximate p(Œ∏|y1 , . . . , yn ), with the approximation improving with increasing S.
The empirical distribution of {Œ∏(1) , . . . , Œ∏(S) } is known as a Monte Carlo approximation to p(Œ∏|y1 , . . . , yn ). Many computer languages and computing environments have procedures for simulating this sampling process. For example,
R has built-in functions to simulate i.i.d. samples from most of the distributions we will use in this book.
Figure 4.1 shows successive Monte Carlo approximations to the density
of the gamma(68,45) distribution, along with the true density function for
comparison. As we see, the empirical distribution of the Monte Carlo samples
provides an increasingly close approximation to the true density as S gets
larger. Additionally, let g(Œ∏) be (just about) any function. The law of large
numbers says that if Œ∏(1) , . . . , Œ∏(S) are i.i.d. samples from p(Œ∏|y1 , . . . , yn ), then
Z
S
1X
g(Œ∏(s) ) ‚Üí E[g(Œ∏)|y1 , . . . , yn ] = g(Œ∏)p(Œ∏|y1 , . . . , yn ) dŒ∏ as S ‚Üí ‚àû .
S s=1
This implies that as S ‚Üí ‚àû,
PS
‚Ä¢ Œ∏¬Ø = s=1 Œ∏(s) /S ‚Üí E[Œ∏|y1 , . . . , yn ];
PS
(s)
¬Ø 2 /(S ‚àí 1) ‚Üí Var[Œ∏|y1 , . . . , yn ];
‚Ä¢
‚àí Œ∏)
s=1 (Œ∏

4.1 The Monte Carlo method

1.0

1.5
Œ∏

2.0

1.5

2.0

1.0

1.5
Œ∏

2.0

1.0

1.5

2.0

1.0

1.5
Œ∏

2.0

1.0
0.0

1.0
0.0

1.0
0.0

2.0
1.0

1.0

2.0

2.0

2.0

1.5

2.0

1.0

S=1000

0.0

1.0

2.0

S=100

0.0

0.0

1.0

2.0

S =10

55

Fig. 4.1. Histograms and kernel density estimates of Monte Carlo approximations
to the gamma(68,45) distribution, with the true density in gray.

‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

#(Œ∏(s) ‚â§ c)/S ‚Üí Pr(Œ∏ ‚â§ c|y1 , . . . , yn );
the empirical distribution of {Œ∏(1) , . . . , Œ∏(S) } ‚Üí p(Œ∏|y1 , . . . , yn );
the median of {Œ∏(1) , . . . , Œ∏(S) } ‚Üí Œ∏1/2 ;
the Œ±-percentile of {Œ∏(1) , . . . , Œ∏(S) } ‚Üí Œ∏Œ± .

Just about any aspect of a posterior distribution we may be interested in can
be approximated arbitrarily exactly with a large enough Monte Carlo sample.
Numerical evaluation
We will first gain some familiarity and confidence with the Monte Carlo procedure by comparing its approximations to a few posterior quantities that
we can compute exactly (or nearly so) by other methods. Suppose we model
Y1 , . . . , Yn |Œ∏ as i.i.d. Poisson(Œ∏), and have a gamma(a, b) prior distribution for
Œ∏. Having
observed Y1 = y1 , . . . , Yn = yn , the posterior distribution is gamma
P
(a+ yi , b+n). ForP
the college-educated population in the birthrate example,
(a = 2, b = 1) and ( yi = 66, n = 44).
P
Expectation: The posterior mean is (a+ yi )/(b+n) = 68/45 = 1.51. Monte
Carlo approximations to this for S ‚àà {10, 100, 1000} can be obtained in R
as follows:
a<‚àí2
; b<‚àí1
sy <‚àí66 ; n<‚àí44
t h e t a . mc10<‚àírgamma ( 1 0 , a+sy , b+n )
t h e t a . mc100<‚àírgamma ( 1 0 0 , a+sy , b+n )
t h e t a . mc1000<‚àírgamma ( 1 0 0 0 , a+sy , b+n )

56

4 Monte Carlo approximation
> mean ( t h e t a . mc10 )
[ 1 ] 1.532794
> mean ( t h e t a . mc100 )
[ 1 ] 1.513947
> mean ( t h e t a . mc1000 )
[ 1 ] 1.501015

Results will vary depending on the seed of the random number generator.
Probabilities: The posterior probability that {Œ∏ < 1.75} can be obtained to a
high degree of precision in R with the command pgamma(1.75,a+sy,b+n) ,
which yields 0.8998. Using the simulated values of Œ∏ from above, the corresponding Monte Carlo approximations were:
> mean ( t h e t a . mc10 <1.75)
[ 1 ] 0.9
> mean ( t h e t a . mc100 <1.75)
[ 1 ] 0.94
> mean ( t h e t a . mc1000 <1.75)
[ 1 ] 0.899

Quantiles: A 95% quantile-based confidence region can be obtained with
qgamma(c(.025,.975),a+sy,b+n) , giving an interval of (1.173,1.891). Approximate 95% confidence regions can also be obtained from the Monte
Carlo samples:
> q u a n t i l e ( t h e t a . mc10 , c ( . 0 2 5 , . 9 7 5 ) )
2.5%
97.5%
1.260291 1.750068
> q u a n t i l e ( t h e t a . mc100 , c ( . 0 2 5 , . 9 7 5 ) )
2.5%
97.5%
1.231646 1.813752
> q u a n t i l e ( t h e t a . mc1000 , c ( . 0 2 5 , . 9 7 5 ) )
2.5%
97.5%
1.180194 1.892473

Figure 4.2 shows the convergence of the Monte Carlo estimates to the correct values graphically, based on cumulative estimates from a sequence of
S = 1000 samples from the gamma(68,45) distribution. Such plots can help
indicate when enough Monte Carlo samples have been made. Additionally,
Monte Carlo standard errors can be obtained
PSto assess the accuracy of approximations to posterior means: Letting Œ∏¬Ø = s=1 Œ∏(s) /S be the sample mean of
the Monte Carlo samples, the Central Limit Theorem says that Œ∏¬Ø is approximately normally p
distributed with expectation E[Œ∏|y1 , . . . , yn ] and standard deviation equal to Var[Œ∏|y1 , . . . yn ]/S. The Monte Carlo P
standard error is the
¬Ø 2 /(S ‚àí 1)
approximation to this standard deviation: Letting œÉ
ÀÜ 2 = (Œ∏(s) ‚àí Œ∏)
be the Monte
Carlo estimate of Var[Œ∏|y1 , . . . , yn ], the Monte Carlo standard
p
error is œÉ
ÀÜ 2 /S. An approximate 95% Monte Carlo confidence interval for the

cumulative cdf at 1.75
0.90
0.94
0.98

cumulative mean
1.45
1.50
1.40
0 200
600
1000
# of Monte Carlo samples

57

cumulative 97.5% quantile
1.4 1.5 1.6 1.7 1.8 1.9

4.2 Posterior inference for arbitrary functions

0 200
600
1000
# of Monte Carlo samples

0 200
600
1000
# of Monte Carlo samples

Fig. 4.2. Estimates of the posterior mean, Pr(Œ∏ < 1.75|y1 , . . . , yn ) and the 97.5%
posterior quantile as a function of the number of Monte Carlo samples. Horizontal
gray lines are the true values.

p
posterior mean of Œ∏ is Œ∏ÀÜ ¬± 2 œÉ
ÀÜ 2 /S. Standard practice is to choose S to be
large enough so that the Monte Carlo standard error is less than the precision to which you want to report E[Œ∏|y1 , . . . , yn ]. For example, suppose you
had generated a Monte Carlo sample of size S = 100 for which the estimate
of Var[Œ∏|y1 , . . . , p
yn ] was 0.024. The approximate Monte Carlo standard error
would then be 0.024/100 = 0.015. If you wanted the difference between
E[Œ∏|y1 , . . . , yn ] and its Monte Carlo estimate to be less than 0.01 with high
probability,
you would need to increase your Monte Carlo sample size so that
p
2 0.024/S < 0.01, i.e. S > 960.

4.2 Posterior inference for arbitrary functions
Suppose we are interested in the posterior distribution of some computable
function g(Œ∏) of Œ∏. In the binomial model, for example, we are sometimes
interested in the log odds:
log odds(Œ∏) = log

Œ∏
=Œ≥.
1‚àíŒ∏

The law of large numbers says that if we generate a sequence {Œ∏(1) , Œ∏(2) , . . .}
Œ∏ (s)
from the posterior distribution of Œ∏, then the average value of log 1‚àíŒ∏
(s) conŒ∏
verges to E[log 1‚àíŒ∏ |y1 , . . . , yn ]. However, we may also be interested in other
Œ∏
aspects of the posterior distribution of Œ≥ = log 1‚àíŒ∏
. Fortunately, these too can
be computed using a Monte Carlo approach:
Ô£º
sample Œ∏(1) ‚àº p(Œ∏|y1 , . . . , yn ), compute Œ≥ (1) = g(Œ∏(1) ) Ô£¥
Ô£¥
Ô£¥
sample Œ∏(2) ‚àº p(Œ∏|y1 , . . . , yn ), compute Œ≥ (2) = g(Œ∏(2) ) Ô£Ω
independently .
..
Ô£¥
.
Ô£¥
Ô£¥
Ô£æ
sample Œ∏(S) ‚àº p(Œ∏|y1 , . . . , yn ), compute Œ≥ (S) = g(Œ∏(S) )

58

4 Monte Carlo approximation

The sequence {Œ≥ (1) , . . . , Œ≥ (S) } constitutes S independent samples from p(Œ≥|y1 ,. . .,
yn ), and so as S ‚Üí ‚àû
PS
‚Ä¢ Œ≥¬Ø = s=1 Œ≥ (s) /S ‚Üí E[Œ≥|y1 , . . . , yn ],
PS
(s)
‚Ä¢
‚àí Œ≥¬Ø )2 /(S ‚àí 1) ‚Üí Var[Œ≥|y1 , . . . , yn ],
s=1 (Œ≥
‚Ä¢ the empirical distribution of {Œ≥ (1) , . . . , Œ≥ (S) } ‚Üí p(Œ≥|y1 , . . . , yn ),
as before.
Example: Log-odds
Fifty-four percent of the respondents in the 1998 General Social Survey reported their religious preference as Protestant, leaving non-Protestants in the
minority. Respondents were also asked if they agreed with a Supreme Court
ruling that prohibited state or local governments from requiring the reading
of religious texts in public schools. Of the n = 860 individuals in the religious
minority (non-Protestant), y = 441 (51%) said they agreed with the Supreme
Court ruling, whereas 353 of the 1011 Protestants (35%) agreed with the
ruling.
Let Œ∏ be the population proportion agreeing with the ruling in the minority
population. Using a binomial sampling model and a uniform prior distribution,
the posterior distribution of Œ∏ is beta(442, 420). Using the Monte Carlo algorithm described above, we can obtain samples of the log-odds Œ≥ = log[Œ∏/(1‚àíŒ∏)]
from both the prior distribution and the posterior distribution of Œ≥. In R, the
Monte Carlo algorithm involves only a few commands:
a<‚àí1 ; b<‚àí1
t h e t a . p r i o r . mc<‚àír b e t a ( 1 0 0 0 0 , a , b )
gamma . p r i o r . mc<‚àí l o g ( t h e t a . p r i o r . mc/(1‚àí t h e t a . p r i o r . mc) )
n0<‚àí860‚àí441 ; n1<‚àí441
t h e t a . p o s t . mc<‚àír b e t a ( 1 0 0 0 0 , a+n1 , b+n0 )
gamma . p o s t . mc<‚àí l o g ( t h e t a . p o s t . mc/(1‚àí t h e t a . p o s t . mc) )

Using the density() function in R , we can plot smooth kernel density approximations to these distributions, as shown in Figure 4.3.
Example: Functions of two parameters
Based on the prior distributions and the data in the birthrate example, the
posterior distributions for the two educational groups are
{Œ∏1 |y1,1 , . . . , yn1 ,1 } ‚àº gamma(219, 112) (women without bachelor‚Äôs degrees)
{Œ∏2 |y1,2 , . . . , yn2 ,2 } ‚àº gamma(68, 45) (women with bachelor‚Äôs degrees).
There are a variety of ways to describe our knowledge about the difference
between Œ∏1 and Œ∏2 . For example, we may be interested in the numerical value

59

0

0.00

1

0.05

p (Œ≥)
0.10 0.15

p( Œ≥|y1...yn)
2
3
4

0.20

5

4.2 Posterior inference for arbitrary functions

‚àí4

‚àí2

0
Œ≥

2

4

‚àí4

‚àí2

0
Œ≥

2

4

Fig. 4.3. Monte Carlo approximations to the prior and posterior distributions of
the log-odds.

of Pr(Œ∏1 > Œ∏2 |Y1,1 = y1,1 , . . . , Yn2 ,2 = yn2 ,2 ), or in the posterior distribution of
Œ∏1 /Œ∏2 . Both of these quantities can be obtained with Monte Carlo sampling:
P111
P44
(1)
(1)
sample Œ∏1 ‚àº p(Œ∏1 | i=1 Yi,1 = 217), sample Œ∏2 ‚àº p(Œ∏2 | i=1 Yi,2 = 66)
P
P44
(2)
(2)
111
sample Œ∏1 ‚àº p(Œ∏1 | i=1 Yi,1 = 217), sample Œ∏2 ‚àº p(Œ∏2 | i=1 Yi,2 = 66)
..
..
.
.
P
P44
(S)
(S)
111
sample Œ∏1 ‚àº p(Œ∏1 | i=1 Yi,1 = 217), sample Œ∏2 ‚àº p(Œ∏2 | i=1 Yi,2 = 66) .
(1)

(1)

(S)

(S)

The sequence {(Œ∏1 , Œ∏2 ), . . . , (Œ∏1 , Œ∏2 )} consists of S independent samples from the joint posterior distribution of Œ∏1 and Œ∏2 , and can be used to
make Monte Carlo approximations
to P
posterior quantities of interest. For
P111
44
example, Pr(Œ∏1 > Œ∏2 | i=1 Yi,1 = 217, i=1 Yi,2 = 66) is approximated by
P
(s)
(s)
S
1
s=1 1(Œ∏1 > Œ∏2 ), where 1(x > y) is the indicator function which is 1 if
S
x > y and zero otherwise. The approximation can be calculated in R with the
following commands:
> a<‚àí2 ; b<‚àí1
> sy1 <‚àí217 ; n1<‚àí111
> sy2 <‚àí66 ; n2<‚àí44
> t h e t a 1 . mc<‚àírgamma ( 1 0 0 0 0 , a+sy1 , b+n1 )
> t h e t a 2 . mc<‚àírgamma ( 1 0 0 0 0 , a+sy2 , b+n2 )
> mean ( t h e t a 1 . mc>t h e t a 2 . mc)
[ 1 ] 0.9708

60

4 Monte Carlo approximation

0.0

0.5

p( Œ≥|y1,y2)
1.0
1.5

2.0

Additionally, if we were interested in the ratio of the means of the two groups,
(1) (1)
(S) (S)
we could use the empirical distribution of {Œ∏1 /Œ∏2 , . . . , Œ∏1 /Œ∏2 } to approximate the posterior distribution of Œ∏1 /Œ∏2 . A Monte Carlo estimate of this
posterior density is given in Figure 4.4.

1.0

1.5
Œ≥ = Œ∏1 Œ∏2

2.0

Fig. 4.4. Monte Carlo estimate to the posterior predictive distribution of Œ≥ = Œ∏1 /Œ∏2 .

4.3 Sampling from predictive distributions
As described in Section 3.1, the predictive distribution of a random variable
YÀú is a probability distribution for YÀú such that
‚Ä¢ known quantities have been conditioned on;
‚Ä¢ unknown quantities have been integrated out.
For example, let YÀú be the number of children of a person who is sampled from
the population of women aged 40 with a college degree. If we knew the true
mean birthrate Œ∏ of this population, we might describe our uncertainty about
YÀú with a Poisson(Œ∏) distribution:
Sampling model:

Pr(YÀú = yÀú|Œ∏) = p(Àú
y |Œ∏) = Œ∏yÀúe‚àíŒ∏ /Àú
y!

We cannot make predictions from this model, however, because we do not
actually know Œ∏. If we did not have any sample data from the population, our
predictive distribution would be obtained by integrating out Œ∏:
R
Predictive model: Pr(YÀú = yÀú) = p(Àú
y |Œ∏)p(Œ∏)dŒ∏
In the case where Œ∏ ‚àº gamma(a, b), we showed in the last chapter that this
predictive distribution is the negative binomial(a, b) distribution. A predictive

4.3 Sampling from predictive distributions

61

distribution that integrates over unknown parameters but is not conditional
on observed data is called a prior predictive distribution. Such a distribution
can be useful in evaluating if a prior distribution for Œ∏ actually translates
into reasonable prior beliefs for observable data YÀú (see Exercise 7.4). After we
have observed a sample Y1 , . . . , Yn from the population, the relevant predictive
distribution for a new observation becomes
Z
Àú
Pr(Y = yÀú|Y1 = y1 , . . . , Yn = yn ) = p(Àú
y |Œ∏, y1 , . . . , yn )p(Œ∏|y1 , . . . , yn ) dŒ∏
Z
= p(Àú
y |Œ∏)p(Œ∏|y1 , . . . , yn ) dŒ∏.
This is called a posterior predictive distribution, because it conditions on an
observed dataset. In the case of a Poisson model with a gamma prior distribution, we showed in Chapter
3 that the posterior predictive distribution is
P
negative binomial(a + yi , b + n).
In many modeling situations, we will be able to sample from p(Œ∏|y1 , . . . , yn )
and p(y|Œ∏), but p(Àú
y |y1 , . . . , yn ) will be too complicated to sample from directly. In this situation we can sample from the posterior predictive distribution
indirectly using a Monte Carlo procedure. Since p(Àú
y |y1 , . . . , yn ) =
R
p(Àú
y |Œ∏)p(Œ∏|y1 , . . . , yn ) dŒ∏, we see that p(Àú
y |y1 , . . . , yn ) is the posterior expectation of p(Àú
y |Œ∏). To obtain the posterior predictive probability that YÀú is equal
to some specific value yÀú, we could just apply the Monte Carlo method of
the previous section: Sample Œ∏(1) , . . . , Œ∏(S) ‚àº i.i.d. p(Œ∏|y1 , . . . , yn ), and then
PS
approximate p(Àú
y |y1 , . . . , yn ) with s=1 p(Àú
y |Œ∏(s) )/S. This procedure will work
well if p(y|Œ∏) is discrete and we are interested in quantities that are easily computed from p(y|Œ∏). However, it will generally be useful to have a set of samples
of YÀú from its posterior predictive distribution. Obtaining these samples can
be done quite easily as follows:
sample Œ∏(1) ‚àº p(Œ∏|y1 , . . . , yn ), sample yÀú(1) ‚àº p(Àú
y |Œ∏(1) )
(2)
(2)
sample Œ∏ ‚àº p(Œ∏|y1 , . . . , yn ), sample yÀú ‚àº p(Àú
y |Œ∏(2) )
..
.
sample Œ∏(S) ‚àº p(Œ∏|y1 , . . . , yn ), sample yÀú(S) ‚àº p(Àú
y |Œ∏(S) ) .
The sequence {(Œ∏, yÀú)(1) , . . . , (Œ∏, yÀú)(S) } constitutes S independent samples from
the joint posterior distribution of (Œ∏, YÀú ), and the sequence {Àú
y (1) , . . . , yÀú(S) }
constitutes S independent samples from the marginal posterior distribution
of YÀú , which is the posterior predictive distribution.
Example: Poisson model
At the end of Chapter 3 it was reported that the predictive probability that
an age-40 woman without a college degree would have more children than an
age-40 woman with a degree was 0.48. To arrive at this answer exactly we
would have to do the following doubly infinite sum:

62

4 Monte Carlo approximation

Pr(YÀú1 > YÀú2 |
‚àû
X

X

‚àû
X

Yi,1 = 217,

X

Yi,2 = 66) =

dnbinom(Àú
y1 , 219, 112) √ó dnbinom(Àú
y2 , 68, 45) .

yÀú2 =0 yÀú1 =Àú
y2 +1

Alternatively, this sum can be approximated with Monte Carlo sampling.
Since YÀú1 and YÀú2 are a posteriori independent, samples from their joint posterior distribution can be made by sampling values of each variable separately
from their individual posterior distributions. Posterior predictive samples from
the conjugate Poisson model can be generated as follows:
P
sample Œ∏(1) ‚àº gamma(a + P yi , b + n), sample yÀú(1) ‚àº Poisson(Œ∏(1) )
sample Œ∏(2) ‚àº gamma(a + yi , b + n), sample yÀú(2) ‚àº Poisson(Œ∏(2) )
..
.
P
sample Œ∏(S) ‚àº gamma(a + yi , b + n) sample yÀú(S) ‚àº Poisson(Œ∏(S) ) .
Monte Carlo samples from the posterior predictive distributions of our two
educational groups can be obtained with just a few commands in R:
> a<‚àí2 ; b<‚àí1
> sy1 <‚àí217 ; n1<‚àí111
> sy2 <‚àí66 ; n2<‚àí44
>
>
>
>

t h e t a 1 . mc<‚àírgamma ( 1 0 0 0 0 , a+sy1 , b+n1 )
t h e t a 2 . mc<‚àírgamma ( 1 0 0 0 0 , a+sy2 , b+n2 )
y1 . mc<‚àír p o i s ( 1 0 0 0 0 , t h e t a 1 . mc)
y2 . mc<‚àír p o i s ( 1 0 0 0 0 , t h e t a 2 . mc)

> mean ( y1 . mc>y2 . mc)
[ 1 ] 0.4823

Once we have generated these Monte Carlo samples from the posterior predictive distribution, we can use them again to calculate other posterior quantities
of interest. For example, Figure 4.5 shows the Monte Carlo approximation to
the posterior distribution of D = (YÀú1 ‚àíYÀú2 ), the difference in number of children
between two individuals, one sampled from each of the two groups.

4.4 Posterior predictive model checking
Let‚Äôs consider for the moment the sample of 40-year-old women without a
college degree. The empirical distribution of the number of children of these
women, along with the corresponding posterior predictive distribution, is
shown in the first panel of Figure 4.6. In this sample of n = 111 women,
the number of women with exactly two children is 38, which is twice the
number of women in the sample with one child. In contrast, this group‚Äôs posterior predictive distribution, shown in gray, suggests that the probability of

63

0

500

p(D|y1,y2)
1000
1500

2000

4.4 Posterior predictive model checking

‚àí10

‚àí5

0
~ ~
D = Y1 ‚àí Y2

5

10

Fig. 4.5. The posterior predictive distribution of D = YÀú1 ‚àí YÀú2 , the difference in
the number of children of two randomly sampled women, one from each of the two
educational populations.

0.0

0.00

0.10

0.5

Pr(Y i = y i )
0.20

1.0

0.30

empirical distribution
predictive distribution

1.5

sampling a woman with two children is slightly less probable than sampling
a woman with one (probabilities of 0.27 and 0.28, respectively). These two
distributions seem to be in conflict. If the observed data have twice as many
women with two children than one, why should we be predicting otherwise?

0

2

4
6
number of children

8

0.5

1.0

1.5
~
t(Y)

2.0

2.5

Fig. 4.6. Evaluation of model fit. The first panel shows the empirical and posterior
predictive distributions of the number of children of women without a bachelor‚Äôs
degree. The second panel shows the posterior predictive distribution of the empirical
odds of having two children versus one child in a dataset of size n = 111. The
observed odds are given in the short vertical line.

64

4 Monte Carlo approximation

One explanation for the large number of women in the sample with two
children is that it is a result of sampling variability: The empirical distribution
of sampled data does not generally match exactly the distribution of the
population from which the data were sampled, and in fact may look quite
different if the sample size is small. A smooth population distribution can
produce sample empirical distributions that are quite bumpy. In such cases,
having a predictive distribution that smoothes over the bumps of the empirical
distribution may be desirable.
An alternative explanation for the large number of women in the sample
with two children is that this is indeed a feature of the population, and the
data are correctly reflecting this feature. In contrast, the Poisson model is
unable to represent this feature of the population because there is no Poisson
distribution that has such a sharp peak at y = 2.
These explanations for the discrepancy between the empirical and predictive distributions can be assessed numerically with Monte Carlo simulation.
For every vector y of length n = 111, let t(y) be the ratio of the number
of 2‚Äôs in y to the number of 1‚Äôs, so for our observed data y obs , t(y obs ) = 2.
Now suppose we were to sample a different set of 111 women, obtaining a
data vector YÀú of length 111 recording their number of children. What sort
of values of t(YÀú ) would we expect? Monte Carlo samples from the posterior
predictive distribution of t(YÀú ) can be obtained with the following procedure
and R-code:
For each s ‚àà {1, . . . , S},
1. sample Œ∏(s) ‚àº p(Œ∏|Y = y obs )
(s)
(s)
(s)
2. sample YÀú
= (Àú
y1 , . . . , yÀún ) ‚àº i.i.d. p(y|Œ∏(s) )
(s)
3. compute t(s) = t(YÀú ).
a<‚àí2 ; b<‚àí1
t . mc<‚àíNULL
for ( s in 1:10000) {
t h e t a 1 <‚àírgamma ( 1 , a+sy1 , b+n1 )
y1 . mc<‚àír p o i s ( n1 , t h e t a 1 )
t . mc<‚àíc ( t . mc , sum ( y1 . mc==2)/sum ( y1 . mc==1))
}

In this Monte Carlo sampling scheme,
{Œ∏(1) , . . . , Œ∏(S) } are samples from the posterior distribution of Œ∏;
(1)
(S)
{YÀú , . . . , YÀú } are posterior predictive datasets, each of size n;
(1)
(S)
Àú
{t , . . . , t } are samples from the posterior predictive distribution of t(Y).
A Monte Carlo approximation to the distribution of t(YÀú ) is shown in the
second panel of Figure 4.6, with the observed value t(yobs ) indicated with a
short vertical line. Out of 10,000 Monte Carlo datasets, only about a half of
a percent had values of t(y) that equaled or exceeded t(y obs ). This indicates

4.5 Discussion and further references

65

that our Poisson model is flawed: It predicts that we would hardly ever see a
dataset that resembled our observed one in terms of t(y). If we were interested
in making inference on the true probability distribution ptrue (y) for each value
of y, then the Poisson model would be inadequate, and we would have to
consider a more complicated model (for example, a multinomial sampling
model). However, a simple Poisson model may suffice if we are interested only
in certain aspects of ptrue . For example, the predictive distribution generated
by the Poisson model will have a mean that approximates the true population
mean, even though ptrue may not be a Poisson distribution. Additionally, for
these data the sample mean and variance are similar, being 1.95 and 1.90
respectively, suggesting that the Poisson model can represent both the mean
and variance of the population.
In terms of data description, we should at least make sure that our model
generates predictive datasets YÀú that resemble the observed dataset in terms
of features that are of interest. If this condition is not met, we may want to
consider using a more complex model. However, an incorrect model can still
provide correct inference for some aspects of the true population (White, 1982;
Bunke and Milhaud, 1998; Kleijn and van der Vaart, 2006). For example, the
Poisson model provides consistent estimation of the population mean, as well
as accurate confidence intervals if the population mean is approximately equal
to the variance.

4.5 Discussion and further references
The use of Monte Carlo methods is widespread in statistics and science in
general. Rubinstein and Kroese (2008) cover Monte Carlo methods for a wide
variety of statistical problems, and Robert and Casella (2004) include more
coverage of Bayesian applications (and cover Markov chain Monte Carlo methods as well).
Using the posterior predictive distribution to assess model fit was suggested by Guttman (1967) and Rubin (1984), and is now common practice.
In some problems, it is useful to evaluate goodness-of-fit using functions that
depend on parameters as well as predicted data. This is discussed in Gelman
et al (1996) and more recently in Johnson (2007). These types of posterior
predictive checks have given rise to a notion of posterior predictive p-values,
which despite their name, do not generally share the same frequentist properties as p-values based on classical goodness-of-fit tests. This distinction is
discussed in Bayarri and Berger (2000), who also consider alternative types of
Bayesian goodness of fit probabilities to serve as a replacement for frequentist
p-values.

5
The normal model

Perhaps the most useful (or utilized) probability model for data analysis is the
normal distribution. There are several reasons for this, one being the central
limit theorem, and another being that the normal model is a simple model
with separate parameters for the population mean and variance - two quantities that are often of primary interest. In this chapter we discuss some of
the properties of the normal distribution, and show how to make posterior
inference on the population mean and variance parameters. We also compare
the sampling properties of the standard Bayesian estimator of the population
mean to those of the unbiased sample mean. Lastly, we discuss the appropriateness of the normal model when the underlying data are not normally
distributed.

5.1 The normal model
A random variable Y is said to be normally distributed with mean Œ∏ and
variance œÉ 2 > 0 if the density of Y is given by
p(y|Œ∏, œÉ 2 ) = ‚àö

1
2œÄœÉ 2

1

e‚àí 2 (

y‚àíŒ∏ 2
œÉ )

, ‚àí‚àû < y < ‚àû.

Figure 5.1 shows normal density curves for a few values of Œ∏ and œÉ 2 . Some
important things to remember about this distribution include that
‚Ä¢ the distribution is symmetric about Œ∏, and the mode, median and mean
are all equal to Œ∏;
‚Ä¢ about 95% of the population lies within two standard deviations of the
mean (more precisely, 1.96 standard deviations);
‚Ä¢ if X ‚àº normal(¬µ, œÑ 2 ), Y ‚àº normal(Œ∏, œÉ 2 ) and X and Y are independent,
then aX + bY ‚àº normal(a¬µ + bŒ∏, a2 œÑ 2 + b2 œÉ 2 );
‚Ä¢ the dnorm, rnorm, pnorm, and qnorm commands in R take the standard
deviation œÉ as their argument, not the variance œÉ 2 . Be very careful about
this when using R - confusing œÉ with œÉ 2 can drastically change your results.
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 5,
c Springer Science+Business Media, LLC 2009


68

5 The normal model

0.8

> dnorm
f u n c t i o n ( x , mean = 0 , sd = 1 , l o g = FALSE)
. I n t e r n a l ( dnorm ( x , mean , sd , l o g ) )

0.0

0.2

p(y|Œ∏
Œ∏,œÉ
œÉ2)
0.4

0.6

Œ∏ = 2,œÉ
œÉ2 = 0.25
Œ∏ = 5,œÉ
œÉ2 = 4
Œ∏ = 7,œÉ
œÉ2 = 1

0

2

4

6

8

10

y

Fig. 5.1. Some normal densities.

The importance of the normal distribution stems primarily from the central limit theorem, which says that under very general conditions, the sum (or
mean) of a set of random variables is approximately normally distributed. In
practice, this means that the normal sampling model will be appropriate for
data that result from the additive effects of a large number of factors.
Example: women‚Äôs height
A study of 1,100 English families from 1893 to 1898 gathered height data
on n = 1375 women over the age of 18. A histogram of these data is shown
in Figure 5.2. The sample mean of these data is y¬Ø = 63.75 and the sample
standard deviation is s = 2.62 inches. One explanation for the variability in
heights among these women is that the women were heterogeneous in terms of
a number of factors controlling human growth, such as genetics, diet, disease,
stress and so on. Variability in these factors among the women results in
variability in their heights. Letting yi be the height in inches of woman i, a
simple additive model for height might be
y1 = a + b √ó gene1 + c √ó diet1 + d √ó disease1 + ¬∑ ¬∑ ¬∑
y2 = a + b √ó gene2 + c √ó diet2 + d √ó disease2 + ¬∑ ¬∑ ¬∑
..
.
yn = a + b √ó genen + c √ó dietn + d √ó diseasen + ¬∑ ¬∑ ¬∑

5.2 Inference for the mean, conditional on the variance

69

0.00

0.05

0.10

0.15

where genei might denote the presence of a particular height-promoting gene,
dieti might measure some aspect of woman i‚Äôs diet, and diseasei might indicate
if woman i had ever had a particular disease. Of course, there may be a
large number of genes, diseases, dietary and other factors that contribute to a
woman‚Äôs height. If the effects of these factors are approximately additive, then
each height measurement yi is roughly equal to a linear combination of a large
number of terms. For such situations, the central limit theorem says that the
empirical distribution of y1 , . . . , yn will look like a normal distribution, and
so the normal model provides an appropriate sampling model for the data.

55

60

65
height in inches

70

Fig. 5.2. Height data and a normal density with Œ∏ = 63.75 and œÉ = 2.62.

5.2 Inference for the mean, conditional on the variance
Suppose our model is {Y1 , . . . , Yn |Œ∏, œÉ 2 } ‚àº i.i.d. normal (Œ∏, œÉ 2 ). Then the joint
sampling density is given by
p(y1 , . . . , yn |Œ∏, œÉ 2 ) =
=

n
Y
i=1
n
Y
i=1

p(yi |Œ∏, œÉ 2 )
‚àö

1
2œÄœÉ 2

‚àí 12

e



yi ‚àíŒ∏
œÉ

(

2 ‚àín/2

= (2œÄœÉ )

2

1X
exp ‚àí
2



yi ‚àí Œ∏
œÉ

2 )
.

Expanding the quadratic term in the exponent, we see that p(y1 , . . . , yn |Œ∏, œÉ 2 )
depends on y1 , . . . , yn through

70

5 The normal model

2
n 
X
yi ‚àí Œ∏
i=1

œÉ

=

1 X 2
Œ∏ X
Œ∏2
yi ‚àí 2 2
yi + n 2 .
2
œÉ
œÉ
œÉ

P
P
From this you can show that { yi2 , yi } make up a two-dimensional sufficient statistic. Knowing
the values of
knowing
P
Pthese quantities is equivalent to
the values of y¬Ø = yi /n and s2 = (yi ‚àí y¬Ø)2 /(n ‚àí 1), and so {¬Ø
y , s2 } are also
a sufficient statistic.
Inference for this two-parameter model can be broken down into two oneparameter problems. We will begin with the problem of making inference for
Œ∏ when œÉ 2 is known, and use a conjugate prior distribution for Œ∏. For any
(conditional) prior distribution p(Œ∏|œÉ 2 ), the posterior distribution will satisfy
1

p(Œ∏|y1 , . . . , yn , œÉ 2 ) ‚àù p(Œ∏|œÉ 2 ) √ó e‚àí 2œÉ2

P

(yi ‚àíŒ∏)2
2

‚àù p(Œ∏|œÉ 2 ) √ó ec1 (Œ∏‚àíc2 ) .
Recall that a class of prior distributions is conjugate for a sampling model if
the resulting posterior distribution is in the same class. From the calculation
above, we see that if p(Œ∏|œÉ 2 ) is to be conjugate, it must include quadratic
2
terms like ec1 (Œ∏‚àíc2 ) . The simplest such class of probability densities on R
is the normal family of densities, suggesting that if p(Œ∏|œÉ 2 ) is normal and
y1 , . . . , yn are i.i.d. normal(Œ∏, œÉ 2 ), then p(Œ∏|y1 , . . . , yn , œÉ 2 ) is also a normal
density. Let‚Äôs evaluate this claim: If Œ∏ ‚àº normal (¬µ0 , œÑ02 ), then
p(Œ∏|y1 , . . . , yn , œÉ 2 ) = p(Œ∏|œÉ 2 )p(y1 , . . . , yn |Œ∏, œÉ 2 )/p(y1 , . . . , yn |œÉ 2 )
‚àù p(Œ∏|œÉ 2 )p(y1 , . . . , yn |Œ∏, œÉ 2 )
1
1 X
‚àù exp{‚àí 2 (Œ∏ ‚àí ¬µ0 )2 } exp{‚àí 2
(yi ‚àí Œ∏)2 }.
2œÑ0
2œÉ
Adding the terms in the exponents and ignoring the -1/2 for the moment, we
have
X
1 2
1 X 2
(Œ∏ ‚àí 2Œ∏¬µ0 + ¬µ20 ) + 2 (
yi ‚àí 2Œ∏
yi + nŒ∏2 ) = aŒ∏2 ‚àí 2bŒ∏ + c, where
2
œÑ0
œÉ
P
1
n
¬µ0
yi
a = 2 + 2 , b = 2 + 2 , and c = c(¬µ0 , œÑ02 , œÉ 2 , y1 , . . . , yn ).
œÑ0
œÉ
œÑ0
œÉ
Now let‚Äôs see if p(Œ∏|œÉ 2 , y1 , . . . , yn ) takes the form of a normal density:
1
p(Œ∏|œÉ 2 , y1 , . . . , yn ) ‚àù exp{‚àí (aŒ∏2 ‚àí 2bŒ∏)}
2
1
1
= exp{‚àí a(Œ∏2 ‚àí 2bŒ∏/a + b2 /a2 ) + b2 /a}
2
2
1
2
‚àù exp{‚àí a(Œ∏ ‚àí b/a) }
( 2 
2 )
1 Œ∏ ‚àí b/a
‚àö
= exp ‚àí
.
2
1/ a

5.2 Inference for the mean, conditional on the variance

71

This
‚àö function has exactly the same shape as a normal density curve, with
1/ a playing the role of the standard deviation and b/a playing the role of
the mean. Since probability distributions are determined by their shape, this
means that p(Œ∏|œÉ 2 , y1 , . . . , yn ) is indeed a normal density. We refer to the mean
and variance of this density as ¬µn and œÑn2 , where
œÑn2

1
= =
a

1
œÑ02

1
+

n
œÉ2

and

b
¬µn = =
a

1
¬µ + œÉn2 y¬Ø
œÑ02 0
.
1
+ œÉn2
œÑ02

Combining information
The (conditional) posterior parameters œÑn2 and ¬µn combine the prior parameters œÑ02 and ¬µ0 with terms from the data.
‚Ä¢ Posterior variance and precision: The formula for 1/œÑn2 is
1
1
n
= 2 + 2,
2
œÑn
œÑ0
œÉ

(5.1)

and so the prior inverse variance is combined with the inverse of the data
variance. Inverse variance is often referred to as the precision. For the
normal model let,
œÉ
Àú 2 = 1/œÉ 2 = sampling precision, i.e. how close the yi ‚Äôs are to Œ∏;
œÑÀú02 = 1/œÑ02 = prior precision;
œÑÀún2 = 1/œÑn2 = posterior precision.
It is convenient to think about precision as the quantity of information on
an additive scale. For the normal model, Equation 5.1 implies that
œÑÀún2 = œÑÀú02 + nÀú
œÉ2 ,
and so posterior information = prior information + data information.
‚Ä¢ Posterior mean: Notice that
¬µn =

œÑÀú02

œÑÀú02
nÀú
œÉ2
¬µ0 + 2
y¬Ø,
2
+ nÀú
œÉ
œÑÀú0 + nÀú
œÉ2

and so the posterior mean is a weighted average of the prior mean and
the sample mean. The weight on the sample mean is n/œÉ 2 , the sampling
precision of the sample mean. The weight on the prior mean is 1/œÑ02 , the
prior precision. If the prior mean were based on Œ∫0 prior observations from
the same (or similar) population as Y1 , . . . , Yn , then we might want to set
œÑ02 = œÉ 2 /Œ∫0 , the variance of the mean of the prior observations. In this
case, the formula for the posterior mean reduces to
¬µn =

Œ∫0
n
¬µ0 +
y¬Ø.
Œ∫0 + n
Œ∫0 + n

72

5 The normal model

Prediction
Consider predicting a new observation YÀú from the population after having
observed (Y1 = y1 , . . . , Yn = yn ). To find the predictive distribution, let‚Äôs use
the following fact:
{YÀú |Œ∏, œÉ 2 } ‚àº normal(Œ∏, œÉ 2 ) ‚áî YÀú = Œ∏ + Àú, {Àú
|Œ∏, œÉ 2 } ‚àº normal(0, œÉ 2 ) .
In other words, saying that YÀú is normal with mean Œ∏ is the same as saying
YÀú is equal to Œ∏ plus some mean-zero normally distributed noise. Using this
result, let‚Äôs first compute the posterior mean and variance of YÀú :
E[YÀú |y1 , . . . , yn , œÉ 2 ] = E[Œ∏ + Àú|y1 , . . . , yn , œÉ 2 ]
= E[Œ∏|y1 , . . . , yn , œÉ 2 ] + E[Àú
|y1 , . . . , yn , œÉ 2 ]
= ¬µn + 0 = ¬µn
Var[YÀú |y1 , . . . , yn , œÉ 2 ] = Var[Œ∏ + Àú|y1 , . . . , yn , œÉ 2 ]
= Var[Œ∏|y1 , . . . , yn , œÉ 2 ] + Var[Àú
|y1 , . . . , yn , œÉ 2 ]
= œÑn2 + œÉ 2 .
Recall from the beginning of the chapter that the sum of independent normal
random variables is also normal. Therefore, since both Œ∏ and Àú, conditional on
y1 , . . . , yn and œÉ 2 , are normally distributed, so is YÀú = Œ∏ + Àú. The predictive
distribution is therefore
YÀú |œÉ 2 , y1 , . . . , yn ‚àº normal(¬µn , œÑn2 + œÉ 2 ) .
It is worthwhile to have some intuition about the form of the variance of YÀú : In
general, our uncertainty about a new sample YÀú is a function of our uncertainty
about the center of the population (œÑn2 ) as well as how variable the population
is (œÉ 2 ). As n ‚Üí ‚àû we become more and more certain about where Œ∏ is, and
the posterior variance œÑn2 of Œ∏ goes to zero. But certainty about Œ∏ does not
reduce the sampling variability œÉ 2 , and so our uncertainty about YÀú never goes
below œÉ 2 .
Example: Midge wing length
Grogan and Wirth (1981) provide data on the wing length in millimeters of
nine members of a species of midge (small, two-winged flies). From these nine
measurements we wish to make inference on the population mean Œ∏. Studies
from other populations suggest that wing lengths are typically around 1.9
mm, and so we set ¬µ0 = 1.9. We also know that lengths must be positive,
implying that Œ∏ > 0. Therefore, ideally we would use a prior distribution for
Œ∏ that has mass only on Œ∏ > 0. We can approximate this restriction with a
normal prior distribution for Œ∏ as follows: Since for any normal distribution

5.3 Joint inference for the mean and variance

73

most of the probability is within two standard deviations of the mean, we
choose œÑ02 so that ¬µ0 ‚àí 2 √ó œÑ0 > 0, or equivalently œÑ0 < 1.9/2 = 0.95. For now,
we take œÑ0 = 0.95, which somewhat overstates our prior uncertainty about Œ∏.
The observations in order of increasing magnitude are (1.64, 1.70, 1.72,
1.74, 1.82, 1.82, 1.82, 1.90, 2.08), giving y¬Ø = 1.804. Using the formulae above
for ¬µn and œÑn2 , we have {Œ∏|y1 , . . . , y9 , œÉ 2 } ‚àº normal (¬µn , œÑn2 ), where
¬µn =
œÑn2 =

1
¬µ + œÉn2 y¬Ø
œÑ02 0
1
+ œÉn2
œÑ02
1
œÑ02

1
+

n
œÉ2

=

=

1.11 √ó 1.9 + œÉ92 1.804
1.11 + œÉ92

1
1.11 +

9
œÉ2

.

0

p( Œ∏|y 1...y n ,œÉ
œÉ2 = 0.017)
2
4
6
8

If œÉ 2 = s2 = 0.017, then {Œ∏|y1 , . . . , y9 , œÉ 2 = 0.017} ‚àº normal (1.805, 0.002).
A 95% quantile-based confidence interval for Œ∏ based on this distribution is
(1.72, 1.89). However, this interval assumes that we are certain that œÉ 2 = s2 ,
when in fact s2 is only a rough estimate of œÉ 2 based on only nine observations.
To get a more accurate representation of our information we need to account
for the fact that œÉ 2 is not known.

0

1

2

3

Œ∏

Fig. 5.3. Prior and conditional posterior distributions for the population mean wing
length in the midge example.

5.3 Joint inference for the mean and variance
Bayesian inference for two or more unknown parameters is not conceptually
different from the one-parameter case. For any joint prior distribution p(Œ∏, œÉ 2 )
for Œ∏ and œÉ 2 , posterior inference proceeds using Bayes‚Äô rule:

74

5 The normal model

p(Œ∏, œÉ 2 |y1 , . . . , yn ) = p(y1 , . . . , yn |Œ∏, œÉ 2 )p(Œ∏, œÉ 2 )/p(y1 , . . . , yn ) .
As before, we will begin by developing a simple conjugate class of prior distributions which make posterior calculations easy.
Recall from our axioms of probability that a joint distribution for two
quantities can be expressed as the product of a conditional probability and a
marginal probability:
p(Œ∏, œÉ 2 ) = p(Œ∏|œÉ 2 )p(œÉ 2 ) .
In the last section, we saw that if œÉ 2 were known, then a conjugate prior
distribution for Œ∏ was normal(¬µ0 , œÑ02 ). Let‚Äôs consider the particular case in
which œÑ02 = œÉ 2 /Œ∫0 :
‚àö
p(Œ∏, œÉ 2 ) = p(Œ∏|œÉ 2 )p(œÉ 2 ) = dnorm(Œ∏, ¬µ0 , œÑ0 = œÉ/ Œ∫0 ) √ó p(œÉ 2 ) .
In this case, the parameters ¬µ0 and Œ∫0 can be interpreted as the mean and
sample size from a set of prior observations.
For œÉ 2 we need a family of prior distributions that has support on (0, ‚àû).
One such family of distributions is the gamma family, as we used for the
Poisson sampling model. Unfortunately, this family is not conjugate for the
normal variance. However, the gamma family does turn out to be a conjugate
class of densities for 1/œÉ 2 (the precision). When using such a prior distribution
we say that œÉ 2 has an inverse-gamma distribution:
precision = 1/œÉ 2 ‚àº gamma(a, b)
variance = œÉ 2 ‚àº inverse-gamma(a, b)
For interpretability later on, instead of using a and b we will parameterize this
prior distribution as
1/œÉ 2 ‚àº gamma (

ŒΩ0 ŒΩ0 2
, œÉ ).
2 2 0

Under this parameterization,
0 /2
‚Ä¢ E[œÉ 2 ] = œÉ02 ŒΩ0ŒΩ/2‚àí1
;
0 /2
‚Ä¢ mode[œÉ 2 ] = œÉ02 ŒΩ0ŒΩ/2+1
,
so mode[œÉ 2 ] < œÉ02 < E[œÉ 2 ];
‚Ä¢ Var[œÉ 2 ] is decreasing in ŒΩ0 .

As we will see in a moment, we can interpret the prior parameters (œÉ02 , ŒΩ0 ) as
the sample variance and sample size of prior observations.
Posterior inference
Suppose our prior distributions and sampling model are as follows:
1/œÉ 2 ‚àº gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2)
Œ∏|œÉ 2 ‚àº normal(¬µ0 , œÉ 2 /Œ∫0 )
Y1 , . . . , Yn |Œ∏, œÉ 2 ‚àº i.i.d. normal (Œ∏, œÉ 2 ) .

5.3 Joint inference for the mean and variance

75

Just as the prior distribution for Œ∏ and œÉ 2 can be decomposed as p(Œ∏, œÉ 2 ) =
p(Œ∏|œÉ 2 )p(œÉ 2 ), the posterior distribution can be similarly decomposed:
p(Œ∏, œÉ 2 |y1 , . . . , yn ) = p(Œ∏|œÉ 2 , y1 , . . . , yn )p(œÉ 2 |y1 , . . . , yn ) .
The conditional distribution of Œ∏ given the data and œÉ 2 can be obtained using
the results of the previous section: Plugging in œÉ 2 /Œ∫0 for œÑ02 , we have
{Œ∏|y1 , . . . , yn , œÉ 2 } ‚àº normal(¬µn , œÉ 2 /Œ∫n ), where
(Œ∫0 /œÉ 2 )¬µ0 + (n/œÉ 2 )¬Ø
y
Œ∫0 ¬µ0 + n¬Ø
y
Œ∫n = Œ∫0 + n and ¬µn =
=
.
Œ∫0 /œÉ 2 + n/œÉ 2
Œ∫n
Therefore, if ¬µ0 is the mean of Œ∫0 prior observations, then E[Œ∏|y1 , . . . , yn , œÉ 2 ] is
the sample mean of the current and prior observations, and Var[Œ∏|y1 , . . . , yn , œÉ 2 ]
is œÉ 2 divided by the total number of observations, both prior and current.
The posterior distribution of œÉ 2 can be obtained by performing an integration over the unknown value of Œ∏:
p(œÉ 2 |y1 , . . . , yn ) ‚àù p(œÉ 2 )p(y1 , . . . , yn |œÉ 2 )
Z
= p(œÉ 2 ) p(y1 , . . . , yn |Œ∏, œÉ 2 )p(Œ∏|œÉ 2 ) dŒ∏ .
This integral can be done without much knowledge of calculus, but it is somewhat tedious and is left as an exercise (Exercise 5.3). The result is that
{1/œÉ 2 |y1 , . . . , yn } ‚àº gamma(ŒΩn /2, ŒΩn œÉn2 /2), where
ŒΩn = ŒΩ0 + n
1
Œ∫0 n
œÉn2 =
[ŒΩ0 œÉ02 + (n ‚àí 1)s2 +
(¬Ø
y ‚àí ¬µ0 )2 ].
ŒΩn
Œ∫n
These formulae suggest an interpretation of ŒΩ0 as a prior sample size, from
2
which
that s2 =
Pn a prior2 sample variance of œÉ0 has been obtained. Recall
2
¬Ø) /(n ‚àí 1) is the sample variance, and (n ‚àí 1)s is the sum of
i=1 (yi ‚àí y
squared observations from the sample mean, which is often called the ‚Äúsum
of squares.‚Äù Similarly, we can think of ŒΩ0 œÉ02 and ŒΩn œÉn2 as prior and posterior
sums of squares, respectively. Multiplying both sides of the last equation by
ŒΩn almost gives us ‚Äúposterior sum of squares equals prior sum of squares plus
data sum of squares.‚Äù However, the third term in the last equation is a bit
harder to understand - it says that a large value of (¬Ø
y ‚àí ¬µ0 )2 increases the
2
posterior probability of a large œÉ . This makes sense for our particular joint
prior distribution for Œ∏ and œÉ 2 : If we want to think of ¬µ0 as the sample mean of
n
(¬Ø
y ‚àí¬µ0 )2 is an estimate of œÉ 2
Œ∫0 prior observations with variance œÉ 2 , then Œ∫Œ∫00+n
and so we want to use the information that this term provides. For situations
in which ¬µ0 should not be thought of as the mean of prior observations, we
will develop an alternative prior distribution in the next chapter.

76

5 The normal model

Example
Returning to the midge data, studies of other populations suggest that the true
mean and standard deviation of our population under study should not be too
far from 1.9 mm and 0.1 mm respectively, suggesting ¬µ0 = 1.9 and œÉ02 = 0.01.
However, this population may be different from the others in terms of wing
length, and so we choose Œ∫0 = ŒΩ0 = 1 so that our prior distributions are only
weakly centered around these estimates from other populations.
The sample mean and variance of our observed data are y¬Ø = 1.804 and
s2 = 0.0169 (s = 0.130). From these values and the prior parameters, we
compute ¬µn and œÉn2 :
Œ∫0 ¬µ0 + n¬Ø
y
1.9 + 9 √ó 1.804
=
= 1.814
Œ∫n
1+9
1
Œ∫0 n
œÉn2 =
[ŒΩ0 œÉ02 + (n ‚àí 1)s2 +
(¬Ø
y ‚àí ¬µ0 )2 ]
ŒΩn
Œ∫n
0.010 + 0.135 + 0.008
=
= 0.015 .
10

¬µn =

These calculations can be done with the following commands in R:
# prior
mu0<‚àí1.9 ; k0<‚àí1
s20 <‚àí.010 ; nu0<‚àí1
# data
y<‚àíc ( 1 . 6 4 , 1 . 7 0 , 1 . 7 2 , 1 . 7 4 , 1 . 8 2 , 1 . 8 2 , 1 . 8 2 , 1 . 9 0 , 2 . 0 8 )
n<‚àíl e n g t h ( y ) ; ybar<‚àímean ( y ) ; s2<‚àívar ( y )
# posterior inference
kn<‚àík0+n ; nun<‚àínu0+n
mun<‚àí ( k0 ‚àómu0 + n‚àó ybar ) / kn
s2n<‚àí ( nu0‚àó s 2 0 +(n‚àí1)‚àó s 2 +k0 ‚àón ‚àó ( ybar‚àímu0 ) ÀÜ 2 / ( kn ) ) / ( nun )
> mun
[ 1 ] 1.814
> s2n
[ 1 ] 0.015324
> s q r t ( s2n )
[ 1 ] 0.1237901

Our joint posterior distribution is completely determined by the values ¬µn =
1.814, Œ∫n = 10, œÉn2 = 0.015, ŒΩn = 10, and can be expressed as
{Œ∏|y1 , . . . , yn , œÉ 2 } ‚àº normal(1.814, œÉ 2 /10),
{1/œÉ 2 |y1 , . . . , yn } ‚àº gamma(10/2, 10 √ó 0.015/2).
Letting œÉ
Àú 2 = 1/œÉ 2 , a contour plot of the bivariate posterior density of (Œ∏, œÉ
Àú2)
appears in the first panel of Figure 5.4. This plot was obtained by computing

5.3 Joint inference for the mean and variance

77

20

40

0.01

60

œÉ2
0.02

~2
œÉ
80 100

0.03

140

0.04

p
dnorm(Œ∏k , ¬µn , 1/ 10Àú
œÉl2 ) √ó dgamma(Àú
œÉl2 , 10/2, 10œÉn2 /2) for each pair of values
2
(Œ∏k , œÉ
Àúl ) on a grid. Similarly, the second panel plots the joint posterior density
of (Œ∏, œÉ 2 ). Notice that the contours are more peaked as a function of Œ∏ for low
values of œÉ 2 than high values.

1.6

1.7

1.8
Œ∏

1.9

2.0

1.6

1.7

1.8
Œ∏

1.9

2.0

Fig. 5.4. Joint posterior distributions of (Œ∏, œÉ
Àú 2 ) and (Œ∏, œÉ 2 ).

Monte Carlo sampling
For many data analyses, interest primarily lies in estimating the population mean Œ∏, and so we would like to calculate quantities like E[Œ∏|y1 , . . . , yn ],
sd[Œ∏|y1 , . . . , yn ], Pr(Œ∏1 < Œ∏2 | y1,1 ,. . ., yn2 ,2 ), and so on. These quantities are
all determined by the marginal posterior distribution of Œ∏ given the data. But
all we know (so far) is that the conditional distribution of Œ∏ given the data
and œÉ 2 is normal, and that œÉ 2 given the data is inverse-gamma. If we could
generate marginal samples of Œ∏, from p(Œ∏|y1 , . . . , yn ), then we could use the
Monte Carlo method to approximate the above quantities of interest. It turns
out that this is quite easy to do by generating samples of Œ∏ and œÉ 2 from their
joint posterior distribution. Consider simulating parameter values using the
following Monte Carlo procedure:
œÉ 2(1) ‚àº inverse gamma(ŒΩn /2, œÉn2 ŒΩn /2),
..
.

Œ∏(1) ‚àº normal(¬µn , œÉ 2(1) /Œ∫n )
..
.

œÉ 2(S) ‚àº inverse gamma(ŒΩn /2, œÉn2 ŒΩn /2),

Œ∏(S) ‚àº normal(¬µn , œÉ 2(S) /Œ∫n ) .

Note that each Œ∏(s) is sampled from its conditional distribution given the data
and œÉ 2 = œÉ 2(s) . This Monte Carlo procedure can be implemented in R with
only two lines of code:

78

5 The normal model

s 2 . p o s t s a m p l e <‚àí 1/rgamma ( 1 0 0 0 0 , nun / 2 , s2n ‚àónun /2 )
t h e t a . p o s t s a m p l e <‚àí rnorm ( 1 0 0 0 0 , mun , s q r t ( s 2 . p o s t s a m p l e /kn ) )

A sequence of pairs {(œÉ 2(1) , Œ∏(1) ), . . . , (œÉ 2(S) , Œ∏(S) )} simulated using this procedure are independent samples from the joint posterior distribution of
p(Œ∏, œÉ 2 |y1 , . . . , yn ). Additionally, the simulated sequence {Œ∏(1) , . . . , Œ∏(S) } can
be seen as independent samples from the marginal posterior distribution of
p(Œ∏|y1 , . . . , yn ), and so we use this sequence to make Monte Carlo approximations to functions involving p(Œ∏|y1 , . . . , yn ), as described in Chapter 4. It
may seem confusing that each Œ∏(s) -value is referred to both as a sample from
the conditional posterior distribution of Œ∏ given œÉ 2 and as a sample from
the marginal posterior distribution of Œ∏ given only the data. To alleviate this
confusion, keep in mind that while Œ∏(1) , . . . , Œ∏(S) are indeed each conditional
samples, they are each conditional on different values of œÉ 2 . Taken together,
they constitute marginal samples of Œ∏.
Figure 5.5 shows samples from the joint posterior distribution of (Œ∏, œÉ 2 ),
as well as kernel density estimates of the marginal posterior distributions.
Any posterior quantities of interest can be approximated from these Monte
Carlo samples. For example, a 95% confidence interval can be obtained in R
with quantile(theta.postsample,c (.025,.975)) , which gives an interval of (1.73,
1.90). This is extremely close to (1.70, 1.90), a frequentist 95% confidence
interval obtained from the t-test. There is a reason for this: It turns out that
p(Œ∏|y1 , . . . , yn ), the marginal posterior distribution of Œ∏, can be obtained in a
closed form. From this form, it can be shown that the posterior distribution
)
‚àön , given y
of t(Œ∏) = œÉ(Œ∏‚àí¬µ
¬Ø and s2 , has a t-distribution with ŒΩ0 + n degrees of
n / Œ∫n
freedom. If Œ∫0 and ŒΩ0 are small, then the posterior distribution of t(Œ∏) will be
very close to the tn‚àí1 distribution. How small can Œ∫0 and ŒΩ0 be?
Improper priors
What if you want to ‚Äúbe Bayesian‚Äù so you can talk about things like Pr(Œ∏ <
c|y1 , . . . , yn ) but want to ‚Äúbe objective‚Äù by not using any prior information?
Since we have referred to Œ∫0 and ŒΩ0 as prior sample sizes, it might seem that
the smaller these parameters are, the more objective the estimates will be. So
it is natural to wonder what happens to the posterior distribution as Œ∫0 and
ŒΩ0 get smaller and smaller. The formulae for ¬µn and œÉn2 are
Œ∫0 ¬µ0 + n¬Ø
y
Œ∫0 + n
1
Œ∫0 n
œÉn2 =
[ŒΩ0 œÉ02 + (n ‚àí 1)s2 +
(¬Ø
y ‚àí ¬µ0 )2 ],
ŒΩ0 + n
Œ∫0 + n

¬µn =

and so as Œ∫0 , ŒΩ0 ‚Üí 0,
¬µn ‚Üí y¬Ø, and
n‚àí1 2
1X
œÉn2 ‚Üí
s =
(yi ‚àí y¬Ø)2 .
n
n

5.4 Bias, variance and mean squared error

79

This has led some to suggest the following ‚Äúposterior distribution‚Äù:
n n1X
{1/œÉ 2 |y1 , . . . , yn } ‚àº gamma( ,
(yi ‚àí y¬Ø)2 )
2 2n
œÉ2
{Œ∏|œÉ 2 , y1 , . . . , yn } ‚àº normal(¬Ø
y, ) .
n
Somewhat more formally, if we let pÀú(Œ∏, œÉ 2 ) = 1/œÉ 2 (which is not a probability
2
density) and set p(Œ∏, œÉ 2 |y) ‚àù p(y|Œ∏, œÉ 2 ) √ó pÀú(Œ∏, œÉP
), we get the same ‚Äúcondin‚àí1 1
tional distribution‚Äù for Œ∏ but a gamma( 2 , 2 (yi ‚àí y¬Ø)2 ) distribution for
1/œÉ 2 (Gelman et al (2004), Chapter 3). You can integrate this latter joint
distribution over œÉ 2 to show that
Œ∏ ‚àí y¬Ø
‚àö |y1 , . . . , yn ‚àº tn‚àí1 .
s/ n
It is interesting to compare this result to the sampling distribution of the
t-statistic, conditional on Œ∏ but unconditional on the data:
Y¬Ø ‚àí Œ∏
‚àö |Œ∏ ‚àº tn‚àí1 .
s/ n
The second statement says that, before you sample the data, your uncertainty
about the scaled deviation of the sample mean Y¬Ø from the population mean Œ∏
is represented with a tn‚àí1 distribution. The first statement says that after you
sample your data, your uncertainty is still represented with a tn‚àí1 distribution.
The difference is that before you sample your data, both Y¬Ø and Œ∏ are unknown.
After you sample your data, then Y¬Ø = y¬Ø is known and this provides us with
information about Œ∏.
There are no proper prior probability distributions on (Œ∏, œÉ 2 ) that will lead
to the above tn‚àí1 posterior distribution for Œ∏, and so inference based on this
posterior distribution is not formally Bayesian. However, sometimes taking
limits like this leads to sensible answers: Theoretical results in Stein (1955)
show that from a decision-theoretic point of view, any reasonable estimator is
a Bayesian estimator or a limit of a sequence of Bayesian estimators, and that
any Bayesian estimator is reasonable (the technical term here is admissible;
see also Berger (1980)).

5.4 Bias, variance and mean squared error
A point estimator of an unknown parameter Œ∏ is a function that converts
your data into a single element of the parameter space Œò. For example, in the
case of a normal sampling model and conjugate prior distribution of the last
section, the posterior mean estimator of Œ∏ is
Œ∏ÀÜb (y1 , . . . , yn ) = E[Œ∏|y1 , . . . , yn ] =

n
Œ∫0
y¬Ø +
¬µ0 = w¬Ø
y + (1 ‚àí w)¬µ0 .
Œ∫0 + n
Œ∫0 + n

5 The normal model

0.01

0.02

0.03

œÉ2
0.04

0.05

0.06

0.07

80

1.7

1.8
Œ∏

1.9

2.0

0

0

10

2

p( œÉ2|y 1...y n )
20
30

p( Œ∏|y 1...y n )
4
6

40

8

50

1.6

0.00

0.02

0.04
œÉ2

0.06

1.6

1.7

1.8
Œ∏

1.9

2.0

Fig. 5.5. Monte Carlo samples from and estimates of the joint and marginal distributions of the population mean and variance. The vertical lines in the third plot give
a 95% quantile-based posterior interval for Œ∏ (gray), as well as the 95% confidence
interval based on the t-statistic (black).

The sampling properties of an estimator such as Œ∏ÀÜb refer to its behavior under
hypothetically repeatable surveys or experiments. Let‚Äôs compare the sampling
properties of Œ∏ÀÜb to Œ∏ÀÜe (y1 , . . . , yn ) = y¬Ø, the sample mean, when the true value
of the population mean is Œ∏0 :
E[Œ∏ÀÜe |Œ∏ = Œ∏0 ] = Œ∏0 , and we say that Œ∏ÀÜe is ‚Äúunbiased,‚Äù
E[Œ∏ÀÜb |Œ∏ = Œ∏0 ] = wŒ∏0 + (1 ‚àí w)¬µ0 , and if ¬µ0 6= Œ∏0 we say that Œ∏ÀÜb is ‚Äúbiased.‚Äù
Bias refers to how close the center of mass of the sampling distribution of an
estimator is to the true value. An unbiased estimator is an estimator with
zero bias, which sounds desirable. However, bias does not tell us how far away

5.4 Bias, variance and mean squared error

81

an estimate might be from the true value. For example, y1 is an unbiased
estimator of the population mean Œ∏0 , but will generally be farther away from
Œ∏0 than y¬Ø. To evaluate how close an estimator Œ∏ÀÜ is likely to be to the true
ÀÜ 0 ],
value Œ∏0 , we might use the mean squared error (MSE). Letting m = E[Œ∏|Œ∏
the MSE is
ÀÜ 0 ] = E[(Œ∏ÀÜ ‚àí Œ∏0 )2 |Œ∏0 ]
MSE[Œ∏|Œ∏
= E[(Œ∏ÀÜ ‚àí m + m ‚àí Œ∏0 )2 |Œ∏0 ]
= E[(Œ∏ÀÜ ‚àí m)2 |Œ∏0 ] + 2E[(Œ∏ÀÜ ‚àí m)(m ‚àí Œ∏0 )|Œ∏0 ] + E[(m ‚àí Œ∏0 )2 |Œ∏0 ].
ÀÜ 0 ] it follows that E[Œ∏ÀÜ ‚àí m|Œ∏0 ] = 0 and so the second term is
Since m = E[Œ∏|Œ∏
zero. The first term is the variance of Œ∏ÀÜ and the third term is the square of
the bias and so
ÀÜ 0 ] = Var[Œ∏|Œ∏
ÀÜ 0 ] + Bias2 [Œ∏|Œ∏
ÀÜ 0 ].
MSE[Œ∏|Œ∏
This means that, before the data are gathered, the expected distance from
the estimator to the true value depends on how close Œ∏0 is to the center of the
distribution of Œ∏ÀÜ (the bias), as well as how spread out the distribution is (the
variance). Getting back to our comparison of Œ∏ÀÜb to Œ∏ÀÜe , the bias of Œ∏ÀÜe is zero,
but
œÉ2
, whereas
n
œÉ2
œÉ2
Var[Œ∏ÀÜb |Œ∏ = Œ∏0 , œÉ 2 ] = w2 √ó
<
,
n
n

Var[Œ∏ÀÜe |Œ∏ = Œ∏0 , œÉ 2 ] =

and so Œ∏ÀÜb has lower variability. Which one is better in terms of MSE?
œÉ2
MSE[Œ∏ÀÜe |Œ∏0 ] = E[(Œ∏ÀÜe ‚àí Œ∏0 )2 |Œ∏0 ] =
n
2
ÀÜ
ÀÜ
MSE[Œ∏b |Œ∏0 ] = E[(Œ∏b ‚àí Œ∏0 ) |Œ∏0 ] = E[{w(¬Ø
y ‚àí Œ∏0 ) + (1 ‚àí w)(¬µ0 ‚àí Œ∏0 )}2 |Œ∏0 ]
œÉ2
= w2 √ó
+ (1 ‚àí w)2 (¬µ0 ‚àí Œ∏0 )2
n
With some algebra, you can show that MSE[Œ∏ÀÜb |Œ∏0 ] < MSE[Œ∏ÀÜe |Œ∏0 ] if
œÉ2 1 + w
n 1‚àíw


1
2
= œÉ2
+
.
n Œ∫0

(¬µ0 ‚àí Œ∏0 )2 <

Some argue that if you know even just a little bit about the population you
are about to sample from, you should be able to find values of ¬µ0 and Œ∫0
such that this inequality holds. In this case, you can construct a Bayesian
estimator that will have a lower average squared distance to the truth than
does the sample mean. For example, if you are pretty sure that your best

82

5 The normal model

prior guess ¬µ0 is within two standard deviations of the true population mean,
then if you pick Œ∫0 = 1 you can be pretty sure that the Bayes estimator has a
lower MSE. To make some of this more clear, let‚Äôs take a look at the sampling
distributions of a few different estimators in the context of an example.
Example: IQ scores
Scoring on IQ tests is designed to produce a normal distribution with a mean
of 100 and a standard deviation of 15 (a variance of 225) when applied to
the general population. Now suppose we are to sample n individuals from a
particular town in the United States and then estimate Œ∏, the town-specific
mean IQ score, based on the sample of size n. For Bayesian estimation, if
we lack much information about the town in question, a natural choice of ¬µ0
would be ¬µ0 = 100.
Suppose that unknown to us the people in this town are extremely exceptional and the true mean and standard deviation of IQ scores in the town are
Œ∏ = 112 and œÉ = 13 (œÉ 2 = 169). The MSEs of the estimators Œ∏ÀÜe and Œ∏ÀÜb are
then
MSE[Œ∏ÀÜe |Œ∏0 ] = Var[Œ∏ÀÜe ] =
MSE[Œ∏ÀÜb |Œ∏0 ] = w2

œÉ2
169
=
n
n

169
+ (1 ‚àí w)2 144,
n

0.08

0.12

relative MSE
0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1

where w = n/(Œ∫0 + n). The ratio MSE[Œ∏ÀÜb |Œ∏0 ]/MSE[Œ∏ÀÜe |Œ∏0 ] is plotted in the first
panel of Figure 5.6 as a function of n, for Œ∫0 = 1, 2 and 3.

0.00

0.04

Œ∫0 = 0
Œ∫0 = 1
Œ∫0 = 2
Œ∫0 = 3

0

10

20
30
sample size

40

50

95

100 105 110 115 120 125
IQ

Fig. 5.6. Mean squared errors and sampling distributions of different estimators of
the population mean IQ score.

5.5 Prior specification based on expectations

83

Notice that when Œ∫0 = 1 or 2 the Bayes estimate has lower MSE than
the sample mean, especially when the sample size is low. This is because even
though the prior guess ¬µ0 = 100 is seemingly way off, it is not actually that far
off when considering the uncertainty in our sample data. A choice of Œ∫0 = 3 on
the other hand puts more weight on the value of 100, and the corresponding
estimator has a generally higher MSE than the sample mean. As n increases,
the bias of each of the estimators shrinks to zero, and the MSEs converge to
the common value of œÉ 2 /n. The second panel of Figure 5.6 shows the sampling
distributions of the sample mean when n = 10, as well as those of the three
Bayes estimators corresponding to Œ∫0 = 1, 2 and 3. This plot highlights the
relative contributions of the bias and variance to the MSE. The sampling
distribution of the sample mean is centered around the true value of 112, but
is more spread out than any of the other distributions. The distribution of the
Œ∫0 = 1 estimator is not quite centered around the true mean, but its variance
is low and so this estimator is closer on average to the truth than the sample
mean.

5.5 Prior specification based on expectations
A p-dimensional exponential family model is a model whose densities can be
written as p(y|œÜ) = h(y)c(œÜ) exp{œÜT t(y)}, where œÜ is the parameter to be
estimated and t(y) = {t1 (y), . . . , tp (y)} is the sufficient statistic. The normal
model is a two-dimensional exponential family model, where
‚Ä¢ t(y) = (y, y 2 ),
‚Ä¢ œÜ = (Œ∏/œÉ 2 , ‚àí(2œÉ 2 )‚àí1 ) and
‚Ä¢ c(œÜ) = |œÜ2 |1/2 exp{œÜ21 /(2œÜ2 )}.
As was the case for one-parameter exponential family models in Section
3.3, a conjugate prior distribution can be written in terms of œÜ, giving
p(œÜ|n0 , t0 ) ‚àù c(œÜ)n0 exp(n0 tT0 œÜ), where t0 = (t01 , t02 ) = (E[Y ], E[Y 2 ]), the
prior expectations of Y and Y 2 . If we reparameterize in terms of (Œ∏, œÉ 2 ), we
get



‚àín0 (Œ∏ ‚àí t01 )2
2
2 ‚àí1/2
p(Œ∏, œÉ |n0 , t0 ) ‚àù (œÉ )
exp
√ó
2œÉ 2



‚àín0 (t02 ‚àí t201 )
2 ‚àí(n0 +5)/2
(œÉ )
exp
.
2œÉ 2
The first term in the big braces is proportional to a normal(t01 , œÉ 2 /n0 ) density,
and the second is proportional to an inverse-gamma((n0 + 3)/2, n0 (t2 ‚àí t21 )/2)
density. To see how our prior parameters t01 and t02 should be determined,
let‚Äôs consider the case where we have a prior expectation ¬µ0 for the population
mean (so E[Y ] = E[E[Y |Œ∏]] = E[Œ∏] = ¬µ0 ), and a prior expectation œÉ02 for the

84

5 The normal model

population variance (so that E[Var[Y |Œ∏, œÉ 2 ]] = E[œÉ 2 ] = œÉ02 ). Then we would
set t01 equal to ¬µ0 , and determine t02 from
t02 = E[Y 2 ] = E[E[Y 2 |Œ∏, œÉ 2 ]]
= E[œÉ 2 + Œ∏2 ]
= œÉ02 + œÉ02 /n0 + ¬µ20 = œÉ02 (n0 + 1)/n0 + ¬µ20 ,
so n0 (t02 ‚àí t201 ) = (n0 + 1)œÉ02 . Thus our joint prior distribution for (Œ∏, œÉ 2 )
would be
Œ∏|œÉ 2 ‚àº normal(¬µ0 , œÉ 2 /n0 ), and
œÉ 2 ‚àº inverse-gamma((n0 + 3)/2, (n0 + 1)œÉ02 /2).
For example, if our prior information is weak we might set n0 = 1, giving
Œ∏|œÉ 2 ‚àº normal(¬µ0 , œÉ 2 ) and 1/œÉ 2 ‚àº inverse-gamma(2, œÉ02 ). It is easy to check
that under this prior distribution the prior expectation of Y is ¬µ0 , and the
prior expectation of Var[Y |Œ∏, œÉ 2 ] is œÉ02 , as desired. Given n i.i.d samples from
the population, our posterior distribution under this prior would be


¬µ0 /œÉ 2 + n¬Ø
y œÉ2
,
{Œ∏|œÉ 2 , y1 , . . . , yn } ‚àº normal
1/œÉ 2 + n n + 1


n
{œÉ 2 |y1 , . . . , yn } ‚àº 2 + n/2, œÉ02 + (n ‚àí 1)s2 +
(¬Ø
y ‚àí ¬µ0 )2 .
n+1

5.6 The normal model for non-normal data
People use the normal model all the time in situations where the data are not
even close to being normally distributed. The justification of this is generally
that while the sampling distribution of a single data point is not normal, the
sampling distribution of the sample mean is close to normal. Let‚Äôs explore this
distinction via a Monte Carlo sampling experiment: The 1998 General Social
Survey (GSS) recorded the number of children for 921 women over the age of
40. Let‚Äôs take these 921 women as our population, and consider estimating the
mean number of children for this population (which is 2.42) based on random
samples Y1 , . . . , Yn of different sizes n.
The true population distribution is plotted in the first panel of Figure 5.7,
and is clearly not normal. For example, the distribution is discrete, bounded
and skewed, whereas a normal distribution is continuous, unbounded and
symmetric.
PnNow let‚Äôs consider the sampling distribution of the sample mean
Y¬Øn = n1 i=1 Yi for n ‚àà {5, 15, 45}. This can be done using a Monte Carlo
approximation as follows: For each n and some large value of S, simulate
(1)
(S)
(s)
{¬Ø
yn , . . . , y¬Øn }, where each y¬Øn is the sample mean of n samples taken without
replacement from the 921 values in the population. The second panel of Figure
5.7 shows the Monte Carlo approximations to the sampling distributions of

0 1 2 3 4 5 6 7 8
y=number of children

6
1

0.0

0.00

2

0.5

3

s2

4

p (y )
1.0

p (y )
0.10
0.20

85

5

n=5
n=15
n=45

1.5

0.30

5.6 The normal model for non-normal data

0

1 2 3 4 5
number of children

6

1.5

2.0

2.5

3.0

y

Fig. 5.7. A non-normal distribution and the distribution of its sample mean for
n ‚àà {5, 15, 45}. The third panel shows a contour plot of the joint sampling density
of {¬Ø
y , s2 } for the case n = 45.

Y¬Ø5 ,Y¬Ø15 and Y¬Ø45 . While the distribution of Y¬Ø5 looks a bit skewed, the sampling
distributions of Y¬Ø15 and Y¬Ø45 are hard to distinguish from normal distributions.
This should not be too much of a surprise, as the central limit theorem tells
us that
p
p(¬Ø
y |Œ∏, œÉ 2 ) ‚âà dnorm(¬Ø
y , Œ∏, œÉ 2 /n),
with the approximation becoming increasingly good as n gets larger. If the
population variance œÉ 2 were known, then an approximate posterior distribution of the population mean, conditional on the sample mean, could be
obtained as
p(Œ∏|¬Ø
y , œÉ 2 ) ‚àù p(Œ∏) √ó p(¬Ø
y |Œ∏, œÉ 2 )
‚âà p(Œ∏) √ó dnorm(¬Ø
y , Œ∏,

p
œÉ 2 /n).

Of course œÉ 2 is generally not known, but it is estimated by s2 . The approximate posterior distribution of (Œ∏, œÉ 2 ) conditional on the estimates (¬Ø
y , s2 ) is
given by
p(Œ∏, œÉ 2 |¬Ø
y , s2 ) ‚àù p(Œ∏, œÉ 2 ) √ó p(¬Ø
y , s2 |Œ∏, œÉ 2 )
= p(Œ∏, œÉ 2 ) √ó p(¬Ø
y |Œ∏, œÉ 2 ) √ó p(s2 |¬Ø
y , Œ∏, œÉ 2 )
p
‚âà p(Œ∏, œÉ 2 ) √ó dnorm(¬Ø
y , Œ∏, œÉ 2 /n) √ó p(s2 |¬Ø
y , Œ∏, œÉ 2 ).

(5.2)

Again, for large n, the approximation of p(¬Ø
y |Œ∏, œÉ 2 ) by the normal density
is generally a good one even if the population is not normally distributed.
However, it is not clear what to put for p(s2 |¬Ø
y , Œ∏, œÉ 2 ). If we knew that the
data were actually sampled from a normal distribution, then results from
statistical theory would say that
n‚àí1 n‚àí1
,
).
2
2œÉ 2
Note that this result says that, for normal populations, Y¬Ø and s2 are independent. Using this sampling model for s2 in Equation 5.2 results in exactly
p(s2 |¬Ø
y , Œ∏, œÉ 2 ) = dgamma(s2 ,

86

5 The normal model

the same conditional distribution for (Œ∏, œÉ 2 ) as p(Œ∏, œÉ 2 |y1 , . . . , yn ) assuming
that the data are normally distributed. However, if the data are not normally
distributed, then s2 is not necessarily gamma-distributed or independent of
y¬Ø. For example, the third panel of Figure 5.7 shows the joint sampling distribution of {Y¬Ø , s2 } for the GSS population. Notice that Y¬Ø and s2 are positively
correlated for this population, as is often the case for positively skewed populations. This suggests that the use of the posterior distribution in Equation
5.2 for non-normal data could give misleading results about the joint distribution of {Œ∏, œÉ 2 }. However, the marginal posterior distribution of Œ∏ based on
5.2 can be remarkably accurate, even for non-normal data. The reasoning is
as follows: The central limit theorem says that for large n
‚àö Y¬Ø ‚àí Œ∏ ¬∑
n
‚àº normal(0, 1),
œÉ
¬∑

where ‚àº means ‚Äúapproximately distributed as.‚Äù Additionally, if n is sufficiently large, then s2 ‚âà œÉ 2 and so
‚àö Y¬Ø ‚àí Œ∏ ¬∑
n
‚àº normal(0, 1).
s
This should
seem familiar: Recall from introductory statistics that for normal
‚àö
data, n(Y¬Ø ‚àí Œ∏)/s has a t-distribution with n ‚àí 1 degrees of freedom. For
large n, s2 is very close to œÉ 2 and the tn‚àí1 distribution is very close to a
normal(0, 1) distribution.
Even though the posterior distribution based on a normal model may provide good inference for the population mean, the normal model can provide
misleading results for other sample quantities. For example, every normal density is symmetric and has a skew of zero, whereas our true population in the
above example has a skew of E[(Y ‚àí Œ∏)3 ]/œÉ 3 = 0.89. Normal-model inference
for samples from this population will underestimate the number of people in
the right tail of the distribution, and so will provide poor estimates of the percentage of people with large numbers of children. In general, using the normal
model for non-normal data is reasonable if we are only interested in obtaining a posterior distribution for the population mean. For other population
quantities the normal model can provide misleading results.

5.7 Discussion and further references
The normal sampling model can be justified in many different ways. For example, Lukacs (1942) shows that a characterizing feature of the normal distribution is that the sample mean and the sample variance are independent (see
also Rao (1958)). From a subjective probability perspective, this suggests that
if your beliefs about the sample mean are independent from those about the
sample variance, then a normal model is appropriate. Also, among all distributions with a given mean Œ∏ and variance œÉ 2 , the normal(Œ∏, œÉ 2 ) distribution

5.7 Discussion and further references

87

is the most diffuse in terms of a measure known as entropy (see Jaynes, 2003,
Chap.7, Chap.11).
From a data analysis perspective, one justification of the normal sampling
model is that, as described in Section 5.6, the sample mean will generally be
approximately normally distributed due to the central limit theorem. Thus
the normal model provides a reasonable sampling model for the sample mean,
if not the sample data. Additionally, the normal model is a simple exponential
family model with sufficient statistics equivalent to the sample mean and variance. As a result, it will provide consistent estimation of the population mean
and variance even if the underlying population is not normal. Additionally,
confidence intervals for the population mean based on the normal model will
generally be asymptotically correct (these results can be derived from those
in White (1982)). However, the normal model may give inaccurate inference
for other population quantities.

6
Posterior approximation with the Gibbs
sampler

For many multiparameter models the joint posterior distribution is nonstandard and difficult to sample from directly. However, it is often the case
that it is easy to sample from the full conditional distribution of each parameter. In such cases, posterior approximation can be made with the Gibbs
sampler, an iterative algorithm that constructs a dependent sequence of parameter values whose distribution converges to the target joint posterior distribution. In this chapter we outline the Gibbs sampler in the context of the
normal model with a semiconjugate prior distribution, and discuss how well
the method is able to approximate the posterior distribution.

6.1 A semiconjugate prior distribution
In the previous chapter we modeled our uncertainty about Œ∏ as depending on
œÉ2 :
‚àö
p(Œ∏|œÉ 2 ) = dnorm (Œ∏, ¬µ0 , œÉ/ Œ∫0 ) .
This prior distribution relates the prior variance of Œ∏ to the sampling variance
of our data in such a way that ¬µ0 can be thought of as Œ∫0 prior samples
from the population. In some situations this makes sense, but in others we
may want to specify our uncertainty about Œ∏ as being independent of œÉ 2 ,
so that p(Œ∏, œÉ 2 ) = p(Œ∏) √ó p(œÉ 2 ). One such joint distribution is the following
‚Äúsemiconjugate‚Äù prior distribution:
Œ∏ ‚àº normal(¬µ0 , œÑ02 )
1/œÉ 2 ‚àº gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2) .
If {Y1 , . . . , Yn |Œ∏, œÉ 2 } ‚àº i.i.d. normal(Œ∏, œÉ 2 ), we showed in Section 5.2 that
{Œ∏|œÉ 2 , y1 , . . . , yn } ‚àº normal(¬µn , œÑn2 ) with

‚àí1
1
n
¬µ0 /œÑ02 + n¬Ø
y /œÉ 2
2
¬µn =
and œÑn =
+ 2
.
1/œÑ02 + n/œÉ 2
œÑ02
œÉ
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 6,
c Springer Science+Business Media, LLC 2009


90

6 Posterior approximation with the Gibbs sampler

In the conjugate case where œÑ02 was proportional to œÉ 2 , we showed that
p(œÉ 2 |y1 , . . . , yn ) was an inverse-gamma distribution, and that a Monte Carlo
sample of {Œ∏, œÉ 2 } from their joint posterior distribution could be obtained by
sampling
1. a value œÉ 2(s) from p(œÉ 2 |y1 , . . . , yn ), an inverse-gamma distribution, then
2. a value Œ∏(s) from p(Œ∏|œÉ 2(s) , y1 , . . . , yn ), a normal distribution.
However, in the case where œÑ02 is not proportional to œÉ 2 , the marginal density
of 1/œÉ 2 is not a gamma distribution, or any other standard distribution from
which we can easily sample.

6.2 Discrete approximations
Letting œÉ
Àú 2 = 1/œÉ 2 be the precision, recall that the posterior distribution
2
of {Œ∏, œÉ
Àú } is equal to the joint distribution of {Œ∏, œÉ 2 , y1 , . . . , yn }, divided by
p(y1 , . . . , yn ), which does not depend on the parameters. Therefore the relative posterior probabilities of one set of parameter values {Œ∏1 , œÉ
Àú12 } to another
2
{Œ∏2 , œÉ
Àú2 } is directly computable:
p(Œ∏1 , œÉ
Àú12 |y1 , . . . , yn )
p(Œ∏1 , œÉ
Àú12 , y1 , . . . , yn )/p(y1 , . . . , yn )
=
2
p(Œ∏2 , œÉ
Àú2 |y1 , . . . , yn )
p(Œ∏2 , œÉ
Àú22 , y1 , . . . , yn )/p(y1 , . . . , yn )
p(Œ∏1 , œÉ
Àú12 , y1 , . . . , yn )
=
.
p(Œ∏2 , œÉ
Àú22 , y1 , . . . , yn )
The joint distribution is easy to compute as it was built out of standard prior
and sampling distributions:
p(Œ∏, œÉ
Àú 2 , y1 , . . . , yn ) = p(Œ∏, œÉ
Àú 2 ) √ó p(y1 , . . . , yn |Œ∏, œÉ
Àú2)
= dnorm(Œ∏, ¬µ0 , œÑ0 ) √ó dgamma(Àú
œÉ 2 , ŒΩ0 /2, ŒΩ0 œÉ02 /2) √ó
n
Y
‚àö
dnorm(yi , Œ∏, 1/ œÉ
Àú 2 ).
i=1

A discrete approximation to the posterior distribution makes use of these
facts by constructing a posterior distribution over a grid of parameter values, based on relative posterior probabilities. This is done by evaluating
p(Œ∏, œÉ
Àú 2 , y1 , . . . , yn ) on a two-dimensional grid of values of {Œ∏, œÉ
Àú 2 }. Letting
2
2
{Œ∏1 , . . . , Œ∏G } and {Àú
œÉ1 , . . . , œÉ
ÀúH } be sequences of evenly spaced parameter values, the discrete approximation to the posterior distribution assigns a posterior probability to each pair {Œ∏k , œÉ
Àúl2 } on the grid, given by

6.2 Discrete approximations

91

p(Œ∏k , œÉ
Àúl2 |y1 , . . . , yn )
PH
Àúh2 |y1 , . . . , yn )
g=1
h=1 p(Œ∏g , œÉ

pD (Œ∏k , œÉ
Àúl2 |y1 , . . . , yn ) = PG

p(Œ∏k , œÉ
Àúl2 , y1 , . . . , yn )/p(y1 , . . . , yn )
PH
Àúh2 , y1 , . . . , yn )/p(y1 , . . . , yn )
g=1
h=1 p(Œ∏g , œÉ

= PG

p(Œ∏k , œÉ
Àúl2 , y1 , . . . , yn )
.
PH
Àúh2 , y1 , . . . , yn )
g=1
h=1 p(Œ∏g , œÉ

= PG

This is a real joint probability distribution for Œ∏ ‚àà {Œ∏1 , . . . , Œ∏G } and œÉ
Àú2 ‚àà
2
2
{Àú
œÉ1 , . . . , œÉ
ÀúH }, in the sense that it sums to 1. In fact, it is the actual posterior
distribution of {Œ∏, œÉ
Àú 2 } if the joint prior distribution for these parameters is
discrete on this grid.
Let‚Äôs try the approximation for the midge data from the previous chapter.
Recall that our data were {n = 9, y¬Ø = 1.804, s2 = 0.017}. The conjugate prior
distribution on Œ∏ and œÉ 2 of Chapter 5 required that the prior variance on Œ∏ be
œÉ 2 /Œ∫0 , i.e. proportional to the sampling variance. A small value of the sampling variance then has the possibly undesirable effect of reducing the nominal
prior uncertainty for Œ∏. In contrast, the semiconjugate prior distribution frees
us from this constraint. Recall that we first suggested that the prior mean
and standard deviation of Œ∏ should be ¬µ0 = 1.9 and œÑ0 = .95, as this would
put most of the prior mass on Œ∏ > 0, which we know to be true. For œÉ 2 , let‚Äôs
use prior parameters of ŒΩ0 = 1 and œÉ02 = 0.01.
The R -code below evaluates p(Œ∏, œÉ
Àú 2 |y1 , . . . , yn ) on a 100√ó100 grid of evenly
spaced parameter values, with Œ∏ ‚àà {1.505, 1.510, . . . , 1.995, 2.00} and œÉ
Àú2 ‚àà
{1.75, 3.5, . . . , 173.25, 175.0}. The first panel of Figure 6.1 gives the discrete
approximation to the joint distribution of {Œ∏, œÉ
Àú 2 }. Marginal and conditional
posterior distributions for Œ∏ and œÉ
Àú 2 can be obtained from the approximation
to the joint distribution with simple arithmetic. For example,
pD (Œ∏k |y1 , . . . , yn ) =

H
X

pD (Œ∏k , œÉ
Àúh2 |y1 , . . . , yn ).

h=1

The resulting discrete approximations to the marginal posterior distributions
of Œ∏ and œÉ
Àú 2 are shown in the second and third panels of Figure 6.1.
mu0<‚àí1.9 ; t20 <‚àí0.95ÀÜ2 ; s20 <‚àí.01 ; nu0<‚àí1
y<‚àíc ( 1 . 6 4 , 1 . 7 0 , 1 . 7 2 , 1 . 7 4 , 1 . 8 2 , 1 . 8 2 , 1 . 8 2 , 1 . 9 0 , 2 . 0 8 )
G<‚àí100 ; H<‚àí100
mean . g r i d <‚àís e q ( 1 . 5 0 5 , 2 . 0 0 , l e n g t h=G)
p r e c . g r i d <‚àís e q ( 1 . 7 5 , 1 7 5 , l e n g t h=H)
p o s t . g r i d <‚àíma t r i x ( nrow=G, n c o l=H)
f o r ( g i n 1 :G) {

92

6 Posterior approximation with the Gibbs sampler

f o r ( h i n 1 :H) {
p o s t . g r i d [ g , h]<‚àí
dnorm ( mean . g r i d [ g ] , mu0 , s q r t ( t 2 0 ) ) ‚àó
dgamma( p r e c . g r i d [ h ] , nu0 / 2 , s 2 0 ‚àónu0 /2 ) ‚àó
prod ( dnorm ( y , mean . g r i d [ g ] , 1 / s q r t ( p r e c . g r i d [ h ] ) ) )
}
}

~ 2|y ...y )
p( œÉ
1
n
0.010
0.020
0.000

0.00

50

~2
œÉ
100

150

p( Œ∏|y 1...y n )
0.02
0.04

p o s t . g r i d <‚àíp o s t . g r i d /sum ( p o s t . g r i d )

1.6 1.7 1.8 1.9 2.0
Œ∏

1.5 1.6 1.7 1.8 1.9 2.0
Œ∏

0

50

100
~2
œÉ

150

Fig. 6.1. Joint and marginal posterior distributions based on a discrete approximation.

Evaluation of this two-parameter posterior distribution at 100 values of
each parameter required a grid of size 100√ó100 = 1002 . In general, to construct
a similarly fine approximation for a p-dimensional posterior distribution we
would need a p-dimensional grid containing 100p posterior probabilities. This
means that discrete approximations will only be feasible for densities having
a small number of parameters.

6.3 Sampling from the conditional distributions
Suppose for the moment you knew the value of Œ∏. The conditional distribution
of œÉ
Àú 2 given Œ∏ and {y1 , . . . , yn } is
p(Àú
œÉ 2 |Œ∏, y1 , . . . , yn ) ‚àù p(y1 , . . . , yn , Œ∏, œÉ
Àú2)
= p(y1 , . . . , yn |Œ∏, œÉ
Àú 2 )p(Œ∏|Àú
œÉ 2 )p(Àú
œÉ2 ) .
If Œ∏ and œÉ
Àú 2 are independent in the prior distribution, then p(Œ∏|Àú
œÉ 2 ) = p(Œ∏) and

6.4 Gibbs sampling

93

p(Àú
œÉ 2 |Œ∏, y1 , . . . , yn ) ‚àù p(y1 , . . . , yn |Œ∏, œÉ
Àú 2 )p(Àú
œÉ2 )
!
n
X
‚àù (Àú
œÉ 2 )n/2 exp{‚àíÀú
œÉ2
(yi ‚àí Œ∏)2 /2} √ó
i=1



2 ŒΩ0 /2‚àí1

(Àú
œÉ )

exp{‚àíÀú
œÉ 2 ŒΩ0 œÉ02 /2}



= (Àú
œÉ 2 )(ŒΩ0 +n)/2‚àí1 √ó exp{‚àíÀú
œÉ 2 √ó [ŒΩ0 œÉ02 +

X

(yi ‚àí Œ∏)2 ]/2}.

This is the form of a gamma density, and so evidently {œÉ 2 |Œ∏, y1 , . . . , yn } ‚àº
inverse-gamma(ŒΩn /2, ŒΩn œÉn2 (Œ∏)/2), where
ŒΩn = ŒΩ0 + n , œÉn2 (Œ∏) =


1 
ŒΩ0 œÉ02 + ns2n (Œ∏) ,
ŒΩn

P
and s2n (Œ∏) = (yi ‚àí Œ∏)2 /n, the unbiased estimate of œÉ 2 if Œ∏ were known. This
means that we can easily sample directly from p(œÉ 2 |Œ∏, y1 , . . . , yn ), as well as
from p(Œ∏|œÉ 2 , y1 , . . . , yn ) as shown at the beginning of the chapter. However,
we do not yet have a way to sample directly from p(Œ∏, œÉ 2 |y1 , . . . , yn ). Can
we use the full conditional distributions to sample from the joint posterior
distribution?
Suppose we were given œÉ 2(1) , a single sample from the marginal posterior
distribution p(œÉ 2 |y1 , . . . , yn ). Then we could sample
Œ∏(1) ‚àº p(Œ∏|œÉ 2(1) , y1 , . . . , yn )
and {Œ∏(1) , œÉ 2(1) } would be a sample from the joint distribution of {Œ∏, œÉ 2 }.
Additionally, Œ∏(1) can be considered a sample from the marginal distribution
p(Œ∏|y1 , . . . , yn ). From this Œ∏-value, we can generate
œÉ 2(2) ‚àº p(œÉ 2 |Œ∏(1) , y1 , . . . , yn ).
But since Œ∏(1) is a sample from the marginal distribution of Œ∏, and œÉ 2(2) is
a sample from the conditional distribution of œÉ 2 given Œ∏(1) , then {Œ∏(1) , œÉ 2(2) }
is also a sample from the joint distribution of {Œ∏, œÉ 2 }. This in turn means
that œÉ 2(2) is a sample from the marginal distribution p(œÉ 2 |y1 , . . . , yn ), which
then could be used to generate a new sample Œ∏(2) , and so on. It seems that
the two conditional distributions could be used to generate samples from the
joint distribution, if only we had a œÉ 2(1) from which to start.

6.4 Gibbs sampling
The distributions p(Œ∏|œÉ 2 , y1 , . . . , yn ) and p(œÉ 2 |Œ∏, y1 , . . . , yn ) are called the full
conditional distributions of Œ∏ and œÉ 2 respectively, as they are each a conditional distribution of a parameter given everything else. Let‚Äôs make the iterative sampling idea described in the previous paragraph more precise. Given
a current state of the parameters œÜ(s) = {Œ∏(s) , œÉ
Àú 2(s) }, we generate a new state
as follows:

94

6 Posterior approximation with the Gibbs sampler

1. sample Œ∏(s+1) ‚àº p(Œ∏|Àú
œÉ 2(s) , y1 , . . . , yn );
2(s+1)
2. sample œÉ
Àú
‚àº p(Àú
œÉ 2 |Œ∏(s+1) , y1 , . . . , yn );
(s+1)
(s+1)
3. let œÜ
= {Œ∏
,œÉ
Àú 2(s+1) }.
This algorithm is called the Gibbs sampler, and generates a dependent sequence of our parameters {œÜ(1) , œÜ(2) , . . . , œÜ(S) }. The R-code to perform this
sampling scheme for the normal model with the semiconjugate prior distribution is as follows:
### data
mean . y<‚àímean ( y ) ; var . y<‚àívar ( y ) ; n<‚àíl e n g t h ( y )
###
### s t a r t i n g v a l u e s
S<‚àí1000
PHI<‚àím at r i x ( nrow=S , n c o l =2)
PHI[1 ,] < ‚àí phi<‚àíc ( mean . y , 1/ var . y )
###
### Gibbs s a m p l i n g
set . seed (1)
for ( s in 2: S) {
# g e n e r a t e a new t h e t a v a l u e from i t s f u l l c o n d i t i o n a l
mun<‚àí ( mu0/ t 2 0 + n‚àómean . y‚àó p h i [ 2 ] ) / ( 1/ t 2 0 + n‚àó p h i [ 2 ] )
t2n<‚àí 1 / ( 1/ t 2 0 + n‚àó p h i [ 2 ] )
p h i [1]< ‚àí rnorm ( 1 , mun , s q r t ( t2n ) )
# g e n e r a t e a new 1/ sigma ÀÜ2 v a l u e from i t s f u l l c o n d i t i o n a l
nun<‚àí nu0+n
s2n<‚àí ( nu0‚àó s 2 0 + ( n‚àí1)‚àó var . y + n ‚àó ( mean . y‚àíp h i [ 1 ] ) ÀÜ 2 ) /nun
p h i [2]< ‚àí rgamma ( 1 , nun / 2 , nun‚àó s2n / 2 )
PHI [ s ,]<‚àí p h i
###

}

In this code, we have used the identity
ns2n (Œ∏) =

n
X

(yi ‚àí Œ∏)2 =

i=1

n
X

(yi ‚àí y¬Ø + y¬Ø ‚àí Œ∏)2

i=1

=

n
X

[(yi ‚àí y¬Ø)2 + 2(yi ‚àí y¬Ø)(¬Ø
y ‚àí Œ∏) + (¬Ø
y ‚àí Œ∏)2 ]

i=1

=

n
X

(yi ‚àí y¬Ø)2 + 0 +

i=1
2

n
X
i=1
2

= (n ‚àí 1)s + n(¬Ø
y ‚àí Œ∏) .

(¬Ø
y ‚àí Œ∏)2

6.4 Gibbs sampling

95

The reason for writing the code this way is because s2 and y¬Ø do not change
with each new Œ∏-value,
(n ‚àí 1)s2 + n(¬Ø
y ‚àí Œ∏)2 is faster than
Pnand computing
2
having to recompute i=1 (yi ‚àí Œ∏) at each iteration.

25

40

40

8

1.70

1.80
Œ∏

1.90

13
11
10

1
7
4

14

20

20

4

~2
œÉ
60 80 100

9

40

1

15

20

25

3 6

~2
œÉ
60 80 100

~2
œÉ
60 80 100

3

12
1.70

1.80
Œ∏

1.90

49
98
5888
3 6
85
1594
67
4869 939
13
80 59
56
282611
60
10
27
53
16
66
40
19
37
32 45
96 2297
235
72
5
57
34
131
63
91
86
81
9
2
76
33
23
30
43
20
71
68 838
70
99
24
10093
83
54
87
41
36
784
738918
74 62
47
78
46
29
4
44 5042
64 25 8290
65552114
51
61 95
17 77 79
52 75
12
1.70

1.80
Œ∏

1.90

Fig. 6.2. The first 5, 15 and 100 iterations of a Gibbs sampler.

Using the midge data from the previous chapter and the prior distributions described above, a Gibbs sampler consisting of 1,000 iterations was constructed. Figure 6.2 plots the first 5, 15 and 100 simulated values, and the
first panel of Figure 6.3 plots the 1,000 values over the contours of the discrete
approximation to p(Œ∏, œÉ
Àú 2 |y1 , . . . , yn ). The second and third panels of Figure
6.3 give density estimates of the distributions of the simulated values of Œ∏ and
œÉ
Àú 2 . Finally, let‚Äôs find some empirical quantiles of our Gibbs samples:
### CI f o r p o p u l a t i o n mean
> q u a n t i l e ( PHI [ , 1 ] , c ( . 0 2 5 , . 5 , . 9 7 5 ) )
2.5%
50%
97.5%
1.707282 1.804348 1.901129
### CI f o r p o p u l a t i o n p r e c i s i o n
> q u a n t i l e ( PHI [ , 2 ] , c ( . 0 2 5 , . 5 , . 9 7 5 ) )
2.5%
50%
97.5%
17.48020 53.62511 129.20020

### CI f o r p o p u l a t i o n s t a n d a r d d e v i a t i o n
> q u a n t i l e ( 1 / s q r t ( PHI [ , 2 ] ) , c ( . 0 2 5 , . 5 , . 9 7 5 ) )
2.5%
50%
97.5%
0.08797701 0.13655763 0.23918408

The empirical distribution of these Gibbs samples very closely resembles the
discrete approximation to their posterior distribution, as can be seen by comparing Figures 6.1 and 6.3. This gives some indication that the Gibbs sampling
procedure is a valid method for approximating p(Œ∏, œÉ 2 |y1 , . . . , yn ).

0

2

50

~2
œÉ
100

p( Œ∏|y 1...y n )
4
6

~ 2|y ...y )
p( œÉ
1
n
0.000 0.004 0.008 0.012

8

6 Posterior approximation with the Gibbs sampler

150

96

1.6

1.7

1.8

1.9

1.6 1.7 1.8 1.9 2.0
Œ∏

Œ∏

0

50 100
~2
œÉ

200

Fig. 6.3. The first panel shows 1,000 samples from the Gibbs sampler, plotted over
the contours of the discrete approximation. The second and third panels give kernel
density estimates to the distributions of Gibbs samples of Œ∏ and œÉ
Àú 2 . Vertical gray
bars on the second plot indicate 2.5% and 97.5% quantiles of the Gibbs samples
of Œ∏, while nearly identical black vertical bars indicate the 95% confidence interval
based on the t-test.

6.5 General properties of the Gibbs sampler
Suppose you have a vector of parameters œÜ = {œÜ1 , . . . , œÜp }, and your information about œÜ is measured with p(œÜ) = p(œÜ1 , . . . , œÜp ). For example, in
the normal model œÜ = {Œ∏, œÉ 2 }, and the probability measure of interest is
(0)
(0)
p(Œ∏, œÉ 2 |y1 , . . . , yn ). Given a starting point œÜ(0) = {œÜ1 , . . . , œÜp }, the Gibbs
sampler generates œÜ(s) from œÜ(s‚àí1) as follows:
(s)

(s‚àí1)

(s)

(s)

(s‚àí1)

(s‚àí1)

1. sample œÜ1 ‚àº p(œÜ1 |œÜ2
, œÜ3
, . . . , œÜp
)
(s)
(s)
(s‚àí1)
(s‚àí1)
2. sample œÜ2 ‚àº p(œÜ2 |œÜ1 , œÜ3
, . . . , œÜp
)
..
.
(s)

(s)

p. sample œÜp ‚àº p(œÜp |œÜ1 , œÜ2 , . . . , œÜp‚àí1 ) .
This algorithm generates a dependent sequence of vectors:
(1)

œÜ(1) = {œÜ1 , . . . , œÜ(1)
p }
(2)

œÜ(2) = {œÜ1 , . . . , œÜ(2)
p }
..
.
(S)
œÜ(S) = {œÜ1 , . . . , œÜ(S)
p }.
In this sequence, œÜ(s) depends on œÜ(0) , . . . , œÜ(s‚àí1) only through œÜ(s‚àí1) , i.e.
œÜ(s) is conditionally independent of œÜ(0) , . . . , œÜ(s‚àí2) given œÜ(s‚àí1) . This is
called the Markov property, and so the sequence is called a Markov chain.
Under some conditions that will be met for all of the models discussed in this
text,

6.5 General properties of the Gibbs sampler

Pr(œÜ(s) ‚àà A) ‚Üí

97

Z
p(œÜ) dœÜ

as s ‚Üí ‚àû.

A

In words, the sampling distribution of œÜ(s) approaches the target distribution
as s ‚Üí ‚àû, no matter what the starting value œÜ(0) is (although some starting
values will get you to the target sooner than others). More importantly, for
most functions g of interest,
Z
S
1X
(s)
g(œÜ ) ‚Üí E[g(œÜ)] = g(œÜ)p(œÜ) dœÜ
S s=1

as S ‚Üí ‚àû.

(6.1)

This means we can approximate E[g(œÜ)] with the sample average of {g(œÜ(1) ),
. . ., g(œÜ(S) )}, just as in Monte Carlo approximation. For this reason, we call
such approximations Markov chain Monte Carlo (MCMC) approximations,
and the procedure an MCMC algorithm. In the context of the semiconjugate
normal model, Equation 6.1 implies that the joint distribution of {(Œ∏(1) , œÉ 2(1) ),
. . ., (Œ∏(1000) , œÉ 2(1000) )} is approximately equal to p(Œ∏, œÉ 2 | y1 ,. . ., yn ) , and that
1000
1 X (s)
E[Œ∏|y1 , . . . , yn ] ‚âà
Œ∏ = 1.804, and
1000 s=1

Pr(Œ∏ ‚àà [1.71, 1.90]|y1 , . . . , yn ) ‚âà 0.95.
We will discuss practical aspects of MCMC in the context of specific models
in the next section and in the next several chapters.
Distinguishing parameter estimation from posterior approximation
A Bayesian data analysis using Monte Carlo methods often involves a confusing array of sampling procedures and probability distributions. With this in
mind it is helpful to distinguish the part of the data analysis which is statistical from that which is numerical approximation. Recall from Chapter 1 that
the necessary ingredients of a Bayesian data analysis are
1. Model specification: a collection of probability distributions {p(y|œÜ), œÜ ‚àà
Œ¶} which should represent the sampling distribution of your data for some
value of œÜ ‚àà Œ¶;
2. Prior specification: a probability distribution p(œÜ), ideally representing
someone‚Äôs prior information about which parameter values are likely to
describe the sampling distribution.
Once these items are specified and the data have been gathered, the posterior
p(œÜ|y) is completely determined. It is given by
p(œÜ|y) =

p(œÜ)p(y|œÜ)
p(œÜ)p(y|œÜ)
=R
,
p(y)
p(œÜ)p(y|œÜ) dœÜ

and so in a sense there is no more modeling or estimation. All that is left is

98

6 Posterior approximation with the Gibbs sampler

3. Posterior summary: a description of the posterior distribution p(œÜ|y), done
in terms of particular quantities of interest such as posterior means, medians, modes, predictive probabilities and confidence regions.
For many models, p(œÜ|y) is complicated, hard to write down, and so on. In
these cases, a useful way to ‚Äúlook at‚Äù p(œÜ|y) is by studying Monte Carlo
samples from p(œÜ|y). Thus, Monte Carlo and MCMC sampling algorithms
‚Ä¢ are not models,
‚Ä¢ they do not generate ‚Äúmore information‚Äù than is in y and p(œÜ),
‚Ä¢ they are simply ‚Äúways of looking at‚Äù p(œÜ|y).
For example, if we have Monte Carlo samples œÜ(1) , . . . , œÜ(S) that are approximate draws from p(œÜ|y), then these samples help describe p(œÜ|y):
P (s) R
1
‚âà œÜp(œÜ|y) dœÜ
S PœÜ
Rc
1
(s)
1(œÜ
‚â§ c) ‚âà Pr(œÜ ‚â§ c|y) = ‚àí‚àû p(œÜ|y) dœÜ.
S
and so on. To keep this distinction in mind, it is useful to reserve the word
estimation to describe how we use p(œÜ|y) to make inference about œÜ, and to
use the word approximation to describe the use of Monte Carlo procedures to
approximate integrals.

6.6 Introduction to MCMC diagnostics
The purpose of Monte Carlo or Markov chain Monte Carlo approximation is
to obtain a sequence of parameter values {œÜ(1) , . . . , œÜ(S) } such that
Z
S
1X
(s)
g(œÜ ) ‚âà g(œÜ)p(œÜ) dœÜ,
S s=1
for any functions g of interest. In other words, we want the empirical average
of {g(œÜ(1) ), . . . , g(œÜ(S) )} to approximate the expected value of g(œÜ) under a
target probability distribution p(œÜ) (in Bayesian inference, the target distribution is usually the posterior distribution). In order for this to be a good
approximation for a wide range of functions g, we need the empirical distribution of the simulated sequence {œÜ(1) , . . . , œÜ(S) } to look like the target
distribution p(œÜ). Monte Carlo and Markov chain Monte Carlo are two ways
of generating such a sequence. Monte Carlo simulation, in which we generate independent samples from the target distribution, is in some sense the
‚Äúgold standard.‚Äù Independent MC samples automatically create a sequence
that
is representative of p(œÜ): The probability that œÜ(s) ‚àà A for any set A is
R
p(œÜ) dœÜ. This is true for every s ‚àà {1, . . . , S} and conditionally or unconA
ditionally on the other values in the sequence. This is not true for MCMC
samples, in which case all we are sure of is that

6.6 Introduction to MCMC diagnostics

lim Pr(œÜ(s) ‚àà A) =

s‚Üí‚àû

99

Z
p(œÜ) dœÜ.
A

0.0

0.1

p( Œ∏)
0.2

0.3

0.4

Let‚Äôs explore the differences between MC and MCMC with a simple example. Our target distribution will be the joint probability distribution of
two variables: a discrete variable Œ¥ ‚àà {1, 2, 3} and a continuous variable
Œ∏ ‚àà R. The target density for this example will be defined as {Pr(Œ¥ =
1), Pr(Œ¥ = 2), Pr(Œ¥ = 3)} = (.45, .10, .45) and p(Œ∏|Œ¥) = dnorm(Œ∏, ¬µŒ¥ , œÉŒ¥ ), where
(¬µ1 , ¬µ2 , ¬µ3 ) = (‚àí3, 0, 3) and (œÉ12 , œÉ22 , œÉ32 ) = (1/3, 1/3, 1/3). This is a mixture
of three normal densities, where we might think of Œ¥ as being a group membership variable and (¬µŒ¥ , œÉŒ¥2 ) as the population meanPand variance for group
Œ¥. A plot of the exact marginal density of Œ∏, p(Œ∏) =
p(Œ∏|Œ¥)p(Œ¥), appears in
the black lines of Figure 6.4. Notice that there are three modes representing
the three different group means.

‚àí6

‚àí4

‚àí2

0
Œ∏

2

4

6

Fig. 6.4. A mixture of normal densities and a Monte Carlo approximation.

It is very easy to obtain independent Monte Carlo samples from the joint
distribution of œÜ = (Œ¥, Œ∏). First, a value of Œ¥ is sampled from its marginal
distribution, then the value is plugged into p(Œ∏|Œ¥), from which a value of Œ∏
is sampled. The sampled pair (Œ¥, Œ∏) represents a sample from the joint distribution of p(Œ¥, Œ∏) = p(Œ¥)p(Œ∏|Œ¥). The empirical distribution of the
P Œ∏-samples
provides an approximation to the marginal distribution p(Œ∏) =
p(Œ∏|Œ¥)p(Œ¥).
A histogram of 1,000 Monte Carlo Œ∏-values generated in this way is shown in
Figure 6.4. The empirical distribution of the Monte Carlo samples looks a lot
like p(Œ∏).
It is also straightforward to construct a Gibbs sampler for œÜ = (Œ¥, Œ∏). A
Gibbs sampler would alternately sample values of Œ∏ and Œ¥ from their full conditional distributions. The full conditional distribution of Œ∏ is already provided,

100

6 Posterior approximation with the Gibbs sampler

and using Bayes‚Äô rule we can show that the full conditional distribution of Œ¥
is given by
Pr(Œ¥ = d) √ó dnorm(Œ∏, ¬µd , œÉd )
Pr(Œ¥ = d|Œ∏) = P3
, for d ‚àà {1, 2, 3}.
d=1 Pr(Œ¥ = d) √ó dnorm(Œ∏, ¬µd , œÉd )

0.0

‚àí4

‚àí2

0.1

0

Œ∏

p( Œ∏)
0.2

2

0.3

4

0.4

The first panel of Figure 6.5 shows a histogram of 1,000 MCMC values of Œ∏
generated with the Gibbs sampler. Notice that the empirical distribution of
the MCMC samples gives a poor approximation to p(Œ∏). Values of Œ∏ near -3
are underrepresented, whereas values near zero and +3 are overrepresented.
What went wrong? A plot of the Œ∏-values versus iteration number in the second
panel of the figure tells the story. The Œ∏-values get ‚Äústuck‚Äù in certain regions,
and rarely move among the three regions represented by the three values of
¬µ. The technical term for this ‚Äústickiness‚Äù is autocorrelation, or correlation
between consecutive values of the chain. In this Gibbs sampler, if we have a
value of Œ∏ near 0 for example, then the next value of Œ¥ is likely to be 2. If Œ¥ is
2, then the next value of Œ∏ is likely to be near 0, resulting in a high degree of
positive correlation between consecutive Œ∏-values in the chain.
Isn‚Äôt the Gibbs sampler guaranteed to eventually provide a good approximation to p(Œ∏)? It is, but ‚Äúeventually‚Äù can be a very long time in some
situations. The first panel of Figure 6.6 indicates that our approximation has
greatly improved after using 10,000 iterations of the Gibbs sampler, although
it is still somewhat inadequate.

‚àí6

‚àí4

‚àí2

0
Œ∏

2

4

6

‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè

0

200

400
600
iteration

800

1000

Fig. 6.5. Histogram and traceplot of 1,000 Gibbs samples.

In the case of a generic parameter œÜ and target distribution p(œÜ), it is
helpful to think of the sequence {œÜ(1) , . . . , œÜ(S) } as the trajectory of a particle
œÜ moving around the parameter space. In terms of MCMC integral approxi-

0.0

‚àí4

0.1

‚àí2

Œ∏
0

p( Œ∏)
0.2

2

0.3

4

0.4

6.6 Introduction to MCMC diagnostics

‚àí6

‚àí4

‚àí2

0
Œ∏

2

4

6

101

‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

0

2000

4000 6000
iteration

8000

Fig. 6.6. Histogram and traceplot of 10,000 Gibbs samples.

mation, the critical thing is that the amount of time Rthe particle spends in a
given set A is proportional to the target probability A p(œÜ) dœÜ.
Now suppose A1 , A2 and A3 are three disjoint subsets of the parameter
space, with Pr(A2 ) < Pr(A1 ) ‚âà Pr(A3 ) (these could be, for example, the
regions near the three modes of the normal mixture distribution above). In
terms of the integral approximation, this means that we want the particle to
spend little time in A2 , and about the same amount of time in A1 as in A3 .
Since in general we do not know p(œÜ) (otherwise we would not be trying to
approximate it), it is possible that we would accidentally start our Markov
chain in A2 . In this case, it is critical that the number of iterations S is large
enough so that the particle has a chance to
1. move out of A2 and into higher probability regions, and
2. move between A1 and A3 , and any other sets of high probability.
The technical term for attaining item 1 is to say that the chain has achieved
stationarity or has converged . If your Markov chain starts off in a region of
the parameter space that has high probability, then convergence generally is
not a big issue. If you do not know if you are starting off in a good region,
assessing convergence is fraught with epistemological problems. In general,
you cannot know for sure if your chain has converged. But sometimes you
can know if your chain has not converged, so we at least check for this latter
possibility. One thing to check for is stationarity, or that samples taken in
one part of the chain have a similar distribution to samples taken in other
parts. For the normal model with semiconjugate prior distributions from the
previous section, stationarity is achieved quite quickly and is not a big issue.
However, for some highly parameterized models that we will see later on, the
autocorrelation in the chain is high, good starting values can be hard to find

102

6 Posterior approximation with the Gibbs sampler

and it can take a long time to get to stationarity. In these cases we need to
run the MCMC sampler for a very long time.
Item 2 above relates to how quickly the particle moves around the parameter space, which is sometimes called the speed of mixing. An independent
MC sampler has perfect mixing: It has zero autocorrelation and can jump between different regions of the parameter space in one step. As we have seen in
the example above, an MCMC sampler might have poor mixing, take a long
time between jumps to different parts of the parameter space and have a high
degree of autocorrelation. How does the correlation of the MCMC samples
affect posterior
approximation? Suppose we want to approximate the integral
R
E[œÜ] = œÜp(œÜ) dœÜ = œÜ0 using the empirical distribution of {œÜ(1) , . . . , œÜ(S) }.
If the œÜ-values P
are independent Monte Carlo samples from p(œÜ), then the
variance of œÜ¬Ø = œÜ(s) /S is
¬Ø = E[(œÜ¬Ø ‚àí œÜ0 )2 ] = Var[œÜ] ,
VarMC [œÜ]
S
R
where Var[œÜ] = œÜ2 p(œÜ) dœÜ ‚àí œÜ20 . Recall from Chapter 4 that the square
¬Ø is the Monte Carlo standard error, and is a measure of how
root of VarMC [œÜ]
R
well we expect œÜ¬Ø to approximate the integral œÜp(œÜ) dœÜ. If we were to rerun
the MC approximation procedure many times, perhaps with different starting
values or random number generators, we expect that œÜp
0 , the true value of the
¬Ø for roughly
integral, would be contained within the interval œÜ¬Ø ¬± 2 VarMC [œÜ]
p
¬Ø
95% of the MC approximations. The width of this interval is 4 √ó VarMC [œÜ],
and we can make this as small as we want by generating more MC samples.
What if we use an MCMC algorithm such as the Gibbs sampler? As can
be seen in Figures 6.5 and 6.6, consecutive MCMC samples œÜ(s) and œÜ(s+1)
can be positively correlated. Assuming stationarity has been achieved, the
expected squared
difference from the MCMC integral approximation œÜ¬Ø to the
R
target œÜ0 = œÜp(œÜ) dœÜ is the MCMC variance, and is given by
¬Ø = E[(œÜ¬Ø ‚àí œÜ0 )2 ]
VarMCMC [œÜ]
1 X (s)
= E[{
(œÜ ‚àí œÜ0 )}2 ]
S
S
X
1 X
= 2 E[ (œÜ(s) ‚àí œÜ0 )2 +
(œÜ(s) ‚àí œÜ0 )(œÜ(t) ‚àí œÜ0 )]
S
s=1
s6=t

=

1
S2

S
X

E[(œÜ(s) ‚àí œÜ0 )2 ] +

s=1

1 X
E[(œÜ(s) ‚àí œÜ0 )(œÜ(t) ‚àí œÜ0 )]
S2
s6=t

X
¬Ø + 1
E[(œÜ(s) ‚àí œÜ0 )(œÜ(t) ‚àí œÜ0 )].
= VarMC [œÜ]
2
S
s6=t

So the MCMC variance is equal to the MC variance plus a term that depends
on the correlation of samples within the Markov chain. This term is generally

6.6 Introduction to MCMC diagnostics

103

positive and so the MCMC variance is higher than the MC variance, meaning
that we expect the MCMC approximation to be further away from œÜ0 than the
MC approximation is. The higher the autocorrelation in the chain, the larger
the MCMC variance and the worse the approximation is. To assess how much
correlation there is in the chain we often compute the sample autocorrelation
function. For a generic sequence of numbers {œÜ1 , . . . , œÜS }, the lag-t autocorrelation function estimates the correlation between elements of the sequence
that are t steps apart:
acf t (œÜ) =

1
S‚àít

PS‚àít

¬Ø

s=1 (œÜs ‚àí œÜ)(œÜs+t ‚àí
PS
1
¬Ø2
s=1 (œÜs ‚àí œÜ)
S‚àí1

¬Ø
œÜ)

,

which is computed by the R-function acf . For the sequence of 10,000 Œ∏-values
plotted in Figure 6.6, the lag-10 autocorrelation is 0.93, and the lag-50 autocorrelation is 0.812. A Markov chain with such a high autocorrelation moves
around the parameter space slowly, taking a long time to achieve the correct
balance among the different regions of the parameter space. The higher the
autocorrelation, the more MCMC samples we need to attain a given level
of precision for our approximation. One way to measure this is to calculate the effective sample size for an MCMC sequence, using the R-command
effectiveSize in the ‚Äúcoda‚Äù package. The effective sample size function estimates the value Seff such that
¬Ø =
VarMCMC [œÜ]

Var[œÜ]
,
Seff

so that Seff can be interpreted as the number of independent Monte Carlo
samples necessary to give the same precision as the MCMC samples. For the
normal mixture density example above, the effective sample size of the 10,000
Gibbs samples of Œ∏ is 18.42, indicating that the precision of the MCMC approximation to E[Œ∏] is as good as the precision that would have been obtained
by only about 18 independent samples of Œ∏.
There is a large literature on the practical implementation and assessment
of Gibbs sampling and MCMC approximation. Much insight can be gained
by hands-on experience supplemented by reading books and articles. A good
article to start with is ‚ÄúPractical Markov chain Monte Carlo‚Äù (Geyer, 1992),
which includes a discussion by many researchers and a large variety of viewpoints on and techniques for MCMC approximation.
MCMC diagnostics for the semiconjugate normal analysis
We now assess the Markov chain of Œ∏ and œÉ 2 values generated by the Gibbs
sampler in Section 6.4. Figure 6.7 plots the values of these two parameters in sequential order, and seems to indicate immediate convergence and
a low degree of autocorrelation. The lag-1 autocorrelation for the sequence

104

6 Posterior approximation with the Gibbs sampler

0.10

1.9
1.8
Œ∏

œÉ2
0.02

‚óè
‚óè

1.5

‚óè

0

200

400
600
iteration

800

1000

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè‚óè

‚óè

1.6

1.7

‚óè

0.06

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè
‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè

0.14

{Œ∏(1) , . . . , Œ∏(1000) } is 0.031, which is essentially zero for approximation purposes. The effective sample size for this sequence is computed in R to be 1,000.
The lag-1 autocorrelation for the œÉ 2 -values is 0.147, with an effective sample
size of 742. While not quite as good as an independently sampled sequence
of parameter values, the Gibbs sampler for this model and prior distribution
performs quite well.

‚óè

‚óè
‚óè

‚óè‚óè
‚óè

‚óè ‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè

‚óè

0

200

400
600
iteration

800

1000

Fig. 6.7. Traceplots for Œ∏ and œÉ 2 .

6.7 Discussion and further references
The term ‚ÄúGibbs sampling‚Äù was coined by Geman and Geman (1984) in their
paper on image analysis, but the algorithm appears earlier in the context
of spatial statistics, for example, Besag (1974) or Ripley (1979). However,
the general utility of the Gibbs sampler for Bayesian data analysis was not
fully realized until the late 1980s (Gelfand and Smith, 1990). See Robert and
Casella (2008) for a historical review.
Assessing the convergence of the Gibbs sampler and the accuracy of the
MCMC approximation is difficult. Several authors have come up with convergence diagnostics (Gelman and Rubin, 1992; Geweke, 1992; Raftery and
Lewis, 1992), although these can only highlight problems and not guarantee
a good approximation (Geyer, 1992).

7
The multivariate normal model

Up until now all of our statistical models have been univariate models, that
is, models for a single measurement on each member of a sample of individuals
or each run of a repeated experiment. However, datasets are frequently multivariate, having multiple measurements for each individual or experiment. This
chapter covers what is perhaps the most useful model for multivariate data,
the multivariate normal model, which allows us to jointly estimate population
means, variances and correlations of a collection of variables. After first calculating posterior distributions under semiconjugate prior distributions, we
show how the multivariate normal model can be used to impute data that are
missing at random.

7.1 The multivariate normal density
Example: Reading comprehension
A sample of twenty-two children are given reading comprehension tests before
and after receiving a particular instructional method. Each student i will then
have two scores, Yi,1 and Yi,2 denoting the pre- and post-instructional scores
respectively. We denote each student‚Äôs pair of scores as a 2 √ó 1 vector Y i , so
that

 

Yi,1
score on first test
Yi=
=
.
Yi,2
score on second test
Things we might be interested in include the population mean Œ∏,

  
E[Yi,1 ]
Œ∏1
E[Y ] =
=
E[Yi,2 ]
Œ∏2
and the population covariance matrix Œ£,

  2

E[Y12 ] ‚àí E[Y1 ]2
E[Y1 Y2 ] ‚àí E[Y1 ]E[Y2 ]
œÉ1 œÉ1,2
Œ£ = Cov[Y ] =
=
,
E[Y1 Y2 ] ‚àí E[Y1 ]E[Y2 ]
E[Y22 ] ‚àí E[Y2 ]2
œÉ1,2 œÉ22
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 7,
c Springer Science+Business Media, LLC 2009


106

7 The multivariate normal model

where the expectations above represent the unknown population averages.
Having information about Œ∏ and Œ£ may help us in assessing the effectiveness
of the teaching method, possibly evaluated with Œ∏2 ‚àí Œ∏1 , or the consistency of
the reading comprehension
p test, which could be evaluated with the correlation
coefficient œÅ1,2 = œÉ1,2 / œÉ12 œÉ22 .
The multivariate normal density
Notice that Œ∏ and Œ£ are both functions of population moments, or population
averages of powers of Y1 and Y2 . In particular, Œ∏ and Œ£ are functions of firstand second-order moments:
first-order moments: E[Y1 ], E[Y2 ]
second-order moments: E[Y12 ], E[Y1 Y2 ], E[Y22 ]
Recall from Chapter 5 that a univariate normal model describes a population
in terms of its mean and variance (Œ∏, œÉ 2 ), or equivalently its first two moments
(E[Y ] = Œ∏, E[Y 2 ] = œÉ 2 + Œ∏2 ). The analogous model for describing first- and
second-order moments of multivariate data is the multivariate normal model.
We say a p-dimensional data vector Y has a multivariate normal distribution
if its sampling density is given by
p(y|Œ∏, Œ£) = (2œÄ)‚àíp/2 |Œ£|‚àí1/2 exp{‚àí(y ‚àí Œ∏)T Œ£ ‚àí1 (y ‚àí Œ∏)/2}
where

Ô£´

Ô£∂
y1
Ô£¨ y2 Ô£∑
Ô£¨ Ô£∑
y=Ô£¨ . Ô£∑
Ô£≠ .. Ô£∏
yp

Ô£´

Ô£∂
Œ∏1
Ô£¨ Œ∏2 Ô£∑
Ô£¨ Ô£∑
Œ∏=Ô£¨ . Ô£∑
Ô£≠ .. Ô£∏
Œ∏p

Ô£∂
œÉ12 œÉ1,2 ¬∑ ¬∑ ¬∑ œÉ1,p
Ô£¨ œÉ1,2 œÉ22 ¬∑ ¬∑ ¬∑ œÉ2,p Ô£∑
Ô£¨
Ô£∑
Œ£=Ô£¨ .
..
.. Ô£∑ .
Ô£≠ ..
.
. Ô£∏
œÉ1,p ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ œÉp2
Ô£´

Calculating this density requires a few operations involving matrix algebra.
For a matrix A, the value of |A| is called the determinant of A, and measures
how ‚Äúbig‚Äù A is. The inverse of A is the matrix A‚àí1 such that AA‚àí1 is equal
to the identity matrix Ip , the p√óp matrix that has ones for its diagonal entries
but is otherwise zero. For a p √ó 1 vector b, bT is its transpose, and is simply
T
the 1 √ó p vector of the same values.
Finally, the
Pp
Ppvector-matrix product b A
is equal to the 1 √ó p vector ( j=1 bj aj,1 , . . . , j=1 bj aj,p ), and the value of
Pp Pp
bT Ab is the single number j=1 k=1 bj bk aj,k . Fortunately, R can compute
all of these quantities for us, as we shall see in the forthcoming example code.
Figure 7.1 gives contour plots and 30 samples from each of three different
two-dimensional multivariate normal densities. In each one Œ∏ = (50, 50)T ,
œÉ12 = 64, œÉ22 = 144, but the value of œÉ1,2 varies from plot to plot, with œÉ1,2 =
‚àí48 for the left density, 0 for the middle and +48 for the density on the right
(giving correlations of -.5, 0 and +.5 respectively). An interesting feature of
the multivariate normal distribution is that the marginal distribution of each
variable Yj is a univariate normal distribution, with mean Œ∏j and variance œÉj2 .

7.2 A semiconjugate prior distribution for the mean

107

x
x
x
xxx xx
xx x
x x x x xx
x xx
x xx
x xx
x x

x

20

40

60
y1

80

y2
20 30 40 50 60 70 80

y2
20 30 40 50 60 70 80

y2
20 30 40 50 60 70 80

This means that the marginal distributions for Y1 from the three populations
in Figure 7.1 are identical (the same holds for Y2 ). The only thing that differs
across the three populations is the relationship between Y1 and Y2 , which is
controlled by the covariance parameter œÉ1,2 .

xx x x
x x
x xxxx
xx x
x xxx x
xx x x x x xx
x
x
x

20

40

60

80

x
x
x
x
x xx
xxx x xx
x xxxxxxx x
x x
xx x

xx

20

40

y1

60

80

y1

Fig. 7.1. Multivariate normal samples and densities.

7.2 A semiconjugate prior distribution for the mean
Recall from Chapters 5 and 6 that if Y1 , . . . , Yn are independent samples from
a univariate normal population, then a convenient conjugate prior distribution
for the population mean is also univariate normal. Similarly, a convenient prior
distribution for the multivariate mean Œ∏ is a multivariate normal distribution,
which we will parameterize as
p(Œ∏) = multivariate normal(¬µ0 , Œõ0 ),
where ¬µ0 and Œõ0 are the prior mean and variance of Œ∏, respectively. What is
the full conditional distribution of Œ∏, given y 1 , . . . , y n and Œ£? In the univariate
case, having normal prior and sampling distributions resulted in a normal full
conditional distribution for the population mean. Let‚Äôs see if this result holds
for the multivariate case. We begin by examining the prior distribution as a
function of Œ∏:
1
p(Œ∏) = (2œÄ)‚àíp/2 |Œõ0 |‚àí1/2 exp{‚àí (Œ∏ ‚àí ¬µ0 )T Œõ‚àí1
0 (Œ∏ ‚àí ¬µ0 )}
2
1
1 T ‚àí1
T ‚àí1
= (2œÄ)‚àíp/2 |Œõ0 |‚àí1/2 exp{‚àí Œ∏ T Œõ‚àí1
0 Œ∏ + Œ∏ Œõ0 ¬µ0 ‚àí ¬µ0 Œõ0 ¬µ0 }
2
2
1 T ‚àí1
T ‚àí1
‚àù exp{‚àí Œ∏ Œõ0 Œ∏ + Œ∏ Œõ0 ¬µ0 }
2
1
= exp{‚àí Œ∏ T A1 Œ∏ + Œ∏ T b1 },
(7.1)
2

108

7 The multivariate normal model

where A0 = Œõ‚àí1
and b0 = Œõ‚àí1
0
0 ¬µ0 . Conversely, Equation 7.1 says that if a
random vector Œ∏ has a density on Rp that is proportional to exp{‚àíŒ∏ T AŒ∏/2 +
Œ∏ T b} for some matrix A and vector b, then Œ∏ must have a multivariate normal
distribution with covariance A‚àí1 and mean A‚àí1 b.
If our sampling model is that {Y 1 , . . . , Y n |Œ∏, Œ£} are i.i.d. multivariate
normal(Œ∏, Œ£), then similar calculations show that the joint sampling density
of the observed vectors y 1 , . . . , y n is
p(y 1 , . . . , y n |Œ∏, Œ£) =

n
Y

(2œÄ)‚àíp/2 |Œ£|‚àí1/2 exp{‚àí(y i ‚àí Œ∏)T Œ£ ‚àí1 (y i ‚àí Œ∏)/2}

i=1
n

= (2œÄ)‚àínp/2 |Œ£|‚àín/2 exp{‚àí

1X
(y ‚àí Œ∏)T Œ£ ‚àí1 (y i ‚àí Œ∏)}
2 i=1 i

1
‚àù exp{‚àí Œ∏ T A1 Œ∏ + Œ∏ T b1 },
2

(7.2)

‚àí1
¬Ø and y
¬Ø is the vector of variable-specific
where A1 = nŒ£P
, b1 = nŒ£ ‚àí1P
y
n
1
¬Ø = ( n i=1 yi,1 , . . . , n1 ni=1 yi,p )T . Combining Equations 7.1 and
averages y
7.2 gives

1
1
p(Œ∏|y 1 , . . . , y n , Œ£) ‚àù exp{‚àí Œ∏ T A0 Œ∏ + Œ∏ T b0 } √ó exp{‚àí Œ∏ T A1 Œ∏ + Œ∏ T b1 }
2
2
1 T
T
= exp{‚àí Œ∏ An Œ∏ + Œ∏ bn }, where
(7.3)
2
‚àí1
An = A0 + A1 = Œõ‚àí1
and
0 + nŒ£
‚àí1
‚àí1
¬Ø.
bn = b0 + b1 = Œõ0 ¬µ0 + nŒ£ y
From the comments in the previous paragraph, Equation 7.3 implies that
the conditional distribution of Œ∏ therefore must be a multivariate normal
‚àí1
distribution with covariance A‚àí1
n and mean An bn , so
‚àí1 ‚àí1
Cov[Œ∏|y 1 , . . . , y n , Œ£] = Œõn = (Œõ‚àí1
)
(7.4)
0 + nŒ£
‚àí1
‚àí1
‚àí1 ‚àí1
‚àí1
¬Ø ) (7.5)
E[Œ∏|y 1 , . . . , y n , Œ£] = ¬µn = (Œõ0 + nŒ£ ) (Œõ0 ¬µ0 + nŒ£ y
p(Œ∏|y 1 , . . . , y n , Œ£) = multivariate normal(¬µn , Œõn ).
(7.6)

It looks a bit complicated, but can be made more understandable by analogy
with the univariate normal case: Equation 7.4 says that posterior precision, or
inverse variance, is the sum of the prior precision and the data precision, just as
in the univariate normal case. Similarly, Equation 7.5 says that the posterior
expectation is a weighted average of the prior expectation and the sample
mean. Notice that, since the sample mean is consistent for the population
mean, the posterior mean also will be consistent for the population mean
even if the true distribution of the data is not multivariate normal.

7.3 The inverse-Wishart distribution

109

7.3 The inverse-Wishart distribution
Just as a variance œÉ 2 must be positive, a variance-covariance matrix Œ£ must
be positive definite, meaning that
x0 Œ£x > 0 for all vectors x.
Positive definiteness guarantees that œÉj2 > 0 for all j and that all correlations
are between -1 and 1. Another requirement of our covariance matrix is that it
is symmetric, which means that œÉj,k = œÉk,j . Any valid prior distribution for
Œ£ must put all of its probability mass on this complicated set of symmetric,
positive definite matrices. How can we formulate such a prior distribution?
Empirical covariance matrices
The sum of squares matrix of a collection of multivariate vectors z 1 , . . . , z n
is given by
n
X
z i z Ti = ZT Z,
i=1

where Z is the n √ó p matrix whose ith row is z Ti . Recall from matrix algebra
that since z i can be thought of as a p √ó 1 matrix, z i z Ti is the following p √ó p
matrix:
Ô£´ 2
Ô£∂
zi,1 zi,1 zi,2 ¬∑ ¬∑ ¬∑ zi,1 zi,p
2
Ô£¨ zi,2 zi,1 zi,2
¬∑ ¬∑ ¬∑ zi,2 zi,p Ô£∑
Ô£¨
Ô£∑
z i z Ti = Ô£¨ .
.. Ô£∑ .
Ô£≠ ..
. Ô£∏
2
zi,p zi,1 zi,p zi,2 ¬∑ ¬∑ ¬∑ zi,p
If the z i ‚Äôs are samples from a population with zero mean, we can think of
the matrix z i z Ti /n as the contribution of vector z i to the estimate of the
covariance matrix of all of the observations. In this mean-zero case, if we
divide ZT Z by n, we get a sample covariance matrix, an unbiased estimator
of the population covariance matrix:
Pn
T
1
1
2
2
n [Z Z]j,j = n Pi=1 zi,j = sj,j = sj
n
T
1
1
i=1 zi,j zi,k = sj,k .
n [Z Z]j,k = n
If n > p and the z i ‚Äôs are linearly independent, then ZT Z will be positive
definite and symmetric. This suggests the following construction of a ‚Äúrandom‚Äù covariance matrix: For a given positive integer ŒΩ0 and a p √ó p covariance
matrix Œ¶0 ,
1. sample z 1 , . . . , z ŒΩP
‚àº i.i.d. multivariate normal(0, Œ¶0 );
0
ŒΩ0
2. calculate ZT Z = i=1
z i z Ti .
We can repeat this procedure over and over again, generating matrices
ZT1 Z1 ,. . .,ZTS ZS . The population distribution of these sum of squares matrices is called a Wishart distribution with parameters (ŒΩ0 , Œ¶0 ), which has the
following properties:

110

7 The multivariate normal model

‚Ä¢ If ŒΩ0 > p, then ZT Z is positive definite with probability 1.
‚Ä¢ ZT Z is symmetric with probability 1.
‚Ä¢ E[ZT Z] = ŒΩ0 Œ¶0 .
The Wishart distribution is a multivariate analogue of the gamma distribution
(recall that if z is a mean-zero univariate normal random variable, then z 2 is a
gamma random variable). In the univariate normal model, our prior distribution for the precision 1/œÉ 2 is a gamma distribution, and our full conditional
distribution for the variance is an inverse-gamma distribution. Similarly, it
turns out that the Wishart distribution is a semi-conjugate prior distribution
for the precision matrix Œ£ ‚àí1 , and so the inverse-Wishart distribution is our
semi-conjugate prior distribution for the covariance matrix Œ£. With a slight
reparameterization, to sample a covariance matrix Œ£ from an inverse-Wishart
distribution we perform the following steps:
1. sample z 1 , . . . , z ŒΩP
‚àº i.i.d. multivariate normal(0, S‚àí1
0
0 );
ŒΩ0
T
2. calculate Z Z = i=1
z i z Ti ;
3. set Œ£ = (ZT Z)‚àí1 .
Under this simulation scheme, the precision matrix Œ£ ‚àí1 has a Wishart(ŒΩ0 , S‚àí1
0 )
distribution and the covariance matrix Œ£ has an inverse-Wishart(ŒΩ0 , S‚àí1
0 ) distribution. The expectations of Œ£ ‚àí1 and Œ£ are
E[Œ£ ‚àí1 ] = ŒΩ0 S‚àí1
0
1
1
‚àí1
E[Œ£] =
(S‚àí1
=
S0 .
0 )
ŒΩ0 ‚àí p ‚àí 1
ŒΩ0 ‚àí p ‚àí 1
If we are confident that the true covariance matrix is near some covariance
matrix Œ£0 , then we might choose ŒΩ0 to be large and set S0 = (ŒΩ0 ‚àí p ‚àí 1)Œ£0 ,
making the distribution of Œ£ concentrated around Œ£0 . On the other hand,
choosing ŒΩ0 = p + 2 and S0 = Œ£0 makes Œ£ only loosely centered around Œ£0 .
Full conditional distribution of the covariance matrix
The inverse-Wishart(ŒΩ0 , S‚àí1
0 ) density is given by
Ô£Æ
Ô£π‚àí1
p
Y
p
/2
p(Œ£) = Ô£∞2ŒΩ0 p/2 œÄ (2) |S0 |‚àíŒΩ0 /2
Œì ([ŒΩ0 + 1 ‚àí j]/2)Ô£ª √ó
j=1

|Œ£|‚àí(ŒΩ0 +p+1)/2 √ó exp{‚àítr(S0 Œ£ ‚àí1 )/2}.

(7.7)

The normalizing constant is quite intimidating. Fortunately we will only have
to work with the second line of the equation. The P
expression ‚Äútr‚Äù stands for
p
trace and for a square p √ó p matrix A, tr(A) = j=1 aj,j , the sum of the
diagonal elements.
We now need to combine the above prior distribution with the sampling
distribution for Y 1 , . . . , Y n :

7.3 The inverse-Wishart distribution

p(y 1 , . . . , y n |Œ∏, Œ£) = (2œÄ)‚àínp/2 |Œ£|‚àín/2 exp{‚àí

n
X

111

(y i ‚àí Œ∏)T Œ£ ‚àí1 (y i ‚àí Œ∏)/2} .

i=1

(7.8)
PK
T
An interesting result from matrix algebra is that the sum
b
Ab
k =
k=1 k
tr(BT BA), where B is the matrix whose kth row is bTk . This means that the
term in the exponent of Equation 7.8 can be expressed as
n
X

(y i ‚àí Œ∏)T Œ£ ‚àí1 (y i ‚àí Œ∏) = tr(SŒ∏ Œ£ ‚àí1 ), where

i=1

SŒ∏ =

n
X

(y i ‚àí Œ∏)(y i ‚àí Œ∏)T .

i=1

The matrix SŒ∏ is the residual sum of squares matrix for the vectors y 1 , . . . , y n
if the population mean is presumed to be Œ∏. Conditional on Œ∏, n1 SŒ∏ provides
an unbiased estimate of the true covariance matrix Cov[Y ] (more
generally,
P
¬Ø )(y i ‚àí
when Œ∏ is not conditioned on the sample covariance matrix is (y i ‚àí y
¬Ø )T /(n ‚àí 1) and is an unbiased estimate of Œ£). Using the above result to
y
combine Equations 7.7 and 7.8 gives the conditional distribution of Œ£:
p(Œ£|y 1 , . . . , y n , Œ∏)
‚àù p(Œ£) √ó p(y 1 , . . . y n |Œ∏, Œ£)

 

‚àù |Œ£|‚àí(ŒΩ0 +p+1)/2 exp{‚àítr(S0 Œ£ ‚àí1 )/2} √ó |Œ£|‚àín/2 exp{‚àítr(SŒ∏ Œ£ ‚àí1 )/2}
= |Œ£|‚àí(ŒΩ0 +n+p+1)/2 exp{‚àítr([S0 + SŒ∏ ]Œ£ ‚àí1 )/2} .
Thus we have
{Œ£|y 1 , . . . , y n , Œ∏} ‚àº inverse-Wishart(ŒΩ0 + n, [S0 + SŒ∏ ]‚àí1 ).

(7.9)

Hopefully this result seems somewhat intuitive: We can think of ŒΩ0 + n as the
‚Äúposterior sample size,‚Äù being the sum of the ‚Äúprior sample size‚Äù ŒΩ0 and the
data sample size. Similarly, S0 + SŒ∏ can be thought of as the ‚Äúprior‚Äù residual
sum of squares plus the residual sum of squares from the data. Additionally,
the conditional expectation of the population covariance matrix is
1
(S0 + SŒ∏ )
ŒΩ0 + n ‚àí p ‚àí 1
ŒΩ0 ‚àí p ‚àí 1
1
n
1
=
S0 +
SŒ∏
ŒΩ0 + n ‚àí p ‚àí 1 ŒΩ0 ‚àí p ‚àí 1
ŒΩ0 + n ‚àí p ‚àí 1 n

E[Œ£|y 1 , . . . , y n , Œ∏] =

and so the conditional expectation can be seen as a weighted average of the
prior expectation and the unbiased estimator. Because it can be shown that SŒ∏
converges to the true population covariance matrix, the posterior expectation
of Œ£ is a consistent estimator of the population covariance, even if the true
population distribution is not multivariate normal.

112

7 The multivariate normal model

7.4 Gibbs sampling of the mean and covariance
In the last two sections we showed that
{Œ∏|y 1 , . . . , y n , Œ£} ‚àº multivariate normal(¬µn , Œõn )
{Œ£|y 1 , . . . , y n , Œ∏} ‚àº inverse-Wishart(ŒΩn , S‚àí1
n ),
where {Œõn , ¬µn } are defined in Equations 7.4 and 7.5, ŒΩn = ŒΩ0 + n and Sn =
S0 + SŒ∏ . These full conditional distributions can be used to construct a Gibbs
sampler, providing us with an MCMC approximation to the joint posterior
distribution p(Œ∏, Œ£|y 1 , . . . , y n ). Given a starting value Œ£ (0) , the Gibbs sampler
generates {Œ∏ (s+1) , Œ£ (s+1) } from {Œ∏ (s) , Œ£ (s) } via the following two steps:
1. Sample Œ∏ (s+1) from its full conditional distribution:
a) compute ¬µn and Œõn from y 1 , . . . , y n and Œ£ (s) ;
b) sample Œ∏ (s+1) ‚àº multivariate normal(¬µn , Œõn ).
2. Sample Œ£ (s+1) from its full conditional distribution:
a) compute Sn from y 1 , . . . , y n and Œ∏ (s+1) ;
b) sample Œ£ (s+1) ‚àº inverse-Wishart(ŒΩ0 + n, S‚àí1
n ).
Steps 1.a and 2.a highlight the fact that {¬µn , Œõn } depend on the value of Œ£,
and that Sn depends on the value of Œ∏, and so these quantities need to be
recalculated at every iteration of the sampler.
Example: Reading comprehension
Let‚Äôs return to the example from the beginning of the chapter in which each
of 22 children were given two reading comprehension exams, one before a
certain type of instruction and one after. We‚Äôll model these 22 pairs of scores as
i.i.d. samples from a multivariate normal distribution. The exam was designed
to give average scores of around 50 out of 100, so ¬µ0 = (50, 50)T would
be a good choice for our prior expectation. Since the true mean cannot be
below 0 or above 100, it is desirable to use a prior variance for Œ∏ that puts
little probability outside of this range. We‚Äôll take the prior variances on Œ∏1
and Œ∏2 to be Œª20,1 = Œª20,2 = (50/2)2 = 625, so that the prior probability
Pr(Œ∏j 6‚àà [0, 100]) is only 0.05. Finally, since the two exams are measuring
similar things, whatever the true values of Œ∏1 and Œ∏2 are it is probable that
they are close. We can reflect this with a prior correlation of 0.5, so that
Œª1,2 = 312.5. As for the prior distribution on Œ£, some of the same logic about
the range of exam scores applies. We‚Äôll take S0 to be the same as Œõ0 , but only
loosely center Œ£ around this value by taking ŒΩ0 = p + 2 = 4.
mu0<‚àíc ( 5 0 , 5 0 )
L0<‚àíma t r i x ( c ( 6 2 5 , 3 1 2 . 5 , 3 1 2 . 5 , 6 2 5 ) , nrow=2 , n c o l =2)
nu0<‚àí4
S0<‚àíma t r i x ( c ( 6 2 5 , 3 1 2 . 5 , 3 1 2 . 5 , 6 2 5 ) , nrow=2 , n c o l =2)

7.4 Gibbs sampling of the mean and covariance

113

The observed values y 1 , . . . , y 22 are plotted as dots in the second panel
¬Ø = (47.18, 53.86)T , the sample variances
of Figure 7.2. The sample mean is y
2
2
are s1 = 182.16 and s2 = 243.65, and the sample correlation is s1,2 /(s1 s2 ) =
0.70. Let‚Äôs use the Gibbs sampler described above to combine this sample
information with our prior distributions to obtain estimates and confidence
intervals for the population parameters. We begin by setting Œ£ (0) equal to the
sample covariance matrix, and iterating from there. In the R-code below, Y is
the 22 √ó 2 data matrix of the observed values.
data ( c h a p t e r 7 ) ; Y<‚àíY. r e a d i n g
n<‚àídim (Y ) [ 1 ] ; ybar<‚àía p p ly (Y, 2 , mean )
Sigma<‚àícov (Y) ; THETA<‚àíSIGMA<‚àíNULL
set . seed (1)
for ( s in 1:5000)
{
###update t h e t a
Ln<‚àís o l v e ( s o l v e ( L0 ) + n‚àó s o l v e ( Sigma ) )
mun<‚àíLn%‚àó%( s o l v e ( L0)%‚àó%mu0 + n‚àó s o l v e ( Sigma)%‚àó%ybar )
t h e t a <‚àírmvnorm ( 1 , mun , Ln )
###
###update Sigma
Sn<‚àí S0 + ( t (Y)‚àíc ( t h e t a ) )%‚àó%t ( t (Y)‚àíc ( t h e t a ) )
Sigma<‚àís o l v e ( r w i s h ( 1 , nu0+n , s o l v e ( Sn ) ) )
###
### s a v e r e s u l t s
THETA<‚àír b i n d (THETA, t h e t a ) ; SIGMA<‚àír b i n d (SIGMA, c ( Sigma ) )
###
}

The above code generates 5,000 values ({Œ∏ (1) , Œ£ (1) }), . . . , {Œ∏ (5000) , Œ£ (5000) })
whose empirical distribution approximates p(Œ∏, Œ£|y 1 , . . . , y n ). It is left as an
exercise to assess the convergence and autocorrelation of this Markov chain.
From these samples we can approximate posterior probabilities and confidence
regions of interest.
> quantile (
THETA[ , 2 ] ‚àíTHETA[ , 1 ] , prob=c ( . 0 2 5 , . 5 , . 9 7 5 ) )
2.5%
50%
97.5%
1.513573 6.668097 11.794824

> mean ( THETA[ , 2 ] >THETA[ , 1 ] )
[ 1 ] 0.9942

The posterior probability Pr(Œ∏2 > Œ∏1 |y 1 , . . . , y n ) = 0.99 indicates strong evidence that, if we were to give exams and instruction to a large population

114

7 The multivariate normal model

65

100

of children, then the average score on the second exam would be higher than
that on the first. This evidence is displayed graphically in the first panel of
Figure 7.2, which shows 97.5%, 75%, 50%, 25% and 2.5% highest posterior
density contours for the joint posterior distribution of Œ∏ = (Œ∏1 , Œ∏2 )T . A highest posterior density contour is a two-dimensional analogue of a confidence
interval. The contours for the posterior distribution of Œ∏ are all mostly above
the 45-degree line Œ∏1 = Œ∏2 .

80

‚óè

60

‚óè ‚óè
‚óè

Œ∏2
55

60

‚óè
‚óè

y2

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè
‚óè

40

50

‚óè
‚óè

‚óè

‚óè‚óè

‚óè

0

40

45

20

‚óè

35

40

45

50
Œ∏1

55

60

0

20

40

60

80

100

y1

Fig. 7.2. Reading comprehension data and posterior distributions

Now let‚Äôs ask a slightly different question - what is the probability that
a randomly selected child will score higher on the second exam than on
the first? The answer to this question is a function of the posterior predictive distribution of a new sample (Y1 , Y2 )T , given the observed values.
The second panel of Figure 7.2 shows highest posterior density contours of
the posterior predictive distribution, which, while mostly being above the
line y2 = y1 , still has substantial overlap with the region below this line,
and in fact Pr(Y2 > Y1 |y 1 , . . . , y n ) = 0.71. How should we evaluate the
effectiveness of the between-exam instruction? On one hand, the fact that
Pr(Œ∏2 > Œ∏1 |y 1 , . . . , y n ) = 0.99 seems to suggest that there is a ‚Äúhighly
significant difference‚Äù in exam scores before and after the instruction, yet
Pr(Y2 > Y1 |y 1 , . . . , y n ) = 0.71 says that almost a third of the students will
get a lower score on the second exam. The difference between these two probabilities is that the first is measuring the evidence that Œ∏2 is larger than Œ∏1
without regard to whether or not the magnitude of the difference Œ∏2 ‚àí Œ∏1 is
large compared to the sampling variability of the data. Confusion over these
two different ways of comparing populations is common in the reporting of
results from experiments or surveys: studies with very large values of n often
result in values of Pr(Œ∏2 > Œ∏1 |y 1 , . . . , y n ) that are very close to 1 (or p-values

7.5 Missing data and imputation

115

that are very close to zero), suggesting a ‚Äúsignificant effect,‚Äù even though
such results say nothing about how large of an effect we expect to see for a
randomly sampled individual.

7.5 Missing data and imputation

60

‚óè

‚óè

30

40

‚óè

‚óè

40
30
20

40
80 100

‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè ‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè

40

‚óè

60

30

40
40

60

80
skin

‚óè

20

‚óè

100

40

60

80 100

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè ‚óè‚óè
‚óè ‚óè‚óè ‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè

‚óè

20

30

40

‚óè
‚óè
‚óè
‚óè

80

‚óè
‚óè ‚óè

40
30
20

0 10
60

‚óè

‚óè

‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè

50
20

‚óè

180

‚óè

40

‚óè

140
‚óè
‚óè ‚óè ‚óè

Frequency
30

‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè

80 100

‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

100

0

100

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè ‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè

20 40 60 80
Y[, j1]

20

40

‚óè

‚óè

60
bmi

80 100

‚óè

‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè ‚óè

20

30

‚óè
‚óè
‚óè ‚óè

Frequency
30
50

‚óè

60

0 10

‚óè‚óè

180

‚óè
‚óè

80 100

60

‚óè

20

100

40

‚óè

40

100

20

140

60

80
60

‚óè

80

‚óè

60 80
Y[, j1]

‚óè
‚óè

‚óè

60

‚óè

100

‚óè

‚óè

40

‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè

‚óè

‚óè

20
40

‚óè

‚óè‚óè

‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè

‚óè

100
100

‚óè

80
‚óè

‚óè
‚óè ‚óè

‚óè ‚óè

‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè ‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè ‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

60

0 10

‚óè
‚óè

60
‚óè

180

Frequency
30

‚óè

‚óè

140
bp

‚óè

‚óè
‚óè

‚óè

‚óè

100

‚óè

60

‚óè
‚óè
‚óè

20

‚óè

40

180
140
100

‚óè

60

60

140
‚óè

‚óè

180

‚óè

‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè

40

140

‚óè

200

‚óè‚óè

50

‚óè

‚óè ‚óè
‚óè
‚óè
‚óè

‚óè

20

0
180

‚óè

60

100

100
150
Y[, j1]

‚óè

‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

40

10

60

80

Frequency
20 30

‚óè

50

100

‚óè
‚óè

80 100

100

40

glu

40

15

25

35

45

Fig. 7.3. Physiological data on 200 women.

Figure 7.3 displays univariate histograms and bivariate scatterplots for
four variables taken from a dataset involving health-related measurements on
200 women of Pima Indian heritage living near Phoenix, Arizona (Smith et al,
1988). The four variables are glu (blood plasma glucose concentration), bp
(diastolic blood pressure), skin ( skin fold thickness) and bmi (body mass
index). The first ten subjects in this dataset have the following entries:

116

7 The multivariate normal model

1
2
3
4
5
6
7
8
9
10

glu
86
195
77
NA
107
97
NA
193
142
128

bp skin bmi
68
28 30.2
70
33
NA
82
NA 35.8
76
43 47.9
60
NA
NA
76
27
NA
58
31 34.3
50
16 25.9
80
15
NA
78
NA 43.3

The NA‚Äôs stand for ‚Äúnot available,‚Äù and so some data for some individuals
are ‚Äúmissing.‚Äù Missing data are fairly common in survey data: Sometimes
people accidentally miss a page of a survey, sometimes a doctor forgets to
write down a piece of medical data, sometimes the response is unreadable,
and so on. Many surveys (such as the General Social Survey) have multiple
versions with certain questions appearing in only a subset of the versions. As
a result, all the subjects may have missing data.
In such situations it is not immediately clear how to do
Qnparameter estimation. The posterior distribution for Œ∏ and Œ£ depends on i=1 p(y i |Œ∏, Œ£), but
p(y i |Œ∏, Œ£) cannot be computed if components of y i are missing. What can
we do? Unfortunately, many software packages either throw away all subjects
with incomplete data, or impute missing values with a population mean or
some other fixed value, then proceed with the analysis. The first approach is
bad because we are throwing away a potentially large amount of useful information. The second is statistically incorrect, as it says we are certain about
the values of the missing data when in fact we have not observed them.
Let‚Äôs carefully think about the information that is available from subjects
with missing data. Let O i = (O1 , . . . , Op )T be a binary vector of zeros and
ones such that Oi,j = 1 implies that Yi,j is observed and not missing, whereas
Oi,j = 0 implies Yi,j is missing. Our observed information about subject i
is therefore O i = oi and Yi,j = yi,j for variables j such that oi,j = 1. For
now, we‚Äôll assume that missing data are missing at random, meaning that O i
and Y i are statistically independent and that the distribution of O i does not
depend on Œ∏ or Œ£. In cases where the data are missing but not at random,
then sometimes inference can be made by modeling the relationship between
O i , Y i and the parameters (see Chapter 21 of Gelman et al (2004)).
In the case where data are missing at random, the sampling probability
for the data from subject i is
p(oi , {yi,j : oi,j = 1}|Œ∏, Œ£) = p(oi ) √ó p({yi,j : oi,j = 1}|Œ∏, Œ£)
Ô£±
Z Ô£≤
= p(oi ) √ó
p(yi,1 , . . . , yi,p |Œ∏, Œ£)
Ô£≥

Y
yi,j :oi,j =0

dyi,j

Ô£º
Ô£Ω
Ô£æ

.

7.5 Missing data and imputation

117

In words, our sampling probability for data from subject i is p(oi ) multiplied by the marginal probability of the observed variables, after integrating out the missing variables. To make this more concrete, suppose
y i = (yi,1 , NA, yi,3 , NA)T , so oi = (1, 0, 1, 0)T . Then
p(oi , yi,1 , yi,3 |Œ∏, Œ£) = p(oi ) √ó p(yi,1 , yi,3 |Œ∏, Œ£)
Z
= p(oi ) √ó p(y i |Œ∏, Œ£) dy2 dy4 .
So the correct thing to do when data are missing at random is to integrate over
the missing data to obtain the marginal probability of the observed data. In
this particular case of the multivariate normal model, this marginal probability is easily obtained: p(yi,1 , yi,3 |Œ∏, Œ£) is simply a bivariate normal density with
mean (Œ∏1 , Œ∏3 )T and a covariance matrix made up of (œÉ12 , œÉ1,3 , œÉ32 ). But combining marginal densities from subjects having different amounts of information
can be notationally awkward. Fortunately, our integration can alternatively
be done quite easily using Gibbs sampling.
Gibbs sampling with missing data
In Bayesian inference we use probability distributions to describe our information about unknown quantities. What are the unknown quantities for our
multivariate normal model with missing data? The parameters Œ∏ and Œ£ are
unknown as usual, but the missing data are also an unknown but key component of our model. Treating it as such allows us to use Gibbs sampling to
make inference on Œ∏, Œ£, as well as to make predictions for the missing values.
Let Y be the n √ó p matrix of all the potential data, observed and unobserved, and let O be the n √ó p matrix in which oi,j = 1 if Yi,j is observed and
oi,j = 0 if Yi,j is missing. The matrix Y can then be thought of as consisting
of two parts:
‚Ä¢ Yobs = {yi,j : oi,j = 1}, the data that we do observe, and
‚Ä¢ Ymiss = {yi,j : oi,j = 0}, the data that we do not observe.
From our observed data we want to obtain p(Œ∏, Œ£, Ymiss |Yobs ), the posterior distribution of unknown and unobserved quantities. A Gibbs sampling
scheme for approximating this posterior distribution can be constructed by
simply adding one step to the Gibbs sampler presented in the previous sec(0)
(s+1)
tion: Given starting values {Œ£ (0) , Ymiss }, we generate {Œ∏ (s+1) , Œ£ (s+1) , Ymiss }
(s)
from {Œ∏ (s) , Œ£ (s) , Ymiss } by
(s)

1. sampling Œ∏ (s+1) from p(Œ∏|Yobs , Ymiss , Œ£ (s) ) ;
(s)
2. sampling Œ£ (s+1) from p(Œ£|Yobs , Ymiss , Œ∏ (s+1) ) ;
(s+1)
3. sampling Ymiss from p(Ymiss |Yobs , Œ∏ (s+1) , Œ£ (s+1) ).
Note that in steps 1 and 2, the fixed value of Yobs combines with the current
(s)
value of Ymiss to form a current version of a complete data matrix Y(s) having

118

7 The multivariate normal model

no missing values. The n rows of the matrix of Y(s) can then be plugged into
formulae 7.6 and 7.9 to obtain the full conditional distributions of Œ∏ and Œ£.
Step 3 is a bit more complicated:
p(Ymiss |Yobs , Œ∏, Œ£) ‚àù p(Ymiss , Yobs |Œ∏, Œ£)
n
Y
=
p(y i,miss , y i,obs |Œ∏, Œ£)
‚àù

i=1
n
Y

p(y i,miss |y i,obs , Œ∏, Œ£),

i=1

so for each i we need to sample the missing elements of the data vector conditional on the observed elements. This is made possible via the following result
about multivariate normal distributions: Let y ‚àº multivariate normal(Œ∏, Œ£),
let a be a subset of variable indices {1, . . . , p} and let b be the complement of
a. For example, if p = 4 then perhaps a = {1, 2} and b = {3, 4}. If you know
about inverses of partitioned matrices you can show that
{y [b] |y [a] , Œ∏, Œ£} ‚àº multivariate normal(Œ∏ b|a , Œ£b|a ), where
Œ∏ b|a = Œ∏ [b] + Œ£[b,a] (Œ£[a,a] )‚àí1 (y [a] ‚àí Œ∏ [a] )
‚àí1

Œ£b|a = Œ£[b,b] ‚àí Œ£[b,a] (Œ£[a,a] )

Œ£[a,b] .

(7.10)
(7.11)

In the above formulae, Œ∏ [b] refers to the elements of Œ∏ corresponding to the
indices in b, and Œ£[a,b] refers to the matrix made up of the elements that are
in rows a and columns b of Œ£.
Let‚Äôs try to gain a little bit of intuition about what is going on in Equations
7.10 and 7.11. Suppose y is a sample from our population of four variables glu,
bp, skin and bmi. If we have glu and bp data for someone (a = {1, 2}) but are
missing skin and bmi measurements (b = {3, 4}), then we would be interested
in the conditional distribution of these missing measurements y [b] given the
observed information y [a] . Equation 7.10 says that the conditional mean of
skin and bmi start off at their unconditional mean Œ∏ [b] , but then are modified
by (y [a] ‚àíŒ∏ [a] ). For example, if a person had higher than average values of glu
and bp, then (y [a] ‚àí Œ∏ [a] ) would be a 2 √ó 1 vector of positive numbers. For our
data the 2√ó2 matrix Œ£[b,a] (Œ£[a,a] )‚àí1 has all positive entries, and so Œ∏ b|a > Œ∏ [b] .
This makes sense: If all four variables are positively correlated, then if we
observe higher than average values of glu and bp, we should also expect
higher than average values of skin and bmi. Also note that Œ£b|a is equal to
the unconditional variance Œ£[b,b] but with something subtracted off, suggesting
that the conditional variance is less than the unconditional variance. Again,
this makes sense: having information about some variables should decrease,
or at least not increase, our uncertainty about the others.
The R code below implements the Gibbs sampling scheme for missing data
described in steps 1, 2 and 3 above:

7.5 Missing data and imputation

119

data ( c h a p t e r 7 ) ; Y<‚àíY. pima . m i s s
### p r i o r p a r a m e t e r s
n<‚àídim (Y ) [ 1 ] ; p<‚àídim (Y ) [ 2 ]
mu0<‚àíc ( 1 2 0 , 6 4 , 2 6 , 2 6 )
sd0 <‚àí(mu0/ 2 )
L0<‚àíma t r i x ( . 1 , p , p ) ; d i a g ( L0)<‚àí1 ; L0<‚àíL0‚àó o u t e r ( sd0 , sd0 )
nu0<‚àíp+2 ; S0<‚àíL0
###
### s t a r t i n g v a l u e s
Sigma<‚àíS0
Y. f u l l <‚àíY
O<‚àí1‚àó(! i s . na (Y) )
for ( j in 1: p)
{
Y. f u l l [ i s . na (Y. f u l l [ , j ] ) , j ]<‚àímean (Y. f u l l [ , j ] , na . rm=TRUE)
}
###
### Gibbs s a m p l e r
THETA<‚àíSIGMA<‚àíY. MISS<‚àíNULL
set . seed (1)
for ( s in 1:1000)
{
###update t h e t a
ybar<‚àía p p ly (Y. f u l l , 2 , mean )
Ln<‚àís o l v e ( s o l v e ( L0 ) + n‚àó s o l v e ( Sigma ) )
mun<‚àíLn%‚àó%( s o l v e ( L0)%‚àó%mu0 + n‚àó s o l v e ( Sigma)%‚àó%ybar )
t h e t a <‚àírmvnorm ( 1 , mun , Ln )
###
###update Sigma
Sn<‚àí S0 + ( t (Y. f u l l )‚àíc ( t h e t a ) )%‚àó%t ( t (Y. f u l l )‚àíc ( t h e t a ) )
Sigma<‚àís o l v e ( r w i s h ( 1 , nu0+n , s o l v e ( Sn ) ) )
###
###update m i s s i n g data
for ( i in 1: n)
{
b <‚àí ( O[ i ,]==0 )
a <‚àí ( O[ i ,]==1 )
iSa <‚àí s o l v e ( Sigma [ a , a ] )
b e t a . j <‚àí Sigma [ b , a]%‚àó% i S a
Sigma . j
<‚àí Sigma [ b , b ] ‚àí Sigma [ b , a]%‚àó% i S a%‚àó%Sigma [ a , b ]
t h e t a . j <‚àí t h e t a [ b ] + b e t a . j %‚àó%(t (Y. f u l l [ i , a ]) ‚àí t h e t a [ a ] )
Y. f u l l [ i , b ] <‚àí rmvnorm ( 1 , t h e t a . j , Sigma . j )
}

120

7 The multivariate normal model

### s a v e r e s u l t s
THETA<‚àír b i n d (THETA, t h e t a ) ; SIGMA<‚àír b i n d (SIGMA, c ( Sigma ) )
Y. MISS<‚àír b i n d (Y. MISS , Y. f u l l [O==0] )
###
}
###

The prior mean of ¬µ0 = (120, 64, 26, 26)T was obtained from national averages,
and the prior variances were based primarily on keeping most of the prior
mass on values that are above zero. These prior distributions are likely much
more diffuse than more informed prior distributions that could be provided
by someone who is familiar with this population or these variables.
The Monte Carlo approximation of E[Œ∏|y 1 , . . . , y n ] is (123.46, 71.03, 29.35,
32.18), obtained by averaging the 1,000 Œ∏-values generated by the Gibbs sampler. Posterior confidence intervals and other quantities can additionally be
obtained in the usual way from the Gibbs samples. We can also average the
1,000 values of Œ£ to obtain E[Œ£|y 1 , . . . , y n ], the posterior expectation of Œ£.
However, when looking at associations among a set of variables, it is often the
correlations that are of interest and not the covariances. To each covariance
matrix Œ£ there corresponds a correlation matrix C, given by
q
n
o
C = cj,k : cj,k = Œ£[j,k] / Œ£[j,j] Œ£[k,k] .
We can convert our 1,000 posterior samples of Œ£ into 1,000 posterior samples
of C using the following R-code:
COR <‚àí a r r a y ( dim=c ( p , p , 1 0 0 0 ) )
for ( s in 1:1000)
{
Sig <‚àím at r i x ( SIGMA [ s , ] , nrow=p , n c o l=p )
COR[ , , s ] <‚àí S i g / s q r t ( o u t e r ( d i a g ( S i g ) , d i a g ( S i g ) ) )
}

This code generates a 4√ó4√ó1000 array, where each ‚Äúslice‚Äù is a 4√ó4 correlation
matrix generated from the posterior distribution. The posterior expectation
of C is
Ô£´
Ô£∂
1.00 0.23 0.25 0.19
Ô£¨ 0.23 1.00 0.25 0.24 Ô£∑
Ô£∑
E[C|y 1 , . . . , y n ] = Ô£¨
Ô£≠ 0.25 0.25 1.00 0.65 Ô£∏
0.19 0.24 0.65 1.00
and marginal posterior 95% quantile-based confidence intervals can be obtained with the command apply(COR, c(1,2), quantile,prob=c(.025,.975) ) .
These are displayed graphically in the left panel of Figure 7.4.
Prediction and regression
Multivariate models are often used to predict one or more variables given the
others. Consider, for example, a predictive model of glu based on measurements of bp, skin and bmi. Using a = {2, 3, 4} and b = {1} in Equation 7.10,

glu
0.3
0.5

glu
0.2 0.4 0.6

0.7

7.5 Missing data and imputation

‚óè

‚óè

‚óè

‚óè
‚óè

0.3

bp

0.5

bp
0.2 0.4 0.6

0.7

‚àí0.2

0.1

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

bmi
0.2 0.4 0.6

0.7

‚àí0.2

0.1

‚óè

skin
0.2 0.4 0.6

0.7

‚àí0.2

0.1

‚óè

bmi
0.3
0.5

‚óè

‚óè

‚óè

‚óè

‚óè

bmi

skin

glu

bmi

skin

bp

glu

‚àí0.2

0.1

‚óè

bp

skin
0.3
0.5

121

Fig. 7.4. Ninety-five percent posterior confidence intervals for correlations (left)
and regression coefficients (right).

the conditional mean of y [b] =glu, given numerical values of y [a] = {bp, skin,
bmi }, is given by
E[y [b] |Œ∏, Œ£, y [a] ] = Œ∏ [b] + Œ≤ Tb|a (y [a] ‚àí Œ∏ [a] )
where Œ≤ Tb|a = Œ£[b,a] (Œ£[a,a] )‚àí1 . Since this takes the form of a linear regression
model, we call the value of Œ≤ b|a the regression coefficient for y [b] given y [a]
based on Œ£. Values of Œ≤ b|a can be computed for each posterior sample of
Œ£, allowing us to obtain posterior expectations and confidence intervals for
these regression coefficients. Quantile-based 95% confidence intervals for each
of {Œ≤ 1|234 , Œ≤ 2|134 , Œ≤ 3|124 , Œ≤ 4|123 } are shown graphically in the second column
of Figure 7.4. The regression coefficients often tell a different story than the
correlations: The bottom row of plots, for example, shows that while there

122

7 The multivariate normal model

is strong evidence that the correlations between bmi and each of the other
variables are all positive, the plots on the right-hand side suggest that bmi is
nearly conditionally independent of glu and bp given skin.

‚óè

78

140

‚óè

135

‚óè
‚óè

76

‚óè
‚óè

predictied bp
70
72
74

predictied glu
120 125 130

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè

‚óè

‚óè
‚óè
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè‚óè

115

‚óè

68

‚óè
‚óè‚óè

‚óè

66

‚óè

110

‚óè

‚óè

‚óè
‚óè

80

‚óè

100

120
140
true glu

160

180

40

50

60

70
80
true bp

90

100

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

‚óè
‚óè ‚óè
‚óè
‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

20

28

‚óè

‚óè

predictied bmi
30
32
34

predictied skin
25
30
35

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè

36

40

‚óè

‚óè

‚óè

‚óè

10

20

30
true skin

‚óè

26

15

‚óè

40

25

‚óè

‚óè

30

35
true bmi

40

Fig. 7.5. True values of the missing data versus their posterior expectations.

Out-of-sample validation
Actually, the dataset we just analyzed was created by taking a complete data
matrix with no missing values and randomly replacing 10% of the entries with
NA‚Äôs. Since the original dataset is available, we can compare values predicted
by the model to the actual sample values. This comparison is made graphically
in Figure 7.5, which plots the true value of yi,j against its posterior mean for
each {i, j} such that oi,j = 0. It looks like we are able to do a better job

7.6 Discussion and further references

123

predicting missing values of skin and bmi than the other two variables. This
makes sense, as these two variables have the highest correlation. If skin is
missing, we can make a good prediction for it based on the observed value of
bmi, and vice-versa. Such a procedure, where we evaluate how well a model
does at predicting data that were not used to estimate the parameters, is
called out-of-sample validation, and is often used to quantify the predictive
performance of a model.

7.6 Discussion and further references
The multivariate normal model can be justified as a sampling model for reasons analogous to those for the univariate normal model (see Section 5.7): It is
characterized by independence between the sample mean and sample variance
(Rao, 1958), it is a maximum entropy distribution and it provides consistent
estimation of the population mean and variance, even if the population is not
multivariate normal.
The multivariate normal and Wishart distributions form the foundation
of multivariate data analysis. A classic text on the subject is Mardia et al
(1979), and one with more coverage of Bayesian approaches is Press (1982).
An area of much current Bayesian research involving the multivariate normal distribution is the study of graphical models (Lauritzen, 1996; Jordan,
1998). A graphical model allows elements of the precision matrix to be exactly equal to zero, implying some variables are conditionally independent of
each other. A generalization of the Wishart distribution, known as the hyperinverse-Wishart distribution, has been developed for such models (Dawid and
Lauritzen, 1993; Letac and Massam, 2007).

8
Group comparisons and hierarchical modeling

In this chapter we discuss models for the comparison of means across groups.
In the two-group case, we parameterize the two population means by their
average and their difference. This type of parameterization is extended to the
multigroup case, where the average group mean and the differences across
group means are described by a normal sampling model. This model, together with a normal sampling model for variability among units within a
group, make up a hierarchical normal model that describes both within-group
and between-group variability. We also discuss an extension to this normal
hierarchical model which allows for across-group heterogeneity in variances in
addition to heterogeneity in means.

8.1 Comparing two groups
The first panel of Figure 8.1 shows math scores from a sample of 10th grade
students from two public U.S. high schools. Thirty-one students from school
1 and 28 students from school 2 were randomly selected to participate in a
math test. Both schools have a total enrollment of around 600 10th graders
each, and both are in urban neighborhoods.
Suppose we are interested in estimating Œ∏1 , the average score we would
obtain if all 10th graders in school 1 were tested, and possibly comparing it
to Œ∏2 , the corresponding average from school 2. The results from the sample
data are y¬Ø1 = 50.81 and y¬Ø2 = 46.15, suggesting that Œ∏1 is larger than Œ∏2 .
However, if different students had been sampled from each of the two schools,
then perhaps y¬Ø2 would have been larger than y¬Ø1 . To assess whether or not the
observed mean difference of y¬Ø1 ‚àí y¬Ø2 = 4.66 is large compared to the sampling
variability it is standard practice to compute the t-statistic, which is the ratio
of the observed difference to an estimate of its standard deviation:

P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 8,
c Springer Science+Business Media, LLC 2009


126

8 Group comparisons and hierarchical modeling

t(y 1 , y 2 ) =

y¬Ø1 ‚àí y¬Ø2
sp

p

1/n1 + 1/n2
50.81 ‚àí 46.15
p
=
= 1.74,
10.44 1/31 + 1/28

0.0

30

40

0.1

score
50

density
0.2

60

0.3

70

0.4

where s2p = [(n1 ‚àí 1)s21 + (n2 ‚àí 1)s22 ]/(n1 + n2 ‚àí 2), the pooled estimate of
the population variance of the two groups. Is this value of 1.74 large? From
introductory statistics, we know that if the population of scores from the two
schools are both normally distributed with the same mean and variance, then
the sampling distribution of the t-statistic t(Y 1 , Y 2 ) is a t-distribution with
n1 + n2 ‚àí 2 = 57 degrees of freedom. The density of this distribution is plotted in the second panel of Figure 8.1, along with the observed value of the
t-statistic. If the two populations indeed follow the same normal population,
then the pre-experimental probability of sampling a dataset that would generate a value of t(Y 1 , Y 2 ) greater in absolute value than 1.74 is p = 0.087. You
may recall that this latter number is called the (two-sided) p-value. While
a small p-value is generally considered as indicating evidence that Œ∏1 and Œ∏2
are different, the p-value should not be confused with the probability that
Œ∏1 = Œ∏2 . Although not completely justified by statistical theory for this purpose, p-values are often used in parameter estimation and model selection.
For example, the following is a commonly taught data analysis procedure for
comparing the population means of two groups:

school 1

school 2

‚àí4

‚àí2

0
t

2

4

Fig. 8.1. Boxplots of samples of 10th grade math scores from two schools, and the
null distribution for testing equality of the population means. The gray line indicates
the observed value of the t-statistic.

8.1 Comparing two groups

127

Model selection based on p-values:
If
‚Äì
‚Äì
‚Äì
If
‚Äì
‚Äì
‚Äì

p < 0.05,
reject the model that the two groups have the same distribution;
conclude that Œ∏1 6= Œ∏2 ;
use the estimates Œ∏ÀÜ1 = y¬Ø1 , Œ∏ÀÜ2 = y¬Ø2 .
p > 0.05
accept the model that the two groups have the same distribution;
conclude that Œ∏1 = Œ∏2 ;
P
P
use the estimates Œ∏ÀÜ1 = Œ∏ÀÜ2 = ( yi,1 + yi,2 )/(n1 + n2 ).

This data analysis procedure results in either treating the two populations
as completely distinct or treating them as exactly identical. Do these rather
extreme alternatives make sense? For our math score data, the above procedure would take the p-value of 0.087 and tell us to treat the population means
of the two groups as being numerically equivalent, although there seems to
be some evidence of a difference. Conversely, it is not too hard to imagine
a scenario where the sample from school 1 might have included a few more
high-performing students, the sample from school 2 a few more low-performing
students, in which case we could have observed a p-value of 0.04 or 0.05. In
this latter case we would have treated each population separately, using only
data from school 1 to estimate Œ∏1 and similarly for school 2. This latter approach seems somewhat inefficient: Since the two samples are both measuring
the same thing on similar populations of students, it might make sense to use
some of the information from one group to help estimate the mean in the
other.
The p-value-based procedure described above can be re-expressed as estimating Œ∏1 as Œ∏ÀÜ1 = w¬Ø
y1 + (1 ‚àí w)¬Ø
y2 , where w = 1 if p < 0.05 and
w = n1 /(n1 + n2 ) otherwise. Instead of using such an extreme procedure,
it might make more sense to allow w to vary continuously and have a value
that depends on such things as the relative sample sizes n1 and n2 , the sampling variability œÉ 2 and our prior information about the similarities of the two
populations. An estimator similar to this is produced by a Bayesian analysis that allows for information to be shared across the groups. Consider the
following sampling model for data from the two groups:
Yi,1 = ¬µ + Œ¥ + i,1
Yi,2 = ¬µ ‚àí Œ¥ + i,2
{i,j } ‚àº i.i.d. normal(0, œÉ 2 ) .
Using this parameterization where Œ∏1 = ¬µ + Œ¥ and Œ∏2 = ¬µ ‚àí Œ¥, we see that Œ¥
represents half the population difference in means, as (Œ∏1 ‚àí Œ∏2 )/2 = Œ¥, and ¬µ
represents the pooled average, as (Œ∏1 + Œ∏2 )/2 = ¬µ. Convenient conjugate prior
distributions for the unknown parameters are

128

8 Group comparisons and hierarchical modeling

p(¬µ, Œ¥, œÉ 2 ) = p(¬µ) √ó p(Œ¥) √ó p(œÉ 2 )
¬µ ‚àº normal(¬µ0 , Œ≥02 )
Œ¥ ‚àº normal(Œ¥0 , œÑ02 )
œÉ 2 ‚àº inverse-gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2).
It is left as an exercise to show that the full conditional distributions of these
parameters are as follows:
2
{¬µ|y 1 , y 2 , Œ¥, œÉ 2 } ‚àº normal(¬µ
Pn1 n , Œ≥n ), where2 Pn2
2
2
¬µn = Œ≥n √ó [¬µ0 /Œ≥0 + i=1 (yi,1 ‚àí Œ¥)/œÉ + i=1 (yi,2 + Œ¥)/œÉ 2 ]
Œ≥n2 = [1/Œ≥02 + (n1 + n2 )/œÉ 2 ]‚àí1
2
{Œ¥|y 1 , y 2 , ¬µ, œÉ 2 } ‚àº normal(Œ¥
P n , œÑn ), where
P
2
2
Œ¥n = œÑn √ó [Œ¥0 /œÑ0 + (yi,1 ‚àí ¬µ)/œÉ 2 ‚àí (yi,2 ‚àí ¬µ)/œÉ 2 ]
œÑn2 = [1/œÑ02 + (n1 + n2 )/œÉ 2 ]‚àí1

{œÉ 2 |y 1 , y 2 , ¬µ, Œ¥} ‚àº inverse-gamma(ŒΩn /2, ŒΩn œÉn2 /2), where
ŒΩn = ŒΩ0 + n1 + P
n2
P
ŒΩn œÉn2 = ŒΩ0 œÉ02 + (yi,1 ‚àí [¬µ + Œ¥])2 + (yi,2 ‚àí [¬µ ‚àí Œ¥])2
Although these formulae seem quite involved, you should try to convince yourself that they make sense. One way to do this is to plug in extreme values for
the prior parameters. For example, if ŒΩ0 = 0 then
P
P
(yi,1 ‚àí [¬µ + Œ¥])2 + (yi,2 ‚àí [¬µ ‚àí Œ¥])2
œÉn2 =
n1 + n2
which is a pooled-sample estimate of the variance if the values of ¬µ and Œ¥ were
known. Similarly, if ¬µ0 = Œ¥0 = 0 and Œ≥02 = œÑ02 = ‚àû, then (defining 0/‚àû = 0)
P
P
P
P
(yi,1 ‚àí Œ¥) + (yi,2 + Œ¥)
(yi,1 ‚àí ¬µ) ‚àí (yi,2 ‚àí ¬µ)
, Œ¥n =
¬µn =
n1 + n2
n1 + n2
and if you plug in ¬µn for ¬µ and Œ¥n for Œ¥, you get y¬Ø1 = ¬µn +Œ¥n and y¬Ø2 = ¬µn ‚àíŒ¥n .
Analysis of the math score data
The math scores were based on results of a national exam in the United States,
standardized to produce a nationwide mean of 50 and a standard deviation
of 10. Unless these two schools were known in advance to be extremely exceptional, reasonable prior parameters can be based on this information. For
the prior distributions of ¬µ and œÉ 2 , we‚Äôll take ¬µ0 = 50 and œÉ02 = 102 = 100,
although this latter value is likely to be an overestimate of the within-school
sampling variability. We‚Äôll make these prior distributions somewhat diffuse,
with Œ≥02 = 252 = 625 and ŒΩ0 = 1. For the prior distribution on Œ¥, choosing
Œ¥0 = 0 represents the prior opinion that Œ∏1 > Œ∏2 and Œ∏2 > Œ∏1 are equally
probable. Finally, since the scores are bounded between 0 an 100, half the

8.1 Comparing two groups

129

difference between Œ∏1 and Œ∏2 must be less than 50 in absolute value, so a value
of œÑ02 = 252 = 625 seems reasonably diffuse.
Using the full-conditional distributions given above, we can construct a
Gibbs sampler to approximate the posterior distribution p(¬µ, Œ¥, œÉ 2 |y 1 , y 2 ). Rcode to do the approximation appears below:
data ( c h a p t e r 8 )
y1<‚àíy . s c h o o l 1 ; n1<‚àíl e n g t h ( y1 )
y2<‚àíy . s c h o o l 2 ; n2<‚àíl e n g t h ( y2 )
##### p r i o r p a r a m e t e r s
mu0<‚àí50 ; g02 <‚àí625
d e l 0 <‚àí0 ; t02 <‚àí625
s20 <‚àí100; nu0<‚àí1
#####
##### s t a r t i n g v a l u e s
mu<‚àí ( mean ( y1 ) + mean ( y2 ) ) / 2
d e l <‚àí ( mean ( y1 ) ‚àí mean ( y2 ) ) / 2
#####
##### Gibbs s a m p l e r
MU<‚àíDEL<‚àíS2<‚àíNULL
set . seed (1)
for ( s in 1:5000)
{
##update s 2
s2 <‚àí1/rgamma ( 1 , ( nu0+n1+n2 ) / 2 ,
( nu0‚àó s 2 0+sum ( ( y1‚àímu‚àíd e l )ÀÜ2)+sum ( ( y2‚àímu+d e l ) ÀÜ 2 ) ) / 2 )
##
##update mu
var . mu<‚àí 1 / ( 1 / g02+ ( n1+n2 ) / s 2 )
mean . mu<‚àívar . mu‚àó (mu0/ g02+sum ( y1‚àíd e l ) / s 2+sum ( y2+d e l ) / s 2 )
mu<‚àírnorm ( 1 , mean . mu, s q r t ( var . mu) )
##
##update d e l
var . d e l <‚àí 1 / ( 1 / t 0 2+ ( n1+n2 ) / s 2 )
mean . d e l <‚àívar . d e l ‚àó ( d e l 0 / t 0 2+sum ( y1‚àímu) / s2‚àísum ( y2‚àímu) / s 2 )
d e l <‚àírnorm ( 1 , mean . d e l , s q r t ( var . d e l ) )
##
##s a v e parameter v a l u e s
MU<‚àíc (MU, mu) ; DEL<‚àíc (DEL, d e l ) ; S2<‚àíc ( S2 , s 2 )
}
#####

130

8 Group comparisons and hierarchical modeling

density
0.00 0.05 0.10 0.15 0.20 0.25

density
0.00 0.05 0.10 0.15 0.20 0.25

Figure 8.2 shows the marginal posterior distributions of ¬µ and Œ¥, and how they
are much more concentrated than their corresponding prior distributions. In
particular, a 95% quantile-based posterior confidence interval for 2 √ó Œ¥, the
difference in average scores between the two schools, is (‚àí.61, 9.98). Although
this interval contains zero, the differences between the prior and posterior
distributions indicate that we have gathered substantial evidence that the
population mean for school 1 is higher than that of school 2. Additionally, the
posterior probability Pr(Œ∏1 > Œ∏2 |y 1 , y 2 ) = Pr(Œ¥ > 0|y 1 , y 2 ) ‚âà 0.96, whereas
the corresponding prior probability was Pr(Œ¥ > 0) = 0.50. However, we should
be careful not to confuse this probability with the probability that a randomly
selected student from school 1 has a higher score than one sampled from school
2. This latter probability can be obtained from the joint posterior predictive
distribution, which gives Pr(Y1 > Y2 |y 1 , y 2 ) ‚âà 0.62.

prior
posterior

30

40

50
¬µ

60

70

prior
posterior

‚àí20

‚àí10

0
Œ¥

10

20

Fig. 8.2. Prior and posterior distributions for ¬µ and Œ¥.

8.2 Comparing multiple groups
The data in the previous section was part of the 2002 Educational Longitudinal Study (ELS), a survey of students from a large sample of schools across
the United States. This dataset includes a population of schools as well as a
population of students within each school. Datasets like this, where there is
a hierarchy of nested populations, are often called hierarchical or multilevel .
Other situations having the same sort of data structure include data on
‚Ä¢ patients within several hospitals,
‚Ä¢ genes within a group of animals, or

8.2 Comparing multiple groups

131

‚Ä¢ people within counties within regions within countries.
The simplest type of multilevel data has two levels, in which one level consists
of groups and the other consists of units within groups. In this case we denote
yi,j as the data on the ith unit in group j.
8.2.1 Exchangeability and hierarchical models
Recall from Chapter 2 that a sequence of random variables Y1 , . . . , Yn is exchangeable if the probability density representing our information about the
sequence satisfies p(y1 , . . . , yn ) = p(yœÄ1 , . . . , yœÄn ) for any permutation œÄ. Exchangeability is a reasonable property of p(y1 , . . . , yn ) if we lack information
distinguishing the random variables. For example, if Y1 , . . . , Yn were math
scores from n randomly selected students from a particular school, then in
the absence of other information about the students we might treat their
math scores as exchangeable. If exchangeability holds for all values of n, then
de Finetti‚Äôs theorem says that an equivalent formulation of our information
is that
œÜ ‚àº p(œÜ)
{Y1 , . . . , Yn |œÜ} ‚àº i.i.d. p(y|œÜ).
In other words, the random variables can be thought of as independent samples
from a population described by some fixed but unknown population feature
œÜ. In the normal model, for example, we take œÜ = {Œ∏, œÉ 2 } and model the data
as conditionally i.i.d. normal(Œ∏, œÉ 2 ).
Now let‚Äôs consider a model describing our information about a hierarchical
data {Y 1 , . . . , Y m }, where Y j = {Y1,j , . . . , Ynj ,j }. What properties should a
model p(y 1 , . . . , y m ) have? Let‚Äôs consider first p(y j ) = p(y1,j , . . . , ynj ,j ), the
marginal probability density of data from a single group j. The discussion
in the preceding paragraph suggests that we should not treat Y1,j , . . . , Ynj ,j
as being independent, as doing so would imply, for example, that p(ynj ,j |
y1,j ,. . .,ynj ‚àí1,j ) = p(ynj ,j ), and that the values of Y1,j , . . . , Ynj ‚àí1,j would
give us no information about Ynj ,j . However, if all that is known about
Y1,j , . . . , Ynj ,j is that they are random samples from group j, then treating
Y1,j , . . . , Ynj ,j as exchangeable makes sense. If group j is large compared to the
sample size nj , then de Finetti‚Äôs theorem and results of Diaconis and Freedman (1980) say that we can model the data within group j as conditionally
i.i.d. given some group-specific parameter œÜj :
{Y1,j , . . . , Ynj ,j |œÜj } ‚àº i.i.d. p(y|œÜj ) .
But how should we represent our information about œÜ1 , . . . , œÜm ? As before,
we might not want to treat these parameters as independent, because doing
so would imply that knowing the values of œÜ1 , . . . , œÜm‚àí1 does not change
our information about the value of œÜm . However, if the groups themselves

132

8 Group comparisons and hierarchical modeling

are samples from some larger population of groups, then exchangeability of
the group-specific parameters might be appropriate. Applying de Finetti‚Äôs
theorem a second time gives
{œÜ1 , . . . , œÜm |œà} ‚àº i.i.d. p(œÜ|œà)
for some sampling model p(œÜ|œà) and an unknown parameter œà. This double
application of de Finetti‚Äôs theorem has led us to three probability distributions:
{y1,j , . . . , ynj ,j |œÜj } ‚àº i.i.d. p(y|œÜj )
{œÜ1 , . . . , œÜm |œà} ‚àº i.i.d. p(œÜ|œà)
œà ‚àº p(œà)

(within-group sampling variability)
(between-group sampling variability)
(prior distribution)

It is important to recognize that the distributions p(y|œÜ) and p(œÜ|œà) both
represent sampling variability among populations of objects: p(y|œÜ) represents
variability among measurements within a group and p(œÜ|œà) represents variability across groups. In contrast, p(œà) represents information about a single
fixed but unknown quantity. For this reason, we refer to p(y|œÜ) and p(œÜ|œà)
as sampling distributions, and are conceptually distinct from p(œà), which is a
prior distribution. In particular, the data will be used to estimate the withinand between-group sampling distributions p(y|œÜ) and p(œÜ|œà), whereas the
prior distribution p(œà) is not estimated from the data.

8.3 The hierarchical normal model
A popular model for describing the heterogeneity of means across several populations is the hierarchical normal model, in which the within- and betweengroup sampling models are both normal:
œÜj = {Œ∏j , œÉ 2 }, p(y|œÜj ) = normal(Œ∏j , œÉ 2 )
œà = {¬µ, œÑ 2 }, p(Œ∏j |œà) = normal(¬µ, œÑ 2 )

(within-group model) (8.1)
(between-group model) (8.2)

It might help to visualize this setup as in Figure 8.3. Note that p(œÜ|œà) only
describes the heterogeneity across group means, and not any heterogeneity in
group-specific variances. In fact, the within-group sampling variability œÉ 2 is
assumed to be constant across groups. At the end of this chapter we will
eliminate this assumption by adding a component to the model that allows
for group-specific variances.
The fixed but unknown parameters in this model are ¬µ, œÑ 2 and œÉ 2 . For
convenience we will use standard semiconjugate normal and inverse-gamma
prior distributions for these parameters:
1/œÉ 2 ‚àº gamma (ŒΩ0 /2, ŒΩ0 œÉ02 /2)
1/œÑ 2 ‚àº gamma (Œ∑0 /2, Œ∑0 œÑ02 /2)
¬µ ‚àº normal (¬µ0 , Œ≥02 )

8.3 The hierarchical normal model
¬µ, œÑ

133

2

Œ∏1

Œ∏2

¬∑¬∑¬∑

Œ∏m‚àí1

Y1

Y2

¬∑¬∑¬∑

Y m‚àí1 Y m

Œ∏m

œÉ2
Fig. 8.3. A graphical representation of the basic hierarchical normal model.

8.3.1 Posterior inference
The unknown quantities in our system include the group-specific means
{Œ∏1 , . . . , Œ∏m }, the within-group sampling variability œÉ 2 and the mean and variance (¬µ, œÑ 2 ) of the population of group-specific means. Joint posterior inference for these parameters can be made by constructing a Gibbs sampler which
approximates the posterior distribution p(Œ∏1 , . . . , Œ∏m , ¬µ, œÑ 2 , œÉ 2 |y 1 , . . . , y m ).
The Gibbs sampler proceeds by iteratively sampling each parameter from
its full conditional distribution. Deriving the full conditional distributions in
this highly parameterized system may seem like a daunting task, but it turns
out that all of the necessary technical details have been covered in Chapters
5 and 6. All that is required of us at this point is that we recognize certain
analogies between the current model and the univariate normal model. Useful
for this will be the following factorization:
p(Œ∏1 , . . . , Œ∏m , ¬µ, œÑ 2 , œÉ 2 |y 1 , . . . , y m )
‚àù p(¬µ, œÑ 2 , œÉ 2 ) √ó p(Œ∏1 , . . . , Œ∏m |¬µ, œÑ 2 , œÉ 2 ) √ó p(y 1 , . . . , y m |Œ∏1 , . . . , Œ∏m , ¬µ, œÑ 2 , œÉ 2 )
Ô£±
Ô£ºÔ£±
Ô£º
nj
m
m Y
Ô£≤Y
Ô£Ω Ô£≤Y
Ô£Ω
= p(¬µ)p(œÑ 2 )p(œÉ 2 )
p(Œ∏j |¬µ, œÑ 2 )
p(yi,j |Œ∏j , œÉ 2 ) .
(8.3)
Ô£≥
Ô£æÔ£≥
Ô£æ
j=1

j=1 i=1

The term in the second pair of brackets is the result of an important conditional independence feature of our model. Conditionally on {Œ∏1 , . . ., Œ∏m , ¬µ, œÑ 2 ,
œÉ 2 }, the random variables Y1,j , . . . , Ynj ,j are independent with a distribution
that depends only on Œ∏j and œÉ 2 and not on ¬µ or œÑ 2 . It is helpful to think about
this fact in terms of the diagram in Figure 8.3: The existence of a path from
(¬µ, œÑ 2 ) to each Y j indicates that while (¬µ, œÑ 2 ) provides information about Y j ,
it only does so indirectly through Œ∏j , which separates the two quantities in
the graph.
Full conditional distributions of ¬µ and œÑ 2
As a function of ¬µ and œÑ 2 , the term in Equation 8.3 is proportional to
p(¬µ)p(œÑ 2 )

m
Y
j=1

p(Œ∏j |¬µ, œÑ ),

134

8 Group comparisons and hierarchical modeling

and so the full conditional distributions of ¬µ and œÑ 2 are also proportional to
this quantity. In particular, this must mean that
Y
p(¬µ|Œ∏1 , . . . , Œ∏m , œÑ 2 , œÉ 2 , y 1 , . . . , y m ) ‚àù p(¬µ)
p(Œ∏j |¬µ, œÑ 2 )
Y
p(œÑ 2 |Œ∏1 , . . . , Œ∏m , ¬µ, œÉ 2 , y 1 , . . . , y m ) ‚àù p(œÑ 2 )
p(Œ∏j |¬µ, œÑ 2 ).
These distributions are exactly the full conditional distributions from the onesample normal problem in Chapter 6. In Chapter 6 we derived the full conditionals of the population mean and variance of a normal population, assuming
independent normal and inverse-gamma prior distributions. In our current situation, Œ∏1 , . . . , Œ∏m are the i.i.d. samples from a normal population, and ¬µ and
œÑ 2 are the unknown population mean and variance. In Chapter 6, we saw that
if y1 , . . . , yn were i.i.d. normal(Œ∏, œÉ 2 ) and Œ∏ had a normal prior distribution,
then the conditional distribution of Œ∏ was also normal. Since our current situation is exactly analogous, the fact that Œ∏1 , . . . , Œ∏m are i.i.d. normal(¬µ, œÑ 2 ) and
¬µ has a normal prior distribution implies that the conditional distribution of
¬µ must be normal as well. Similarly, just as œÉ 2 had an inverse-gamma conditional distribution in Chapter 6, œÑ 2 has an inverse-gamma distribution in
the current situation. Applying the results of Chapter 6 with the appropriate
symbolic replacements, we have
 ¬Ø 2

mŒ∏/œÑ + ¬µ0 /Œ≥02
2
2
2 ‚àí1
{¬µ|Œ∏1 , . . . , Œ∏m , œÑ } ‚àº normal
, [m/œÑ + 1/Œ≥0 ]
m/œÑ 2 + 1/Œ≥02
P


Œ∑0 + m Œ∑0 œÑ02 + (Œ∏j ‚àí ¬µ)2
{1/œÑ 2 |Œ∏1 , . . . , Œ∏m , ¬µ} ‚àº gamma
,
.
2
2
Full conditional of Œ∏j
Collecting the terms in Equation 8.3 that depend on Œ∏j shows that the full
conditional distribution of Œ∏j must be proportional to
p(Œ∏j |¬µ, œÑ 2 , œÉ 2 , y 1 , . . . , y m ) ‚àù p(Œ∏j |¬µ, œÑ 2 )

nj
Y

p(yi,j |Œ∏j , œÉ 2 ).

i=1

This says that, conditional on {¬µ, œÑ 2 , œÉ 2 , y j }, Œ∏j must be conditionally independent of the other Œ∏‚Äôs as well as independent of the data from groups other
than j. Again, it is helpful to refer to Figure 8.3: While there is a path from
each Œ∏j to every other Œ∏k , the paths go through (¬µ, œÑ 2 ) or œÉ 2 . We can think
of this as meaning that the Œ∏‚Äôs contribute no information about each other
beyond that contained in ¬µ,œÑ 2 and œÉ 2 .
The terms in the above equation include a normal density for Œ∏j multiplied
by a product of normal densities where Œ∏j is the mean. Mathematically, this is
exactly the same setup as the one-sample normal model, in which p(Œ∏j |¬µ, œÑ 2 )
is the prior distribution instead of the sampling model for the Œ∏‚Äôs. The full
conditional distribution is therefore

8.4 Example: Math scores in U.S. public schools

{Œ∏j |y1,j , . . . , ynj ,j , œÉ 2 } ‚àº normal(

135

nj y¬Øj /œÉ 2 + 1/œÑ 2
, [nj /œÉ 2 + 1/œÑ 2 ]‚àí1 ).
nj /œÉ 2 + 1/œÑ 2

Full conditional of œÉ 2
Using Figure 8.3 and the arguments in the previous two paragraphs, you
should convince yourself that œÉ 2 is conditionally independent of {¬µ, œÑ 2 } given
{y 1 , . . . , y m , Œ∏1 , . . . , Œ∏m }. The derivation of the full conditional of œÉ 2 is similar
to that in the one-sample normal model, except now we have information
about œÉ 2 from m separate groups:
p(œÉ 2 |Œ∏1 , . . . , Œ∏m , y 1 , . . . , y m ) ‚àù p(œÉ 2 )

nj
m Y
Y

p(yi,j |Œ∏j , œÉ 2 )

j=1 i=1

‚àù (œÉ 2 )‚àíŒΩ0 /2+1 e‚àí

2
ŒΩ0 œÉ 0
2œÉ 2

PP

(œÉ 2 )‚àí

P

nj /2 ‚àí

e

(yi,j ‚àíŒ∏j )2
2œÉ 2

.

Adding the powers of œÉ 2 and collecting the terms in the exponent, we recognize
this as proportional to an inverse-gamma density, giving
nj
m
m X
X
X
1
1
{1/œÉ 2 |Œ∏, y 1 , . . . , y n } ‚àº gamma( [ŒΩ0 +
nj ], [ŒΩ0 œÉ02 +
(yi,j ‚àí Œ∏j )2 ]).
2
2
j=1
j=1 i=1

PP
Note that
(yi,j ‚àí Œ∏j )2 is the sum of squared residuals across all groups,
conditional on the within-group means, and so the conditional distribution
concentrates probability around a pooled-sample estimate of the variance.

8.4 Example: Math scores in U.S. public schools
Let‚Äôs return to the 2002 ELS data described previously. This survey included
10th grade children from 100 different large urban public high schools, all
having a 10th grade enrollment of 400 or greater. Data from these schools are
shown in Figure 8.4, with scores from students within the same school plotted
along a common vertical bar.
A histogram of the sample averages is shown in the first panel of Figure 8.5.
The range of average scores is quite large, with the lowest average being 36.6
and the highest 65.0. The second panel of the figure shows the relationship
between the sample average and the sample size. This plot seems to indicate
that very extreme sample averages tend to be associated with schools with
small sample sizes. For example, the school with the highest sample average
has the lowest sample size, and many schools with low sample averages also
have low sample sizes. This relationship between sample averages and sample
size is fairly common in hierarchical datasets. To understand this phenomenon,
consider a situation in which all Œ∏j ‚Äôs were equal to some common value, say
Œ∏0 , but the sample sizes were different. The expected value of each sample

136

8 Group comparisons and hierarchical modeling

‚óè

80

‚óè
‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè ‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè
‚óè ‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè ‚óè‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè ‚óè ‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè
‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè ‚óè‚óè ‚óè‚óè ‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè ‚óè ‚óè ‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

20

30

40

math score
50 60

70

‚óè

0

20

40
60
80
rank of school‚àíspecific math score average

100

‚óè

‚óè

40

65

Fig. 8.4. A graphical representation of the ELS data.

60

‚óè
‚óè
‚óè

‚óè

‚óè

0

40

10

sample mean
45
50
55

Frequency
20
30

‚óè

35

40

45

50 55 60
sample mean

65

70

5

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè
‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

10

15
20
25
sample size

‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè

30

Fig. 8.5. Empirical distribution of sample means, and the relationship between
sample mean and sample size.

average would then be E[Y¬Øj |Œ∏j , œÉ 2 ] = Œ∏j = Œ∏0 , but the variances would depend
on the sample size, since Var[Y¬Øj |œÉj2 ] = œÉ 2 /nj . As a result, sample averages for
groups with large sample sizes would be very close to Œ∏0 , whereas the sample
averages for groups with small sample sizes would be farther away, both less
than and greater than Œ∏0 . For this reason, it is not uncommon that groups
with very high or very low sample averages are also those groups with low
sample sizes.

8.4 Example: Math scores in U.S. public schools

137

8.4.1 Prior distributions and posterior approximation
The prior parameters we need to specify are
(ŒΩ0 , œÉ02 ) for p(œÉ 2 ),
(Œ∑0 , œÑ02 ) for p(œÑ02 ) and
(¬µ0 , Œ≥02 ) for p(¬µ).
As described above, the math exam was designed to give a nationwide variance
of 100. Since this variance includes both within-school and between-school
variance, the within-school variance should be at most 100, which we take as
œÉ02 . This is likely to be an overestimate, and so we only weakly concentrate the
prior distribution around this value by taking ŒΩ0 = 1. Similarly, the betweenschool variance should not be more than 100, and so we take œÑ02 = 100 and
Œ∑0 = 1. Finally, the nationwide mean over all schools is 50. Although the mean
for large urban public schools may be different than the nationwide average,
it should not differ by too much. We take ¬µ0 = 50 and Œ≥02 = 25, so that the
prior probability that ¬µ is in the interval (40, 60) is about 95%.
Posterior approximation proceeds by iterative sampling of each unknown
quantity from its full conditional distribution. Given a current state of the
(s)
(s)
unknowns {Œ∏1 , . . . , Œ∏m , ¬µ(s) , œÑ 2(s) , œÉ 2(s) }, a new state is generated as follows:
1.
2.
3.
4.

(s)

(s)

sample ¬µ(s+1) ‚àº p(¬µ|Œ∏1 , . . . , Œ∏m , œÑ 2(s) );
(s)
(s)
sample œÑ 2(s+1) ‚àº p(œÑ 2 |Œ∏1 , . . . , Œ∏m , ¬µ(s+1) );
(s)
(s)
sample œÉ 2(s+1) ‚àº p(œÉ 2 |Œ∏1 , . . . , Œ∏m , y 1 , . . . , y m );
(s+1)
for each j ‚àà {1, . . . , m}, sample Œ∏j
‚àº p(Œ∏j |¬µ(s+1) , œÑ 2(s+1) , œÉ 2(s+1) , y j ).

The order in which the new parameters are generated does not matter. What
does matter is that each parameter is updated conditional upon the most
current value of the other parameters. This Gibbs sampling procedure can be
implemented in R with the code below:
data ( c h a p t e r 8 ) ; Y<‚àíY. s c h o o l . mathscore
### weakly i n f o r m a t i v e p r i o r s
nu0<‚àí1 ; s20 <‚àí100
eta0 <‚àí1 ; t20 <‚àí100
mu0<‚àí50 ; g20<‚àí25
###
### s t a r t i n g v a l u e s
m<‚àíl e n g t h ( u n i q u e (Y [ , 1 ] ) )
n<‚àísv<‚àíybar<‚àír e p (NA,m)
f o r ( j i n 1 :m)
{
ybar [ j ]<‚àímean (Y[Y[ ,1]== j , 2 ] )
sv [ j ]<‚àívar (Y[Y[ ,1]== j , 2 ] )
n [ j ]<‚àísum (Y[ ,1]== j )

138

8 Group comparisons and hierarchical modeling

}
t h e t a <‚àíybar ; sigma2<‚àímean ( sv )
mu<‚àímean ( t h e t a ) ; tau2<‚àívar ( t h e t a )
###
### s e t u p MCMC
set . seed (1)
S<‚àí5000
THETA<‚àíma t r i x ( nrow=S , n c o l=m)
SMT<‚àím at r i x ( nrow=S , n c o l =3)
###
### MCMC a l g o r i t h m
for ( s in 1: S)
{
# sample new v a l u e s o f t h e t h e t a s
f o r ( j i n 1 :m)
{
v t h e t a <‚àí1/(n [ j ] / sigma2+1/tau2 )
e t h e t a <‚àív t h e t a ‚àó ( ybar [ j ] ‚àó n [ j ] / sigma2+mu/ tau2 )
t h e t a [ j ]<‚àírnorm ( 1 , e t h e t a , s q r t ( v t h e t a ) )
}
#sample new v a l u e o f sigma2
nun<‚àínu0+sum ( n )
s s <‚àínu0‚àó s 2 0
f o r ( j i n 1 :m) { s s <‚àís s+sum ( (Y[Y[ ,1]== j ,2] ‚àí t h e t a [ j ] ) ÀÜ 2 ) }
sigma2 <‚àí1/rgamma ( 1 , nun / 2 , s s / 2 )
#sample a new v a l u e o f mu
vmu<‚àí 1 / (m/ tau2+1/g20 )
emu<‚àí vmu‚àó (m‚àómean ( t h e t a ) / tau2 + mu0/ g20 )
mu<‚àírnorm ( 1 , emu , s q r t (vmu ) )
# sample a new v a l u e o f tau2
etam<‚àíe t a 0+m
s s <‚àí e t a 0 ‚àó t 2 0 + sum ( ( t h e t a ‚àímu) ÀÜ 2 )
tau2 <‚àí1/rgamma ( 1 , etam / 2 , s s / 2 )
#s t o r e r e s u l t s
THETA[ s ,]<‚àí t h e t a
SMT[ s ,]<‚àí c ( sigma2 , mu, tau2 )
}
###

Running this algorithm produces an S √ó m matrix THETA, containing a value
of the within-school mean for each school at each iteration of the Markov

8.4 Example: Math scores in U.S. public schools

139

chain. Additionally, the S √ó 3 matrix SMT stores values of œÉ 2 , ¬µ and œÑ 2 , representing approximate, correlated draws from the posterior distribution of these
parameters.
MCMC diagnostics

‚óè
‚óè
‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè

‚óè
‚óè

‚óè

‚óè
‚óè
‚óè
‚óè

‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè

‚óè

90

‚óè

45

‚óè
‚óè
‚óè

49

‚óè

95

50

Before we make inference using these MCMC samples we should determine if
there might be any problems with the Gibbs sampler. The first thing we want
to do is to see if there are any indications that the chain is not stationary,
i.e. if the simulated parameter values are moving in a consistent direction.
One way to do this is with traceplots, or plots of the parameter values versus
iteration number. However, when the number of samples is large, such plots
can be difficult to read because of the high density of the plotted points (see,
for example, Figure 6.6). Standard practice is to plot only a subsequence of
MCMC samples, such as every 100th sample. Another approach is to produce boxplots of sequential groups of samples, as is done in Figure 8.6. The
first boxplot in the first plot, for example, represents the empirical distribution of {¬µ(1) , . . . , ¬µ(500) }, the second boxplot represents the distribution of
{¬µ(501) , . . . , ¬µ(1000) }, and so on. Each of the 10 boxplots represents 1/10th of
the MCMC samples. If stationarity has been achieved, then the distribution
of samples in any one boxplot should be the same as that in any other. If
we were to see that the medians or interquartile ranges of the boxplots were
moving in a consistent direction with iteration number, then we would suspect
that stationarity had not been achieved and we would have to run the chain
longer.

‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè

25
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

500

‚óè

2000 3500
iteration

5000

‚óè
‚óè

500

‚óè

‚óè

‚óè
‚óè
‚óè

‚óè

‚óè
‚óè

15

‚óè
‚óè

80

‚óè
‚óè

‚óè

75

47
‚óè

46

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

œÑ2

œÉ2
85

¬µ
48

35

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè

2000 3500
iteration

5000

‚óè

500

2000 3500
iteration

5000

Fig. 8.6. Stationarity plots of the MCMC samples of ¬µ, œÉ 2 and œÑ 2 .

There does not seem to be any evidence that the chain has not achieved
stationarity, so we move on to see how quickly the Gibbs sampler is moving
around the parameter space. Lag-1 autocorrelations for the sequences of ¬µ,
œÉ 2 and œÑ 2 are 0.15, 0.053 and 0.312, and the effective sample sizes are 3706,
4499 and 2503, respectively. Approximate Monte Carlo standard errors can be
obtained by dividing the approximated posterior standard deviations by the

140

8 Group comparisons and hierarchical modeling

square root of the effective sample sizes, giving values of (0.009, 0.04, 0.09) for
¬µ, œÉ 2 and œÑ 2 respectively. These are quite small compared to the scale of the
approximated posterior expectations of these parameters, (48.12, 84.85, 24.84).
Diagnostics should also be performed for the Œ∏-parameters: The effective sample sizes for the 100 sequences of Œ∏-values ranged between 3,627 and 5,927,
with Monte Carlo standard errors ranging between 0.02 and 0.05.
8.4.2 Posterior summaries and shrinkage

46 47 48 49 50
¬µ

p( œÑ2|y 1...y m )
0.04
0.00

0.0

0.00

p(¬µ
¬µ|y 1...y m )
0.2
0.4
0.6

p( œÉ2|y 1...y m )
0.04 0.08 0.12

0.08

Figure 8.7 shows Monte Carlo approximations to the posterior densities of
{¬µ, œÉ 2 , œÑ 2 }. The posterior means of ¬µ, œÉ and œÑ are 48.12, 9.21 and 4.97 respec-

75

80

85 90
œÉ2

95

10

20

30
œÑ2

40

50

Fig. 8.7. Marginal posterior distributions, with 2.5%, 50% and 97.5% quantiles
given by vertical lines.

tively, indicating that roughly 95% of the scores within a classroom are within
4√ó9.21 ‚âà 37 points of each other, whereas 95% of the average classroom scores
are within 4 √ó 4.97 ‚âà 20 points of each other.
One of the motivations behind hierarchical modeling is that information
can be shared across groups. Recall that, conditional on ¬µ, œÑ 2 , œÉ 2 and the
data, the expected value of Œ∏j is a weighted average of y¬Øj and ¬µ:
E[Œ∏j |y j , ¬µ, œÑ, œÉ] =

y¬Øj nj /œÉ 2 + ¬µ/œÑ 2
.
nj /œÉ 2 + 1/œÑ 2

As a result, the expected value of Œ∏j is pulled a bit from y¬Øj towards ¬µ by
an amount depending on nj . This effect is called shrinkage. The first panel
of Figure 8.8 plots y¬Øj versus Œ∏ÀÜj = E[Œ∏j |y 1 , . . . , y m ] for each group. Notice
that the relationship roughly follows a line with a slope that is less than
one, indicating that high values of y¬Øj correspond to slightly less high values
of Œ∏ÀÜj , and low values of y¬Øj correspond to slightly less low values of Œ∏ÀÜj . The
second panel of the plot shows the amount of shrinkage as a function of the
group-specific sample size. Groups with low sample sizes get shrunk the most,

8.4 Example: Math scores in U.S. public schools

141

60

‚óè

‚óè
‚óè

6
4
y ‚àí Œ∏^
2

‚óè
‚óè
‚óè
‚óè‚óè

40

45

50
y

0

‚óè

55

60

65

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè
‚óè ‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè

‚àí2

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè

‚óè

‚óè

‚àí4

40

45

Œ∏^
50

55

‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè

8

whereas groups with large sample sizes hardly get shrunk at all. This makes
sense: The larger the sample size for a group, the more information we have
for that group and the less information we need to ‚Äúborrow‚Äù from the rest of
the population.

‚óè

5

‚óè

10

15
20
25
sample size

30

Fig. 8.8. Shrinkage as a function of sample size.

Suppose our task is to rank the schools according to what we think their
performances would be if every student in each school took the math exam.
In this case, it makes sense to rank the schools according to the schoolspecific posterior expectations {E[Œ∏1 |y 1 , . . . , y m ], . . . , E[Œ∏m |y 1 , . . . , y m ]}. Alternatively, we could ignore the results of the hierarchical model and just use
the school-specific sample averages {¬Ø
y1 , . . . , y¬Øm }. The two methods will give
similar but not exactly the same rankings. Consider the posterior distributions
of Œ∏46 and Œ∏82 , as shown in Figure 8.9. Both of these schools have exceptionally
low sample means, in the bottom 10% of all schools. The first thing to note
is that the posterior density for school 46 is more peaked than that of school
82. This is because the sample size for school 46 is 21 students, whereas that
of school 82 is only 5 students. Therefore, our degree of certainty for Œ∏46 is
much higher than that for Œ∏82 .
The raw data for the two schools are shown in dotplots below the posterior
densities, with the large dots representing the sample means y¬Ø46 and y¬Ø82 . Note
that while the posterior expectation for school 82 is higher than that of 46
(42.53 compared to 41.31), the sample mean for school 82 is lower than that of
46 (38.76 compared to 40.18). Does this make sense? Suppose on the day of the
exam the student who got the lowest exam score in school 82 did not come to
class. Then the sample mean for school 82 would have been 41.99 as opposed to
38.76, a change of more than three points. In contrast, if the lowest performing
student in school 46 had not shown up, y¬Ø46 would have been 40.9 as opposed

142

8 Group comparisons and hierarchical modeling

0.2

to 40.18, a change of only three quarters of a point. In other words, the low
value of the sample mean for school 82 can be explained by either Œ∏82 being
very low, or just the possibility that a few of the five sampled students were
among the poorer performing students in the school. In contrast, for school 46
this latter possibility cannot explain the low value of the sample mean: Even
if a few of the sampled students were unrepresentative of their school-specific
average, it would not affect the sample mean as much because of the larger
sample size. For this reason, it makes sense to shrink the expectation of school
82 towards the population expectation E[¬µ|y 1 , . . . , y n ] = 48.11 by a greater
amount than for the expectation of school 46.

0.0

0.1

school 46
school 82
E[¬µ
¬µ|y 1...y m ]

‚óè ‚óè

‚óè

‚óè ‚óè‚óè

‚óè

‚óè

30

‚óè‚óè ‚óè
‚óè

‚óè‚óè

‚óè‚óè

‚óè ‚óè ‚óè ‚óè‚óè

‚óè

‚óè ‚óè
‚óè

40

‚óè

‚óè

‚óè

50

60

math score

Fig. 8.9. Data and posterior distributions for two schools.

To some people this reversal of rankings may seem strange or ‚Äúunfair:‚Äù
The performance by the sampled students in school 46 was better on average
than those sampled in school 82, so why should they be ranked lower? While
‚Äúfairness‚Äù may be debated, the hierarchical model reflects the objective fact
that there is more evidence that Œ∏46 is exceptionally low than there is evidence
that Œ∏82 is exceptionally low. There are many other real-life situations where
differing amounts of evidence results in a switch of a ranking. For example,
on any given basketball team there are ‚Äúbench‚Äù players who play very few
minutes during any given game. As such, many bench players have taken
only a few free throws in their entire career, and many have an observed free
throw shooting percentage of 100%. Under certain circumstances during a
basketball game (a ‚Äútechnical foul‚Äù) the coach has the opportunity to choose
from among any of his or her players to take a free throw and hopefully score
a point. In practice, coaches always choose an experienced veteran player with
a percentage of around 87% over a bench player who has made, for example
three of three free throws in his career. While it may seem ‚Äúunfair,‚Äù it is the

8.5 Hierarchical modeling of means and variances

143

right decision to make: The coaches recognize that it is very unlikely that the
bench player‚Äôs true free throw shooting percentage is anywhere near 100%.

8.5 Hierarchical modeling of means and variances
If the population means vary across groups, shouldn‚Äôt we allow for the
possibility that the population variances also vary across groups? Letting
œÉj2 be the variance for group j, our sampling model would then become
Y1,j , . . . , Ynj ,j ‚àº i.i.d. normal(Œ∏j , œÉj2 ), and our full conditional distribution for
each Œ∏j would be
!
nj y¬Øj /œÉj2 + 1/œÑ 2
2
2
2 ‚àí1
{Œ∏j |y1,j , . . . , ynj ,j , œÉj } ‚àº normal
, [nj /œÉj + 1/œÑ ]
.
nj /œÉj2 + 1/œÑ 2
How does œÉj2 get estimated? If we were to specify that
2
œÉ12 , . . . , œÉm
‚àº i.i.d. gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2),

(8.4)

then as is shown in Chapter 6 the full conditional distribution of œÉj2 is


X
{1/œÉj2 |y1,j , . . . , ynj ,j , Œ∏j } ‚àº gamma [ŒΩ0 + nj ]/2, [ŒΩ0 œÉ02 +
(yi,j ‚àí Œ∏j )2 ]/2 ,
2
and estimation for œÉ12 , . . . , œÉm
can proceed by iteratively sampling their values
along with the other parameters in a Gibbs sampler.
If ŒΩ0 and œÉ02 are fixed in advance at some particular values, then the
distribution in (8.4) represents a prior distribution on variances such that, for
2
2
2
|œÉ12 , . . . , œÉm‚àí1
) = p(œÉm
), and so the information we may have
example, p(œÉm
2
2
2
about œÉ1 , . . . , œÉm‚àí1 is not used to help us estimate œÉm
. This seems inefficient:
2
If the sample size in group m were small and we saw that œÉ12 , . . . , œÉm‚àí1
were
tightly concentrated around a particular value, then we would want to use this
2
fact to improve our estimation of œÉm
. In other words, we want to be able to
learn about the sampling distribution of the œÉj2 ‚Äôs and use this information to
improve our estimation for groups that may have low sample sizes. This can be
done by treating ŒΩ0 and œÉ02 as parameters to be estimated, in which case (8.4)
is properly thought of as a sampling model for across-group heterogeneity in
population variances, and not as a prior distribution. Putting this together
with our model for heterogeneity in population means gives a hierarchical
model for both means and variances, which is depicted graphically in Figure
8.10.
2
The unknown parameters to be estimated include {(Œ∏1 , œÉ12 ), . . . , (Œ∏m , œÉm
)}
2
representing the within-group sampling distributions, {¬µ, œÑ } representing
across-group heterogeneity in means and {ŒΩ0 , œÉ02 } representing across-group
heterogeneity in variances. As before, the joint posterior distribution for all of

144

8 Group comparisons and hierarchical modeling
¬µ, œÑ 2
Œ∏1

Œ∏2

¬∑¬∑¬∑

Œ∏m‚àí1

Y1

Y2

¬∑¬∑¬∑

Y m‚àí1 Y m

œÉ12

œÉ22

¬∑¬∑¬∑

2
œÉm‚àí1

Œ∏m

2
œÉm

ŒΩ0 , œÉ02
Fig. 8.10. A graphical representation of the hierarchical normal model with heterogeneous means and variances.

these parameters can be approximated by iteratively sampling each parameter from its full conditional distribution given the others. The full conditional
distributions for ¬µ and œÑ 2 are unchanged from the previous section and the
full conditional distributions of Œ∏j and œÉj2 are given above. What remains to
do is to specify the prior distributions for ŒΩ0 and œÉ02 and obtain their full
conditional distributions. A conjugate class of prior densities for œÉ02 are the
gamma densities: If p(œÉ02 ) ‚àº gamma(a, b), then it is straightforward to show
that
m

1
1X
2
(1/œÉj2 )).
p(œÉ02 |œÉ12 , . . . , œÉm
, ŒΩ0 ) = dgamma(a + mŒΩ0 , b +
2
2 j=1
Notice that for small values of a and b the conditional mean of œÉ02 is approxi2
mately the harmonic mean of œÉ12 , . . . , œÉm
.
A simple conjugate prior for ŒΩ0 does not exist, but if we restrict ŒΩ0 to be
a whole number, then it is easy to sample from its full conditional distribution. For example, if we let the prior on ŒΩ0 be the geometric distribution on
{1, 2, . . .} so that p(ŒΩ0 ) ‚àù e‚àíŒ±ŒΩ0 , then the full conditional distribution of ŒΩ0 is
proportional to
2
p(ŒΩ0 |œÉ02 , œÉ12 , . . . , œÉm
)
2
2
‚àù p(ŒΩ0 ) √ó p(œÉ1 , . . . , œÉm
|ŒΩ0 , œÉ02 )
Ô£´
Ô£∂ŒΩ0 /2‚àí1

m Y
m
1
1 X
(ŒΩ0 œÉ02 /2)ŒΩ0 /2
Ô£≠
Ô£∏
√ó exp{‚àíŒΩ0 (Œ± + œÉ02
(1/œÉj2 ))}.
‚àù
2
Œì (ŒΩ0 /2)
œÉ
2
j=1 j

While not pretty, this unnormalized probability distribution can be computed
for a large range of ŒΩ0 -values and then sampled from. For example, the R-code
to sample from this distribution is as follows:

8.5 Hierarchical modeling of means and variances

145

# NUMAX, a l p h a must be s p e c i f i e d ,
# sigma2 . s c h o o l s i s t h e v e c t o r o f c u r r e n t v a l u e s
# of the s choo l s p e c i f i c population v a r i a n c e s .
x<‚àí1:NUMAX
lpnu0<‚àí m‚àó ( . 5 ‚àó x‚àó l o g ( s 2 0 ‚àóx/2)‚àílgamma ( x / 2 ) ) +
( x/2 ‚àí1)‚àósum ( l o g ( 1 / sigma2 . s c h o o l s ) )
+
‚àíx ‚àó ( a l p h a + . 5 ‚àó s 2 0 ‚àósum ( 1 / sigma2 . s c h o o l s ) )
nu0<‚àísample ( x , 1 , prob=exp ( lpnu0‚àímax( lpnu0 ) ) )

8.5.1 Analysis of math score data

0.0

0.00

p(¬µ
¬µ|y 1...y m )
0.4

p(œÑœÑ2|y 1...y m )
0.04
0.08

Let‚Äôs re-analyze the math score data with our hierarchical model for schoolspecific means and variances. We‚Äôll take the parameters in our prior distributions to be the same as in the previous section, with Œ± = 1 and
{a = 1, b = 100} for the prior distributions on ŒΩ0 and œÉ02 . After running a
Gibbs sampler for 5,000 iterations, the posterior distributions of {¬µ, œÑ 2 , ŒΩ0 , œÉ02 }
are approximated and plotted in Figure 8.11. Additionally, the posterior distributions of ¬µ and œÑ 2 under the hierarchical model with constant group variance
is shown in gray lines for comparison.

47

48
¬µ

49

50

10

20

30
œÑ2

40

50

0

0.00

p(ŒΩ
ŒΩ0|y 1...y m )
200
500

p(œÉ
œÉ20|y 1...y m )
0.04 0.08

46

7

9 11

14

17
ŒΩ0

20

23

26

60

65

70

75

80
œÉ20

85

90

95

Fig. 8.11. Posterior distributions of between-group heterogeneity parameters.

The hierarchical model of Section 8.3, in which all within-group variances
are forced to be equal, is equivalent to a value of ŒΩ0 = ‚àû in this hierarchical

146

8 Group comparisons and hierarchical modeling

140

‚óè

20
50

100
s2

10

‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè

‚óè

150

‚óè

‚óè

^
s2 ‚àí œÉ 2
‚àí10 0

‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚àí30

120
80
60

‚óè‚óè
‚óè

‚óè

‚óè
‚óè
‚óè

^
œÉ2
100

30

model. In contrast, a low value of ŒΩ0 = 1 would indicate that the variances are
quite unequal, and little information about variances should be shared across
groups. Our hierarchical analysis indicates that neither of these extremes is
appropriate, as the posterior distribution of ŒΩ0 is concentrated around a moderate value of 14 or 15. This estimated distribution of œÉ12 , . . . , œÉn2 is used to
shrink extreme sample variances towards an across-group center, as is shown
in Figure 8.12. The relationship between sample size and the amount of variance shrinkage is shown in the second panel of the plot. As with estimation for
the group means, the larger amounts of shrinkage generally occur for groups
with smaller sample sizes.

5

10

15
20
25
sample size

30

Fig. 8.12. Shrinkage as a function of sample size.

8.6 Discussion and further references
Lindley and Smith (1972) laid the foundation for Bayesian hierarchical modeling, although the idea of shrinking the estimates of the individual group
means towards an across-group mean goes back at least to Kelley (1927) in
the context of educational testing. In the statistical literature, the benefits
of this type of estimation are referred to as the ‚ÄúStein effect‚Äù (Stein, 1956,
1981). Estimators of this type generally take the form Œ∏ÀÜj = wj y¬Øj + (1 ‚àí wj )¬Ø
y,
where y¬Ø is an average over all groups and the wj ‚Äôs depend on nj , œÉ 2 and
œÑ 2 . So-called empirical Bayes procedures obtain estimates of œÉ 2 and œÑ 2 from
the data, then plug these values into the formula for Œ∏ÀÜj (Efron and Morris,
1973; Casella, 1985). Such procedures often yield estimates of the Œ∏j ‚Äôs that
are nearly equivalent to those from Bayesian procedures, but ignore uncer-

8.6 Discussion and further references

147

tainty in the values of œÉ 2 and œÑ 2 . For a detailed treatment of empirical Bayes
methods, see Carlin and Louis (1996).
Terminology for hierarchical models is inconsistent in the literature. For
the simple hierarchical model yi,j = Œ∏j +i,j , Œ∏j = ¬µ+Œ≥j , the Œ∏j ‚Äôs (or Œ≥j ‚Äôs) may
be referred to as either ‚Äúfixed effects‚Äù or ‚Äúrandom effects,‚Äù usually depending
on how they are estimated. The distribution of the Œ∏j ‚Äôs is unfortunately often
referred to as a prior distribution, which mischaracterizes Bayesian inference
and renders the distinction between prior information and population distribution somewhat meaningless. For a discussion of some of this confusion, see
Gelman and Hill (2007, pp. 245-246).
Hierarchical modeling of variances is not common, perhaps due to the
mean parameters being of greater interest. However, erroneously assuming a
common within-group variance could lead to improper pooling of information,
or to the shrinkage of group-specific parameters by inappropriate amounts.

9
Linear regression

Linear regression modeling is an extremely powerful data analysis tool, useful
for a variety of inferential tasks such as prediction, parameter estimation and
data description. In this section we give a very brief introduction to the linear regression model and the corresponding Bayesian approach to estimation.
Additionally, we discuss the relationship between Bayesian and ordinary least
squares regression estimates.
One difficult aspect of regression modeling is deciding which explanatory
variables to include in a model. This variable selection problem has a natural
Bayesian solution: Any collection of models having different sets of regressors
can be compared via their Bayes factors. When the number of possible regressors is small, this allows us to assign a posterior probability to each regression
model. When the number of regressors is large, the space of models can be
explored with a Gibbs sampling algorithm.

9.1 The linear regression model
Regression modeling is concerned with describing how the sampling distribution of one random variable Y varies with another variable or set of variables
x = (x1 , . . . , xp ). Specifically, a regression model postulates a form for p(y|x),
the conditional distribution of Y given x. Estimation of p(y|x) is made using
data y1 , . . . , yn that are gathered under a variety of conditions x1 , . . . , xn .
Example: Oxygen uptake (from Kuehl (2000))
Twelve healthy men who did not exercise regularly were recruited to take part
in a study of the effects of two different exercise regimen on oxygen uptake. Six
of the twelve men were randomly assigned to a 12-week flat-terrain running
program, and the remaining six were assigned to a 12-week step aerobics
program. The maximum oxygen uptake of each subject was measured (in
liters per minute) while running on an inclined treadmill, both before and
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 9,
c Springer Science+Business Media, LLC 2009


150

9 Linear regression

after the 12-week program. Of interest is how a subject‚Äôs change in maximal
oxygen uptake may depend on which program they were assigned to. However,
other factors, such as age, are expected to affect the change in maximal uptake
as well.

change in maximal oxygen uptake
‚àí10 ‚àí5
0
5
10
15

‚óè

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè

‚óè

‚óè

‚óè

aerobic
running

‚óè
‚óè

20

22

24

26

28

30

age

Fig. 9.1. Change in maximal oxygen uptake as a function of age and exercise
program.

How might we estimate the conditional distribution of oxygen uptake for
a given exercise program and age? One possibility would be to estimate a
population mean and variance for each age and program combination. For
example, we could estimate a mean and variance from the 22-year-olds in
the study who were assigned the running program, and a separate mean and
variance for the 22-year-olds assigned to the aerobics program. The data from
this study, shown in Figure 9.1, indicate that such an approach is problematic.
For example, there is only one 22-year-old assigned to the aerobics program,
which is not enough data to provide information about a population variance.
Furthermore, there are many age/program combinations for which there are
no data.
One solution to this problem is to assume that the conditional distribution
p(y|x) changes smoothly as a function of x, so that data we have at one value
of x can inform us about what might be going on at a different value. A linear
regression model is a particular type of smoothly changing model for p(y|x)
that specifies that the conditional expectation E[Y |x] has a form that is linear
in a set of parameters:
Z
yp(y|x) dy = E[Y |x] = Œ≤1 x1 + ¬∑ ¬∑ ¬∑ + Œ≤p xp = Œ≤ T x .

9.1 The linear regression model

151

It is important to note that such a model allows a great deal of freedom for
x1 , . . . , xp . For example, in the oxygen uptake example we could let x1 = age
and x2 = age2 if we thought there might be a quadratic relationship between
maximal uptake and age. However, Figure 9.1 does not indicate any quadratic
relationships, and so a reasonable model for p(y|x) could include two different
linear relationships between age and uptake, one for each group:
Yi
xi,1
xi,2
xi,3
xi,4

= Œ≤1 xi,1 + Œ≤2 xi,2 + Œ≤3 xi,3 + Œ≤4 xi,4 + i , where
= 1 for each subject i
= 0 if subject i is on the running program, 1 if on aerobic
= age of subject i
= xi,2 √ó xi,3

(9.1)

Under this model the conditional expectations of Y for the two different levels
of xi,1 are
E[Y |x] = Œ≤1 + Œ≤3 √ó age if x1 = 0, and
E[Y |x] = (Œ≤1 + Œ≤2 ) + (Œ≤3 + Œ≤4 ) √ó age if x1 = 1.
In other words, the model assumes that the relationship is linear in age for
both exercise groups, with the difference in intercepts given by Œ≤2 and the
difference in slopes given by Œ≤4 . If we assumed that Œ≤2 = Œ≤4 = 0, then we
would have an identical line for both groups. If we assumed Œ≤4 = 0 then
we would have a different line for each group but they would be parallel.
Allowing all coefficients to be non-zero gives us two unrelated lines. Some
different possibilities are depicted graphically in Figure 9.2.
We still have not specified anything about p(y|x) beyond E[Y |x]. The normal linear regression model specifies that, in addition to E[Y |x] being linear,
the sampling variability around the mean is i.i.d. from a normal distribution:
1 , . . . , n ‚àº i.i.d. normal(0, œÉ 2 )
Yi = Œ≤ T xi + i .
This model provides a complete specification of the joint probability density
of observed data y1 , . . . , yn conditional upon x1 , . . . , xn and values of Œ≤ and
œÉ2 :
p(y1 , . . . , yn |x1 , . . . xn , Œ≤, œÉ 2 )
n
Y
=
p(yi |xi , Œ≤, œÉ 2 )

(9.2)

i=1

= (2œÄœÉ 2 )‚àín/2 exp{‚àí

n
1 X
(yi ‚àí Œ≤ T xi )2 }.
2œÉ 2 i=1

(9.3)

Another way to write this joint probability density is in terms of the multivariate normal distribution: Let y be the n-dimensional column vector

152

9 Linear regression

change in maximal oxygen uptake
‚àí10 ‚àí5
0
5
10
15

Œ≤3 = 0 Œ≤4 = 0

Œ≤2 = 0 Œ≤4 = 0
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

change in maximal oxygen uptake
‚àí10 ‚àí5
0
5
10
15

Œ≤4 = 0
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

20

‚óè

22

‚óè

24

26
age

28

30

20

22

24

26
age

28

30

Fig. 9.2. Least squares regression lines for the oxygen uptake data, under four
different models.

(y1 , . . . , yn )T , and let X be the n √ó p matrix whose ith row is xi . Then the
normal regression model is that
{y|X, Œ≤, œÉ 2 } ‚àº multivariate normal (XŒ≤, œÉ 2 I),
where I is the p √ó p identity matrix and
Ô£´
Ô£∂
Ô£∂
Ô£∂ Ô£´
x1 ‚Üí Ô£´ Ô£∂ Ô£´
Œ≤1 x1,1 + ¬∑ ¬∑ ¬∑ + Œ≤p x1,p
E[Y1 |Œ≤, x1 ]
Ô£¨ x2 ‚Üí Ô£∑ Œ≤1
Ô£¨
Ô£∑Ô£¨ Ô£∑ Ô£¨
Ô£∑ Ô£¨
Ô£∑
..
..
XŒ≤ = Ô£¨ . Ô£∑ Ô£≠ ... Ô£∏ = Ô£≠
Ô£∏=Ô£≠
Ô£∏.
.
.
Ô£≠ .. Ô£∏
Œ≤p
Œ≤1 xn,1 + ¬∑ ¬∑ ¬∑ + Œ≤p xn,p
E[Yn |Œ≤, xn ]
xn ‚Üí
The density (9.3) depends on Œ≤ through the residuals (yi ‚àí Œ≤ T xi ). Given
the observed data, the term P
in the exponent is maximized when the sum of
n
squared residuals, SSR(Œ≤) = i=1 (yi ‚àíŒ≤ T xi )2 is minimized. To find the value

9.1 The linear regression model

153

of Œ≤ at which this minimum occurs it is helpful to rewrite SSR(Œ≤) in matrix
notation:
SSR(Œ≤) =

n
X

(yi ‚àí Œ≤ T xi )2 = (y ‚àí XŒ≤)T (y ‚àí XŒ≤)

i=1

= y T y ‚àí 2Œ≤ T XT y + Œ≤ T XT XŒ≤ .
Recall from calculus that
d
1. a minimum of a function g(z) occurs at a value z such that dz
g(z) = 0;
2. the derivative of g(z) = az is a and the derivative of g(z) = bz 2 is 2bz.

These facts translate over to the multivariate case, and can be used to obtain
the minimizer of SSR(Œ≤):

d
d  T
SSR(Œ≤) =
y y ‚àí 2Œ≤ T XT y + Œ≤ T XT XŒ≤
dŒ≤
dŒ≤
= ‚àí2XT y + 2XT XŒ≤ , therefore
d
SSR(Œ≤) = 0 ‚áî ‚àí2XT y + 2XT XŒ≤ = 0
dŒ≤
‚áî XT XŒ≤ = XT y
‚áî Œ≤ = (XT X)‚àí1 XT y .
ÀÜ = (XT X)‚àí1 XT y is called the ‚Äúordinary least squares‚Äù (OLS)
The value Œ≤
ols
estimate of Œ≤, as it provides the value of Œ≤ that minimizes the sum of squared
residuals. This value is unique as long as the inverse (XT X)‚àí1 exists. The
ÀÜ also plays a role in Bayesian estimation, as we shall see in the next
value Œ≤
ols
section.
9.1.1 Least squares estimation for the oxygen uptake data
Let‚Äôs find the least squares regression estimates for the model in Equation 9.1,
and use the results to evaluate differences between the two exercise groups.
The ages of the 12 subjects, along with their observed changes in maximal
oxygen uptake, are
x3 = (23, 22, 22, 25, 27, 20, 31, 23, 27, 28, 22, 24)
y = (‚àí0.87, ‚àí10.74, ‚àí3.27, ‚àí1.97, 7.50, ‚àí7.25, 17.05, 4.96, 10.40,
11.05, 0.26, 2.51) ,
with the first six elements of each vector corresponding to the subjects in
the running group and the latter six corresponding to subjects in the aerobics
group. After constructing the 12 √ó 4 matrix X out of the vectors x1 , x2 , x3 , x4
defined as in (9.1), the matrices XT X and XT y can be computed:

154

9 Linear regression

Ô£´

12
Ô£¨
6
XT X = Ô£¨
Ô£≠ 294
155

6
6
155
155

294
155
7314
4063

Ô£∂
Ô£´
Ô£∂
155
29.63
Ô£¨
Ô£∑
155 Ô£∑
Ô£∑ XT y = Ô£¨ 46.23 Ô£∑ .
Ô£≠ 978.81 Ô£∏
4063 Ô£∏
4063
1298.79

Inverting the XT X matrix and multiplying the result by XT y give the vector
ÀÜ = (‚àí51.29, 13.11, 2.09, ‚àí.32)T . This means that the estimated linear relaŒ≤
ols
tionship between uptake and age has an intercept and slope of -51.29 and 2.09
for the running group, and ‚àí51.29 + 13.11 = ‚àí38.18 and 2.09 ‚àí 0.32 = 1.77
for the aerobics group. These two lines are plotted in the fourth panel of FigÀÜ )/(n ‚àí p),
ure 9.2. An unbiased estimate of œÉ 2 can be obtained from SSR(Œ≤
ols
2
which for these data gives œÉ
ÀÜols = 8.54. The sampling variance of the vector
ÀÜ can be shown to be equal to (XT X)‚àí1 œÉ 2 . We do not know the true value
Œ≤
ols
2
of œÉ 2 , but the value of œÉ
ÀÜols
can be plugged in to give standard errors for the
ÀÜols . These are 12.25, 15.76, 0.53 and 0.65 for the four regrescomponents of Œ≤
ÀÜols to their standard errors
sion coefficients in order. Comparing the values of Œ≤
suggests that the evidence for differences between the two exercise regimen is
not very strong. We will explore this further in the next few sections.

9.2 Bayesian estimation for a regression model
We begin with a simple semiconjugate prior distribution for Œ≤ and œÉ 2 to be
used when there is information available about the parameters. In situations
where prior information is unavailable or difficult to quantify, an alternative
‚Äúdefault‚Äù class of prior distributions is given.
9.2.1 A semiconjugate prior distribution
The sampling density of the data (Equation 9.3), as a function of Œ≤, is
1
SSR(Œ≤)}
2œÉ 2
1
= exp{‚àí 2 [y T y ‚àí 2Œ≤ T XT y + Œ≤ T XT XŒ≤]}.
2œÉ

p(y|X, Œ≤, œÉ 2 ) ‚àù exp{‚àí

The role that Œ≤ plays in the exponent looks very similar to that played by
y, and the distribution of y is multivariate normal. This suggests that a
multivariate normal prior distribution for Œ≤ is conjugate. Let‚Äôs see if this is
correct: If Œ≤ ‚àº multivariate normal(Œ≤ 0 , Œ£0 ), then
p(Œ≤|y, X, œÉ 2 )
‚àù p(y|X, Œ≤, œÉ 2 ) √ó p(Œ≤)
1
1
‚àù exp{‚àí (‚àí2Œ≤ T XT y/œÉ 2 + Œ≤ T XT XŒ≤/œÉ 2 ) ‚àí (‚àí2Œ≤ T Œ£0‚àí1 Œ≤ 0 + Œ≤ T Œ£0‚àí1 Œ≤)}
2
2
1 T ‚àí1
T
T
‚àí1
2
= exp{Œ≤ (Œ£0 Œ≤ 0 + X y/œÉ ) ‚àí Œ≤ (Œ£0 + XT X/œÉ 2 )Œ≤}.
2

9.2 Bayesian estimation for a regression model

155

Referring back to Chapter 7, we recognize this as being proportional to a
multivariate normal density, with
Var[Œ≤|y, X, œÉ 2 ] = (Œ£0‚àí1 + XT X/œÉ 2 )‚àí1
E[Œ≤|y, X, œÉ 2 ] = (Œ£0‚àí1 + XT X/œÉ 2 )‚àí1 (Œ£0‚àí1 Œ≤ 0 + XT y/œÉ 2 ).

(9.4)
(9.5)

As usual, we can gain some understanding of these formulae by considering
some limiting cases. If the elements of the prior precision matrix Œ£0‚àí1 are small
in magnitude, then the conditional expectation E[Œ≤|y, X, œÉ 2 ] is approximately
equal to (XT X)‚àí1 XT y, the least squares estimate. On the other hand, if the
measurement precision is very small (œÉ 2 is very large), then the expectation
is approximately Œ≤ 0 , the prior expectation.
As in most normal sampling problems, the semiconjugate prior distribution
for œÉ 2 is an inverse-gamma distribution. Letting Œ≥ = 1/œÉ 2 be the measurement
precision, if Œ≥ ‚àº gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2), then
p(Œ≥|y, X, Œ≤) ‚àù p(Œ≥)p(y|X, Œ≤, Œ≥)
h
i h
i
‚àù Œ≥ ŒΩ0 /2‚àí1 exp(‚àíŒ≥ √ó ŒΩ0 œÉ02 /2) √ó Œ≥ n/2 exp(‚àíŒ≥ √ó SSR(Œ≤)/2)
= Œ≥ (ŒΩ0 +n)/2‚àí1 exp(‚àíŒ≥[ŒΩ0 œÉ02 + SSR(Œ≤)]/2),
which we recognize as a gamma density, so that
{œÉ 2 |y, X, Œ≤} ‚àº inverse-gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSR(Œ≤)]/2).
Constructing a Gibbs sampler to approximate the joint posterior distribution
p(Œ≤, œÉ 2 |y, X) is then straightforward: Given current values {Œ≤ (s) , œÉ 2(s) }, new
values can be generated by
1. updating Œ≤:
a) compute V = Var[Œ≤|y, X, œÉ 2(s) ] and m = E[Œ≤|y, X, œÉ 2(s) ]
b) sample Œ≤ (s+1) ‚àº multivariate normal(m, V)
2. updating œÉ 2 :
a) compute SSR(Œ≤ (s+1) )
b) sample œÉ 2(s+1) ‚àº inverse-gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSR(Œ≤ (s+1) )]/2).
9.2.2 Default and weakly informative prior distributions
A Bayesian analysis of a regression model requires specification of the prior
parameters (Œ≤ 0 , Œ£0 ) and (ŒΩ0 , œÉ02 ). Finding values of these parameters that
represent actual prior information can be difficult. In the oxygen uptake experiment, for example, a quick scan of a few articles on exercise physiology
indicates that males in their 20s have an oxygen uptake of around 150 liters
per minute with a standard deviation of 15. If we take 150¬±2√ó15 = (120, 180)
as a prior expected range of the oxygen uptake distribution, then the changes
in oxygen uptake should lie within (-60,60) with high probability. Considering

156

9 Linear regression

our subjects in the running group, this means that the line Œ≤1 + Œ≤3 x should
produce values between -60 and 60 for all values of x between 20 and 30. A
little algebra then shows that we need a prior distribution on (Œ≤1 , Œ≤3 ) such
that ‚àí300 < Œ≤1 < 300 and ‚àí12 < Œ≤3 < 12 with high probability. This could
be done by taking Œ£0,1,1 = 1502 and Œ£0,2,2 = 62 , for example. However, we
would still need to specify the prior variances of the other parameters, as well
as the six prior correlations between the parameters. The task of constructing
an informative prior distribution only gets harder as the number
of regressors

increases, as the number of prior correlation parameters is p2 , which increases
quadratically in p.
Sometimes an analysis must be done in the absence of precise prior information, or information that is easily converted into the parameters of a conjugate prior distribution. In these situations one could stick to least squares
estimation, with the drawback that probability statements about Œ≤ would be
unavailable. Alternatively, it is sometimes possible to justify a prior distribution with other criteria. One idea is that, if the prior distribution is not
going to represent real prior information about the parameters, then it should
be as minimally informative as possible. The resulting posterior distribution
would then represent the posterior information of someone who began with
little knowledge of the population being studied. To some, such an analysis
would give a ‚Äúmore objective‚Äù result than using an informative prior distribution, especially one that did not actually represent real prior information.
One type of weakly informative prior is the unit information prior (Kass and
Wasserman, 1995). A unit information prior is one that contains the same
amount of information as that would be contained in only a single observaÀÜ is its inverse variance, or (XT X)/œÉ 2 .
tion. For example, the precision of Œ≤
ols
Since this can be viewed as the amount of information in n observations, the
amount of information in one observation should be ‚Äúone nth‚Äù as much, i.e.
(XT X)/(nœÉ 2 ). The unit information prior thus sets Œ£0‚àí1 = (XT X)/(nœÉ 2 ).
ÀÜ , thus centering
Kass and Wasserman (1995) further suggest setting Œ≤ 0 = Œ≤
ols
the prior distribution of Œ≤ around the OLS estimate. Such a distribution cannot be strictly considered a real prior distribution, as it requires knowledge of
y to be constructed. However, it only uses a small amount of the information
in y, and can be loosely thought of as the prior distribution of a person with
unbiased but weak prior information. In a similar way, the prior distribution
2
2
of œÉ 2 can be weakly centered around œÉ
ÀÜols
by taking ŒΩ0 = 1 and œÉ02 = œÉ
ÀÜols
.
Another principle for constructing a prior distribution for Œ≤ is based on
the idea that the parameter estimation should be invariant to changes in the
scale of the regressors. For example, suppose someone were to analyze the
oxygen uptake data using x
Àúi,3 = age in months, instead of xi,3 = age in years.
It makes sense that our posterior distribution for 12 √ó Œ≤Àú3 in the model with
x
Àúi,3 should be the same as the posterior distribution for Œ≤3 based on the model
with xi,3 . This condition requires, for example, that the posterior expected
change in y for a year change in age is the same, whether age is recorded in

9.2 Bayesian estimation for a regression model

157

terms of months or years. More generally, suppose X is a given set of regressors
Àú = XH for some p √ó p matrix H. If we obtain the posterior distribution
and X
Àú from y and X,
Àú then,
of Œ≤ from y and X, and the posterior distribution of Œ≤
according to this principle of invariance, the posterior distributions of Œ≤ and
Àú should be the same. Some linear algebra shows that this condition will
HŒ≤
be met if Œ≤ 0 = 0 and Œ£0 = k(XT X)‚àí1 for any positive value k. A popular
specification of k is to relate it to the error variance œÉ 2 , so that k = gœÉ 2 for
some positive value g. These choices of prior parameters result in a version
of the so-called ‚Äúg-prior‚Äù (Zellner, 1986), a widely studied and used prior
distribution for regression parameters (Zellner‚Äôs original g-prior allowed Œ≤ 0 to
be non-zero). Under this invariant g-prior the conditional distribution of Œ≤
given (y, X, œÉ 2 ) is still multivariate normal, but Equations 9.4 and 9.5 reduce
to the following simpler forms:
Var[Œ≤|y, X, œÉ 2 ] = [XT X/(gœÉ 2 ) + XT X/œÉ 2 ]‚àí1
g
=
œÉ 2 (XT X)‚àí1
g+1
E[Œ≤|y, X, œÉ 2 ] = [XT X/(gœÉ 2 ) + XT X/œÉ 2 ]‚àí1 XT y/œÉ 2
g
=
(XT X)‚àí1 XT y .
g+1

(9.6)

(9.7)

Parameter estimation under the g-prior is simplified as well: It turns out that,
under this prior distribution, p(œÉ 2 |y, X) is an inverse-gamma distribution,
which means that we can directly sample (œÉ 2 , Œ≤) from their posterior distribution by first sampling from p(œÉ 2 |y, X) and then from p(Œ≤|œÉ 2 , y, X).
Derivation of p(œÉ 2 |y, X)
The marginal posterior density of œÉ 2 is proportional to p(œÉ 2 ) √ó p(y|X, œÉ 2 ).
Using the rules of marginal probability, the latter term in this product can be
expressed as the following integral:
Z
p(y|X, œÉ 2 ) = p(y|X, Œ≤, œÉ 2 )p(Œ≤|X, œÉ 2 ) dŒ≤ .
Writing out the two densities inside the integral, we have
1
(y ‚àí XŒ≤)T (y ‚àí XŒ≤)] √ó
2œÉ 2
1
|2œÄgœÉ 2 (XT X)‚àí1 |‚àí1 exp[‚àí
Œ≤ T XT XŒ≤].
2gœÉ 2

p(y|X, Œ≤, œÉ 2 )p(Œ≤|X, œÉ 2 ) = (2œÄœÉ 2 )‚àín/2 exp[‚àí

Combining the terms in the exponents gives
i
1 h
‚àí 2 (y ‚àí XŒ≤)T (y ‚àí XŒ≤) + Œ≤ T XT XŒ≤/g
2œÉ
i
1 h
= ‚àí 2 y T y ‚àí 2y T XŒ≤ + Œ≤ T XT XŒ≤(1 + 1/g)
2œÉ
1
1
1
= ‚àí 2 y T y ‚àí (Œ≤ ‚àí m)T V‚àí1 (Œ≤ ‚àí m) + mT V‚àí1 m ,
2œÉ
2
2

158

9 Linear regression

where
V=

g
g
œÉ 2 (XT X)‚àí1 and m =
(XT X)‚àí1 XT y.
g+1
g+1

This means that we can write p(y|Œ≤, X)p(Œ≤|X, z, œÉ 2 ) as

 

1 T
1 T ‚àí1
2 ‚àín/2
‚àíp/2
(2œÄœÉ )
exp(‚àí 2 y y) √ó (1 + g)
exp( m V m) √ó
2œÉ
2


1
‚àí1/2
T ‚àí1
|2œÄV|
exp[‚àí (Œ≤ ‚àí m) V (Œ≤ ‚àí m)] .
2
The third term in the product is the only term that depends on Œ≤. This term is
exactly the multivariate normal density with mean m and variance V, which
as a probability density must integrate to 1. This means that if we integrate
the whole thing with respect to Œ≤ we are left with only the first two terms:
Z
2
p(y|X, œÉ ) = p(y|Œ≤, X)p(Œ≤|X, œÉ 2 ) dŒ≤

 

1
1
= (2œÄœÉ 2 )‚àín/2 exp(‚àí 2 y T y) √ó (1 + g)‚àíp/2 exp( mT V‚àí1 m) ,
2œÉ
2
which, after combining the terms in the exponents, is
p(y|X, œÉ 2 ) = (2œÄ)‚àín/2 (1 + g)‚àíp/2 (œÉ 2 )‚àín/2 exp(‚àí

1
SSRg ),
2œÉ 2

where SSRg is defined as
SSRg = y T y ‚àí mT V‚àí1 m = y T (I ‚àí

g
X(XT X)‚àí1 XT )y.
g+1

P
ÀÜ xi )2 as g ‚Üí ‚àû. The effect
The term SSRg decreases to SSRols = (yi ‚àí Œ≤
ols
of g is that it shrinks down the magnitude of the regression coefficients and
can prevent overfitting of the data.
The last step in identifying p(œÉ 2 |y, X) is to multiply p(y|X, œÉ 2 ) by the
prior distribution. Letting Œ≥ = 1/œÉ 2 ‚àº gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2), we have
p(Œ≥|y, X) ‚àù p(Œ≥)p(y|X, Œ≥)
h
i h
i
‚àù Œ≥ ŒΩ0 /2‚àí1 exp(‚àíŒ≥ √ó ŒΩ0 œÉ02 /2) √ó Œ≥ n/2 exp(‚àíŒ≥ √ó SSRg /2)
= Œ≥ (ŒΩ0 +n)/2‚àí1 exp[‚àíŒ≥ √ó (ŒΩ0 œÉ02 + SSRg )/2]
‚àù dgamma(Œ≥, [ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSRg ]/2) ,
and so {œÉ 2 |y, X} ‚àº inverse-gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSRg ]/2). These calculations, along with Equations 9.6 and 9.7, show that under this prior distribution, p(œÉ 2 |y, X) and p(Œ≤|y, X, œÉ 2 ) are inverse-gamma and multivariate
normal distributions respectively. Since we can sample from both of these distributions, samples from the joint posterior distribution p(œÉ 2 , Œ≤|y, X) can be
made with Monte Carlo approximation, and Gibbs sampling is unnecessary.
A sample value of (œÉ 2 , Œ≤) from p(œÉ 2 , Œ≤|y, X) can be made as follows:

9.2 Bayesian estimation for a regression model

159

1. sample 1/œÉ 2 ‚àº gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSRg ]/2);
g ÀÜ
g
2. sample Œ≤ ‚àº multivariate normal( g+1
Œ≤ ols , g+1
œÉ 2 [XT X]‚àí1 ).
R-code to generate multiple independent Monte Carlo samples from the posterior distribution is below:
data ( c h a p t e r 9 ) ; y<‚àíyX . o2uptake [ , 1 ] ; X<‚àíyX . o2uptake [ , ‚àí 1 ]
g<‚àíl e n g t h ( y ) ; nu0<‚àí1 ; s20 <‚àí8.54
S<‚àí1000
## data : y , X
## p r i o r p a r a m e t e r s : g , nu0 , s 2 0
## number o f i n d e p e n d e n t s a m p l e s t o g e n e r a t e : S
n<‚àídim (X ) [ 1 ] ; p<‚àídim (X ) [ 2 ]
Hg<‚àí ( g / ( g +1)) ‚àó X%‚àó%s o l v e ( t (X)%‚àó%X)%‚àó%t (X)
SSRg<‚àí t ( y)%‚àó%( d i a g ( 1 , nrow=n ) ‚àí Hg ) %‚àó%y
s2 <‚àí1/rgamma ( S , ( nu0+n ) / 2 , ( nu0 ‚àó s 2 0+SSRg ) / 2 )
Vb<‚àí g ‚àó s o l v e ( t (X)%‚àó%X) / ( g+1)
Eb<‚àí Vb%‚àó%t (X)%‚àó%y
E<‚àím at r i x ( rnorm ( S‚àóp , 0 , s q r t ( s 2 ) ) , S , p )
beta<‚àít ( t (E%‚àó%c h o l (Vb ) ) +c (Eb ) )

Bayesian analysis of the oxygen uptake data
We will use the invariant g-prior with g = n = 12, ŒΩ0 = 1 and œÉ02 =
2
œÉ
ÀÜols
= 8.54. The posterior mean of Œ≤ can be obtained directly from Equation 9.7: Since E[Œ≤|y, X, œÉ 2 ] does not depend on œÉ 2 , we have E[Œ≤|y, X] =
g ÀÜ
E[Œ≤|y, X, œÉ 2 ] = g+1
Œ≤ ols , so the posterior means of the four regression parameters are 12 √ó (‚àí51.29, 13.11, 2.09, ‚àí0.32)/13 = (‚àí47.35, 12.10, 1.93, ‚àí0.29).
Posterior standard deviations of these parameters are (14.41, 18.62, 0.62,
0.77), based on 1,000 independent Monte Carlo samples generated using the
R-code above. The marginal and joint posterior distributions for (Œ≤2 , Œ≤4 ) are
given in Figure 9.3, along with the (marginal) prior distributions for comparison. The posterior distributions seem to suggest only weak evidence of
a difference between the two groups, as the 95% quantile-based posterior intervals for Œ≤2 and Œ≤4 both contain zero. However, these parameters taken by
themselves do not quite tell the whole story. According to the model, the average difference in y between two people of the same age x but in different
exercise programs is Œ≤2 + Œ≤4 x. Thus the posterior distribution for the effect of
the aerobics program over the running program is obtained via the posterior
distribution of Œ≤2 + Œ≤4 x for each age x. Boxplots of these posterior distributions are shown in Figure 9.4, which indicates reasonably strong evidence of
a difference at young ages, and less evidence at the older ones.

9 Linear regression

‚àí50

0
Œ≤2

50

2
1
Œ≤4
0
‚àí1
‚àí2

0.000

0.010

0.0 0.1 0.2 0.3 0.4 0.5

0.020

160

100

‚àí2

0
Œ≤4

2

‚àí60

‚àí20

Œ≤2

20

60

‚àí10

‚àí5

Œ≤2 + Œ≤4age
0
5

10

15

Fig. 9.3. Posterior distributions for Œ≤2 and Œ≤4 , with marginal prior distributions in
the first two plots for comparison.

20

22

24

26

28

30

age

Fig. 9.4. Ninety-five percent confidence intervals for the difference in expected
change scores between aerobics subjects and running subjects.

9.3 Model selection
Often in regression analysis we are faced with a large number of possible regressor variables, even though we suspect that a majority of the regressors
have no true relationship to the variable Y . In these situations, including all
of the possible variables in a regression model can lead to poor statistical
performance. Standard statistical advice is that we should include in our regression model only those variables for which there is substantial evidence
of an association with y. Doing so not only produces simpler, more aesthetically pleasing data analyses, but also generally provides models with better
statistical properties in terms of prediction and estimation.

9.3 Model selection

161

Example: Diabetes
Baseline data for ten variables x1 , . . . , x10 on a group of 442 diabetes patients
were gathered, as well as a measure y of disease progression taken one year after the baseline measurements. From these data we hope to make a predictive
model for y based on the baseline measurements. While a regression model
with ten variables would not be overwhelmingly complex, it is suspected that
the relationship between y and the xj ‚Äôs may not be linear, and that including second-order terms like x2j and xj xk in the regression model might aid
in prediction. The regressors therefore include ten main effects x1 , . . . , x10 ,
10
= 45 interactions of the form xj xk and nine quadratic terms x2j (one
2
of the regressors, x2 = sex, is binary, so x2 = x22 , making it unnecessary to
include x22 ). This gives a total of p = 64 regressors. To help with the interpretation of the parameters and to put the regressors on a common scale, all
of the variables have been centered and scaled so that y and the columns of
X all have mean zero and variance one.
In this section we will build predictive regression models for y based on
the 64 regressor variables. To evaluate the models, we will randomly divide
the 442 diabetes subjects into 342 training samples and 100 test samples,
providing us with a training dataset (y, X) and a test dataset (y test , Xtest ).
We will fit the regression model using the training data and then use the
ÀÜ The performance
ÀÜ test = Xtest Œ≤.
estimated regression coefficients to generate y
ÀÜ test . Let‚Äôs
of the predictive model can then be evaluated by comparing y test to y
begin by building a predictive model with ordinary least squares regression
with all 64 variables. The first panel of Figure 9.5 plots the true values of the
ÀÜ where Œ≤
ÀÜ
ÀÜ test = Xtest Œ≤,
100 test samples y test versus their predicted values y
was estimated using the 342 training samples. While there is clearly a positive
relationship between the true values and the predictions,
there is quite a bit of
P
1
error: The average squared predictive error is 100
(ytest,i ‚àí yÀÜtest,i )2 = 0.67,
whereas if wePjust predicted each test case to be zero, our predictive error
1
2
would be 100
ytest,i
= 0.97.

2

2

5

‚óè

‚óè

‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè ‚óè
‚óè ‚óè ‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè‚óè ‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè

‚àí0.5

0.5
y test

1.5

1
y^test
0

^
Œ≤ols
0

‚àí1

‚àí5

y^test
1
0
‚àí1

‚àí1.5

‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè

‚óè

0 10

30
50
regressor index

‚àí1.5

‚àí0.5

0.5
y test

1.5

Fig. 9.5. Predicted values and regression coefficients for the diabetes data.

162

9 Linear regression

The second panel of Figure 9.5 shows the estimated values of each of the 64
regression coefficients. Of note is that the majority of coefficients are estimated
to be quite small. Perhaps our predictions could be improved by removing from
the regression model those variables that show little evidence of being nonzero. By doing so, we hope to remove from the predictive model any regressors
that have spurious associations to Y (i.e. those associations specific only to
the training data), leaving only those regressors that would have associations
for any group of subjects (i.e. both the training and test data). One standard
way to assess the evidence that the true value of a regression coefficient Œ≤j is
not zero is with a t-statistic, which is obtained by dividing the OLS estimate
1/2
Œ≤ÀÜj by its standard error, so tj = Œ≤ÀÜj /[ÀÜ
œÉ 2 (XT X)‚àí1
. We might then consider
j,j ]
removing from the model those regressor variables with small absolute values
of tj . For example, consider the following procedure:
ÀÜ = (XT X)‚àí1 XT y and its t-statistics.
1. Obtain the estimator Œ≤
ols
2. If there are any regressors j such that |tj | < tcutoff ,
a) find the regressor jmin having the smallest value of |tj | and remove
column jmin from X.
b) return to step 1.
3. If |tj | > tcutoff for all variables j remaining in the model, then stop.
Such procedures, in which a potentially large set of regressors is reduced to
a smaller set, are called model selection procedures. The procedure defined in
steps 1, 2 and 3 above describes a type of backwards elimination procedure,
in which all regressors are initially included but then are iteratively removed
until the remaining regressors satisfy some criterion. A standard choice for
tcutoff is an upper quantile of a t or standard normal distribution. If we apply
the above procedure to the diabetes data with tcutoff = 1.65 (corresponding
roughly to a p-value of 0.10), then 44 of the 64 variables are eliminated,
leaving 20 variables in the regression model. The third plot of Figure 9.5
shows y test versus predicted values based on the reduced-model regression
coefficients. The plot indicates that the predicted values from this model are
more accurate than those
and indeed the average squared
P from the full model,
1
predictive error is 100
(ytest,i ‚àí yÀÜtest,i )2 = 0.53.
Backwards selection is not without its drawbacks, however. What sort of
model would this procedure produce if there were no association between Y
and any of the regressors? We can evaluate this by creating a new data vector
Àú by randomly permuting the values of y. Since in this case the value of xi has
y
Àú and the columns of X is zero.
no effect on yÀúi , the ‚Äútrue‚Äù association between y
However, the OLS regression model will still pick up spurious associations:
The first panel of Figure 9.6 shows the t-statistics for one randomly generated
Àú of y. Initially, only one regressor has a t-statistic greater than
permutation y
1.65, but as we sequentially remove the columns of X the estimated variance of
the remaining regressors decreases and their t-statistics increase in value. With
tcutoff = 1.65, the procedure arrives at a regression model with 18 regressors,
17 of which have t-statistics greater than 2 in absolute value, and four of which

9.3 Model selection

163

4
t‚àístatistic
0
2
‚àí2
‚àí4

‚àí4

‚àí2

t‚àístatistic
0
2

4

Àú was generated without regard to
have statistics greater than 3. Even though y
X, the backwards selection procedure erroneously suggests that many of the
regressors do have an association. Such misleading results are fairly common
with backwards elimination and other sequential model selection procedures
(Berk, 1978).

0

10

20 30 40 50
regressor index

60

0

10

20 30 40 50
regressor index

60

Fig. 9.6. t-statistics for the regression of yÀú on X, before and after backwards elimination.

9.3.1 Bayesian model comparison
The Bayesian solution to the model selection problem is conceptually straightforward: If we believe that many of the regression coefficients are potentially
equal to zero, then we simply come up with a prior distribution that reflects
this possibility. This can be accomplished by specifying that each regression
coefficient has some non-zero probability of being exactly zero. A convenient
way to represent this is to write the regression coefficient for variable j as
Œ≤j = zj √ó bj , where zj ‚àà {0, 1} and bj is some real number. With this parameterization, our regression equation becomes
yi = z1 b1 xi,1 + ¬∑ ¬∑ ¬∑ + zp bp xi,p + i .
The zj ‚Äôs indicate which regression coefficients are non-zero. For example, in
the oxygen uptake problem,

164

9 Linear regression

E[Y |x, b, z = (1, 0, 1, 0)] = b1 x1 + b3 x3
= b1 + b3 √ó age
E[Y |x, b, z = (1, 1, 0, 0)] = b1 x1 + b2 x2
= b1 + b2 √ó group
E[Y |x, b, z = (1, 1, 1, 0)] = b1 x1 + b2 x2 + b3 x3
= b1 + b2 √ó group + b3 √ó age.
Each value of z = (z1 , . . . , zp ) corresponds to a different model, or more specifically, a different collection of variables having non-zero regression coefficients.
For example, we say that the model with z = (1, 0, 1, 0) is a linear regression
model for y as a function of age. The model with z = (1, 1, 1, 0) is referred
to as a regression model for y as a function of age, but with a group-specific
intercept. With this parameterization, choosing which variables to include in
a regression model is equivalent to choosing which zj ‚Äôs are 0 and which are 1.
Bayesian model selection proceeds by obtaining a posterior distribution
for z. Of course, doing so requires a joint prior distribution on {z, Œ≤, œÉ 2 }. It
turns out that a version of the g-prior described in the previous section allows
us to evaluate p(y|X, z) for each possible model z. Given a prior distribution
p(z) over models, this allows us to compute a posterior probability for each
regression model:
p(z)p(y|X, z)
p(z|y, X) = P
.
Àú)
z )p(y|X, z
zÀú p(Àú
Alternatively, we can compare the evidence for any two models with the posterior odds:
p(z a |y, X)
p(z a )
p(y|X, z a )
=
√ó
p(z b |y, X)
p(z b )
p(y|X, z b )
posterior odds = prior odds √ó ‚ÄúBayes factor‚Äù

odds(z a , z b |y, X) =

The Bayes factor can be interpreted as how much the data favor model z a
over model z b . In order to obtain a posterior distribution over models, we will
have to compute p(y|X, z) for each model z under consideration.
Computing the marginal probability
The marginal probability is obtained from the integral
Z Z
p(y|X, z) =
p(y, Œ≤, œÉ 2 |X, z) dŒ≤dœÉ 2
Z Z
=
p(y|Œ≤, X)p(Œ≤|X, z, œÉ 2 )p(œÉ 2 ) dŒ≤ dœÉ 2 .

(9.8)

Using a version of the g-prior distribution for Œ≤, we will be able to compute
this integral without needing much calculus. For any given z with pz non-zero
entries, let Xz be the n √ó pz matrix corresponding to the variables j for which

9.3 Model selection

165

zj = 1, and similarly let Œ≤ z be the pz √ó 1 vector consisting of the entries of
Œ≤ for which zj = 1. Our modified g-prior distribution for Œ≤ is that Œ≤j = 0 for
j‚Äôs such that zj = 0, and that
{Œ≤ z |Xz , œÉ 2 } ‚àº multivariate normal (0, gœÉ 2 [Xz T Xz ]‚àí1 ).
If we integrate (9.8) with respect to Œ≤ first, we have

Z Z
2
2
p(y|X, z) =
p(y|X, z, œÉ , Œ≤)p(Œ≤|X, z, œÉ ) dŒ≤ p(œÉ 2 ) dœÉ 2
Z
= p(y|X, z, œÉ 2 )p(œÉ 2 ) dœÉ 2 .
The form for the marginal probability p(y|X, z, œÉ 2 ) was computed in the last
section. Using those results, writing Œ≥ = 1/œÉ 2 and letting p(Œ≥) be the gamma
density with parameters (ŒΩ0 /2, ŒΩ0 œÉ02 /2), we can show that conditional density
of (y, Œ≥) given (X, z) is
h
i
z
p(y|X, z, Œ≥) √ó p(Œ≥) = (2œÄ)‚àín/2 (1 + g)‚àípz /2 √ó Œ≥ n/2 e‚àíŒ≥SSRg /2 √ó
h
i
2
(ŒΩ0 œÉ02 /2)ŒΩ0 /2 Œì (ŒΩ0 /2)‚àí1 Œ≥ ŒΩ0 /2‚àí1 e‚àíŒ≥ŒΩ0 œÉ0 /2 , (9.9)
where SSRzg is as in the last section except based on the regressor matrix Xz :
SSRzg = y T (I ‚àí

g
Xz (XTz Xz )‚àí1 Xz )y .
g+1

The part of Equation 9.9 that depends on Œ≥ is proportional to a gamma
density, but in this case the normalizing constant is the part that we need:
Œ≥ (ŒΩ0 +n)/2‚àí1 exp[‚àíŒ≥ √ó (ŒΩ0 œÉ02 + SSRzg )/2] =
Œì ([ŒΩ0 + n]/2)
√ó dgamma[Œ≥, (ŒΩ0 + n)/2, (ŒΩ0 œÉ02 + SSRzg )/2] .
([ŒΩ0 œÉ02 + SSRzg ]/2)(ŒΩ0 +n)/2‚àí1
Since the gamma density integrates to 1, the integral of the left-hand side of
the above equation must be equal to the constant on the right-hand side. Multiplying this constant by the other terms in Equation 9.9 gives the marginal
probability we are interested in:
p(y|X, z) = œÄ ‚àín/2

Œì ([ŒΩ0 + n]/2)
(ŒΩ0 œÉ02 )ŒΩ0 /2
(1 + g)‚àípz /2
.
Œì (ŒΩ0 /2)
(ŒΩ0 œÉ02 + SSRzg )(ŒΩ0 +n)/2

Now suppose we set g = n and use the unit information prior for p(œÉ 2 ) for each
model z, so that ŒΩ0 = 1 for all z, but œÉ02 is the estimated residual variance
under the least squares estimate for model z. In this case, the ratio of the
probabilities under any two models z a and z b is

166

9 Linear regression

p(y|X, z a )
= (1 + n)(pzb ‚àípza )/2
p(y|X, z b )



s2za
s2zb

1/2
√ó

s2zb + SSRzgb
s2za + SSRzga

!(n+1)/2
.

Notice that the ratio of the marginal probabilities is essentially a balance
between model complexity and goodness of fit: A large value of pzb compared
to pza penalizes model z b , although a large value of SSRzga compared to SSRzgb
penalizes model z a .
Oxygen uptake example
Recall our regression model for the oxygen uptake data:
E[Yi |Œ≤, xi ] = Œ≤1 xi,1 +
Œ≤2 xi,2
+ Œ≤3 xi,3 +
Œ≤4 xi,4
= Œ≤1 + Œ≤2 √ó groupi + Œ≤3 √ó agei + Œ≤4 √ó groupi √ó agei .
The question of whether or not there is an effect of group translates into the
question of whether or not Œ≤2 and Œ≤4 are non-zero. Recall from our analyses
in the previous sections that the estimated magnitudes of Œ≤2 and Œ≤4 were
not large compared to their standard deviations, suggesting that maybe there
is not an effect of group. However, we also noticed from the joint posterior
distribution that Œ≤2 and Œ≤4 were negatively correlated, so whether or not Œ≤2
is zero affects our information about Œ≤4 .
z
(1,0,0,0)
(1,1,0,0)
(1,0,1,0)
(1,1,1,0)
(1,1,1,1)

model
log p(y|X, z) p(z|y, X)
Œ≤1
-44.33
0.00
Œ≤1 + Œ≤2 √ó groupi
-42.35
0.00
Œ≤1 + Œ≤3 √ó agei
-37.66
0.18
Œ≤1 + Œ≤2 √ó groupi + Œ≤3 √ó agei
-36.42
0.63
Œ≤1 + Œ≤2 √ó groupi + Œ≤3 √ó agei + Œ≤4 √ó groupi √ó agei
-37.60
0.19

Table 9.1. Marginal probabilities of the data under five different models.

We can formally evaluate whether Œ≤2 or Œ≤4 should be zero by computing the probability of the data under a variety of competing models. Table
9.1 lists five different regression models that we might like to consider for
these data. Using the g-prior for Œ≤ with g = n, and a unit information prior
distribution for œÉ 2 for each value of z, the values of log p(y|X, z) can be computed for each of the five values of z we are considering. If we give each of
these models equal prior weight, then posterior probabilities for each model
can be computed as well. These calculations indicate that, among these five
models, the most probable model is the one corresponding z = (1, 1, 1, 0),
having a slope for age with a separate intercept for each group. The evidence
for an age effect is strong, as the posterior probabilities of the three models
that include age essentially sum to 1. The evidence for an effect of group is

9.3 Model selection

167

weaker, as the combined probability for the three models with a group effect is
0.00+0.63+0.19=0.82. However, this probability is substantially higher than
the corresponding prior probability of 0.20+0.20+0.20=0.60 for these three
models.
9.3.2 Gibbs sampling and model averaging
If we allow each of the p regression coefficients to be either zero or non-zero,
then there are 2p different models to consider. If p is large, then it will be
impractical for us to compute the marginal probability of each model. The
diabetes data, for example, has p = 64 possible regressors, so the total number
of models is 264 ‚âà 1.8√ó1019 . In these situations our data analysis goals become
more modest: For example, we may be content with a decent estimate of Œ≤
from which we can make predictions, or a list of relatively high-probability
models. These items can be obtained with a Markov chain which searches
through the space of models for values of z with high posterior probability.
This can be done with a Gibbs sampler in which we iteratively sample each zj
from its full conditional distribution. Specifically, given a current value z =
(z1 , . . . , zp ), a new value of zj is generated by sampling from p(zj |y, X, z ‚àíj ),
where z ‚àíj refers to the values of z except the one corresponding to regressor
j. The full conditional probability that zj is 1 can be written as oj /(1 + oj ),
where oj is the conditional odds that zj is 1, given by
oj =

Pr(zj = 1|y, X, z ‚àíj )
Pr(zj = 1) p(y|X, z ‚àíj , zj = 1)
=
√ó
.
Pr(zj = 0|y, X, z ‚àíj )
Pr(zj = 0) p(y|X, z ‚àíj , zj = 0)

We may also want to obtain posterior samples of Œ≤ and œÉ 2 . Using the results
of Section 9.2, values of these parameters can be sampled directly from their
conditional distributions given z, y and X: For each z in our MCMC sample,
we can construct the matrix Xz which consists of only those columns j corresponding to non-zero values of zj . Using this matrix of regressors, a value
of œÉ 2 can be sampled from p(œÉ 2 |X, y, z) (an inverse-gamma distribution) and
then a value of Œ≤ can be sampled from p(Œ≤|X, y, z, œÉ 2 ) (a multivariate normal
distribution). Our Gibbs sampling scheme therefore looks something like the
following:
Œ≤ (s)
z (s)
œÉ 2(s)
(s+1)
z (s+1) œÉ 2(s+1) Œ≤
(s+1)
More precisely, generating values of {z
, œÉ (s+1) , Œ≤ (s+1) } from z (s) is
achieved with the following steps:

1. Set z = z (s) ;
2. For j ‚àà {1, . . . , p} in random order, replace zj with a sample from
p(zj |z ‚àíj , y, X);
3. Set z (s+1) = z;

168

9 Linear regression

4. Sample œÉ 2(s+1) ‚àº p(œÉ 2 |z (s+1) , y, X);
5. Sample Œ≤ (s+1) ‚àº p(Œ≤|z (s+1) , œÉ 2(s+1) , y, X).
Note that the entries of z (s+1) are not sampled from their full conditional distributions given œÉ 2(s) and Œ≤ (s) . This is not a problem: The Gibbs sampler for z
ensures that the distribution of z (s) converges to the target posterior distribution p(z|y, X). Since (œÉ 2(s) , Œ≤ (s) ) are direct samples from p(œÉ 2 , Œ≤|z (s) , y, X),
the distribution of (œÉ 2(s) , Œ≤ (s) ) converges to p(œÉ 2 , Œ≤|y, X). R-code to implement the Gibbs sampling algorithm in z, along with a function lpy.X that
calculates the log of p(y|X), is below. This code can be combined with the
code in the previous section in order to generate samples of {z, œÉ 2 , Œ≤} from
the joint posterior distribution.
##### a f u n c t i o n t o compute t h e m a r g i n a l p r o b a b i l i t y
l p y . X<‚àíf u n c t i o n ( y , X, g=l e n g t h ( y ) ,
nu0=1 , s 2 0=t r y ( summary ( lm ( yÀú‚àí1+X) ) $sigma ÀÜ 2 , s i l e n t=TRUE) )
{
n<‚àídim (X ) [ 1 ] ; p<‚àídim (X ) [ 2 ]
i f ( p==0) { Hg<‚àí0 ; s20<‚àímean ( y ÀÜ 2 ) }
i f ( p>0) { Hg<‚àí(g / ( g +1)) ‚àó X%‚àó%s o l v e ( t (X)%‚àó%X)%‚àó%t (X) }
SSRg<‚àí t ( y)%‚àó%( d i a g ( 1 , nrow=n ) ‚àí Hg )%‚àó%y
‚àí.5‚àó( n‚àó l o g ( p i )+p‚àó l o g (1+g )+( nu0+n ) ‚àó l o g ( nu0‚àó s 2 0+SSRg)‚àí
nu0‚àó l o g ( nu0 ‚àó s 2 0 ) ) +
lgamma ( ( nu0+n ) / 2 ) ‚àí lgamma ( nu0 / 2 )
}
#####
##### s t a r t i n g v a l u e s and MCMC s e t u p
z<‚àír e p ( 1 , dim (X ) [ 2 ] )
l p y . c<‚àíl p y .X( y ,X[ , z==1,drop=FALSE ] )
S<‚àí10000
Z<‚àím at r i x (NA, S , dim (X ) [ 2 ] )
#####
##### Gibbs s a m p l e r
for ( s in 1: S)
{
f o r ( j i n sample ( 1 : dim (X ) [ 2 ] ) )
{
zp<‚àíz ; zp [ j ]<‚àí1‚àízp [ j ]
l p y . p<‚àíl p y .X( y ,X[ , zp==1,drop=FALSE ] )
r<‚àí ( l p y . p ‚àí l p y . c )‚àó( ‚àí1)ÀÜ ( zp [ j ]==0)
z [ j ]<‚àírbinom ( 1 , 1 , 1 / ( 1 + exp(‚àí r ) ) )
i f ( z [ j ]==zp [ j ] ) { l p y . c<‚àíl p y . p}
}
Z [ s ,]<‚àí z
}
#####

9.3 Model selection

169

Diabetes example
Using a uniform distribution on z, the Gibbs sampling scheme described above
was run for S = 10, 000 iterations, generating 10,000 samples of (z, œÉ 2 , Œ≤)
which we can use to approximate the posterior distribution p(z, œÉ 2 , Œ≤|y, X).
How good will our approximation be? Recall that with p = 64 the total number of models, or possible values of z, is 264 ‚âà 1019 , which is 1015 times as
large as the number of approximate posterior samples we have. It should then
not be too much of a surprise that, in the 10,000 MCMC samples of z, only
32 of the possible models were sampled more than once: 28 models were sampled twice, two were sampled three times and two others were sampled five
and six times. This means that, for large p, the Gibbs sampling scheme provides a poor approximation to the posterior distribution of z. Nevertheless, in
many situations where most of the regressors have no effect the Gibbs sampler
can still provide reasonable estimates of the marginal posterior distributions
of individual zj ‚Äôs or Œ≤j ‚Äôs. The first panel of Figure 9.7 shows the estimated
posterior probabilities Pr(zj = 1|y, X) for each of the 64 regressors. There
are six regression coefficients having a posterior probability higher than 0.5
of being non-zero. These six regressors are a subset of the 20 that remained
after the backwards selection procedure described above. How well does this
Bayesian approach do in terms of prediction?
As usual, we can approximate
PS
(s)
ÀÜ
the posterior mean of Œ≤ with Œ≤
=
Œ≤
/S. This parameter estimate
bma
s=1
is sometimes called the (Bayesian) model averaged estimate of Œ≤, because it
is an average of regression parameters from different values of z, i.e. over different regression models. This estimate, obtained by averaging the regression
coefficients from several high-probability models, often performs better than
the estimate of Œ≤ obtained by considering only a single model. Returning to
the problem of predicting data from the diabetes test set, we can compute
ÀÜ
ÀÜ test = XŒ≤
model-averaged predicted values y
bma . These predicted values are
plotted against the true values y test in the second panel of Figure 9.7. These
predictions have an average squared error of 0.452, which is better than the
OLS estimates using either the full model or the one obtained from backwards
elimination.
Finally, we evaluate the Bayesian model selection procedure when there is
no relationship between Y and x. Recall from above that when the backwards
Àú , which was conelimination procedure was applied to the permuted vector y
structed independently of X, it erroneously returned 18 regressors. Running
the Gibbs sampler above on the same dataset (Àú
y , X) for 10,000 iterations proP (s)
vides approximated posterior probabilities Pr(zj = 1|y, X) = zj /S, all of
which are less than 1/2, and all but two of which are less than 1/4. In contrast
to the backwards selection procedure, for these data the Bayesian approach
to model selection does not erroneously identify any regressors as having an
Àú.
effect on the distribution of y

9 Linear regression

1.0

170

‚óè

‚óè

‚àí1.0

0.2

y^test
0.0

1.0

Pr(z j = 1|y,X)
0.4 0.6 0.8

‚óè

0

10

20

30
40
regressor index

50

60

‚óè

‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè ‚óè ‚óè‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè ‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè

‚àí1.5

‚àí0.5

0.5
y test

1.5

Fig. 9.7. The first panel shows posterior probabilities that each coefficient is nonzero. The second panel shows ytest versus predictions based on the model averaged
estimate of Œ≤.

9.4 Discussion and further references
There are many approaches to Bayesian model selection that use prior distributions allowing elements of Œ≤ to be identically zero. George and McCulloch
(1993) parameterize Œ≤j as bj √ó zj , where zj ‚àà {0, 1}, and use Gibbs sampling
to do model selection. Liang et al (2008) review various types of g-priors in
terms of two types of asymptotic consistency: model consistency and predictive consistency. The former is concerned with selecting the ‚Äútrue model,‚Äù
and the latter with making accurate posterior predictions. As pointed out by
Leamer (1978), selecting a model and then acting as if it were true understates
the uncertainty in the model selection process, and can result in suboptimal
predictive performance. Predictive performance can be improved by Bayesian
model averaging, i.e. averaging the predictive distributions under the different
models according to their posterior probability (Madigan and Raftery, 1994;
Raftery et al, 1997).
Many have argued that in most situations none of the regression models
under consideration are actually true. Results of Bernardo and Smith (1994,
Section 6.1.6) and Key et al (1999) indicate that in this situation, Bayesian
model selection can still be meaningful in a decision-theoretic sense, where
the task is to select the model with the best predictive performance. In this
case, model selection proceeds using a modified Bayes factor that is similar to
a cross-validation criterion.

10
Nonconjugate priors and Metropolis-Hastings
algorithms

When conjugate or semiconjugate prior distributions are used, the posterior
distribution can be approximated with the Monte Carlo method or the Gibbs
sampler. In situations where a conjugate prior distribution is unavailable or
undesirable, the full conditional distributions of the parameters do not have
a standard form and the Gibbs sampler cannot be easily used. In this section
we present the Metropolis-Hastings algorithm as a generic method of approximating the posterior distribution corresponding to any combination of prior
distribution and sampling model. This section presents the algorithm in the
context of two examples: The first involves Poisson regression, which is a type
of generalized linear model. The second is a longitudinal regression model in
which the observations are correlated over time.

10.1 Generalized linear models
Example: Song sparrow reproductive success
A sample from a population of 52 female song sparrows was studied over
the course of a summer and their reproductive activities were recorded. In
particular, the age and number of new offspring were recorded for each sparrow
(Arcese et al, 1992). Figure 10.1 shows boxplots of the number of offspring
versus age. The figure indicates that two-year-old birds in this population
had the highest median reproductive success, with the number of offspring
declining beyond two years of age. This is not surprising from a biological
point of view: One-year-old birds are in their first mating season and are
relatively inexperienced compared to two-year-old birds. As birds age beyond
two years they experience a general decline in health and activity.
Suppose we wish to fit a probability model to these data, perhaps to understand the relationship between age and reproductive success, or to make
population forecasts for this group of birds. Since the number of offspring for
each bird is a non-negative integer {0,1,2,. . . }, a simple probability model
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 10,
c Springer Science+Business Media, LLC 2009


10 Nonconjugate priors and Metropolis-Hastings algorithms

0

1

2

offspring
3
4

5

6

7

172

1

2

3

4

5

6

age

Fig. 10.1. Number of offspring versus age.

for Y =number of offspring conditional on x=age would be a Poisson model,
{Y |x} ‚àº Poisson(Œ∏x ). One possibility would be to estimate Œ∏x separately for
each age group. However, the number of birds of each age is small and so the
estimates of Œ∏x would be imprecise. To add stability to the estimation we will
assume that the mean number of offspring is a smooth function of age. We
will want to allow this function to be quadratic so that we can represent the
increase in mean offspring while birds mature and the decline they experience
thereafter. One possibility would be to express Œ∏x as Œ∏x = Œ≤1 + Œ≤2 x + Œ≤3 x2 .
However, such a parameterization might allow some values of Œ∏x to be negative, which is not physically possible. As an alternative, we will model the
log-mean of Y in terms of this regression, so that
log E[Y |x] = log Œ∏x = Œ≤1 + Œ≤2 x + Œ≤3 x2 ,
which means that E[Y |x] = exp(Œ≤1 + Œ≤2 x + Œ≤3 x2 ), which is always greater
than zero.
The resulting model, {Y |x} ‚àº Poisson(exp[Œ≤ T x]), is called a Poisson regression model . The term Œ≤ T x is called the linear predictor . In this regression model the linear predictor is linked to E[Y |x] via the log function, and
so we say that this model has a log link . The Poisson regression model is
a type of generalized linear model , a model which relates a function of the
expectation to a linear predictor of the form Œ≤ T x. Another common generalized linear model is the logistic regression model for binary data. Writing
Pr(Y = 1|x) = E[Y |x] = Œ∏x , the logistic regression model parameterizes Œ∏x as
exp(Œ≤ T x)
, so that
1 + exp(Œ≤ T x)
Œ∏x
Œ≤ T x = log
.
1 ‚àí Œ∏x
Œ∏x =

10.2 The Metropolis algorithm

173

The function log Œ∏x /(1 ‚àí Œ∏x ) relating the mean to the linear predictor is called
the logit function, so the logistic regression model could be described as a
binary regression model with a logit link. Notice that the logit link forces Œ∏x
to be between zero and one, even though Œ≤ T x can range over the whole real
line.
As in the case of ordinary regression, a natural class of prior distributions
for Œ≤ is the class of multivariate normal distributions. However, for neither the
Poisson nor the logistic regression model would a prior distribution from this
class result in a multivariate normal posterior distribution for Œ≤. Furthermore,
standard conjugate prior distributions for generalized linear models do not
exist (except for the normal regression model).
One possible way to calculate the posterior distribution is to use a gridbased approximation, similar to the approach we used in Section 6.2: We
can evaluate p(y|X, Œ≤) √ó p(Œ≤) on a three-dimensional grid of Œ≤-values, then
normalize the result to obtain a discrete approximation to p(Œ≤|X, y). Figure
10.2 shows approximate marginal and joint distributions of Œ≤2 and Œ≤3 based on
the prior distribution Œ≤ ‚àº multivariate normal(0, 100 √ó I) and a grid having
100 values for each parameter. Computing these quantities for this threeparameter model required the calculation of p(y|X, Œ≤) √ó p(Œ≤) at 1 million
grid points. While feasible for this problem, a Poisson regression with only
two more regressors and the same grid density would require 10 billion grid
points, which is prohibitively large. Additionally, grid-based approximations
can be very inefficient: The third panel of Figure 10.2 shows a strong negative
posterior correlation between Œ≤2 and Œ≤3 , which means that the probability
mass is concentrated along a diagonal and so the vast majority of points of our
cubical grid have essentially zero probability. In contrast, an approximation of
p(Œ≤|X, y) based on Monte Carlo samples could be stored in a computer much
more efficiently, since our Monte Carlo sample would not include any points
that have essentially zero posterior probability. Although independent Monte
Carlo sampling from the posterior is not available for this Poisson regression
model, the next section will show how to construct a Markov chain that can
approximate p(Œ≤|X, y) for any prior distribution p(Œ≤).

10.2 The Metropolis algorithm
Let‚Äôs consider a very generic situation where we have a sampling model
Y ‚àº p(y|Œ∏) and a prior distribution p(Œ∏). Although in most problems
p(y|Œ∏) and Rp(Œ∏) can be calculated for any values of y and Œ∏, p(Œ∏|y) =
p(Œ∏)p(y|Œ∏)/ p(Œ∏0 )p(y|Œ∏0 ) dŒ∏0 is often hard to calculate due to the integral
in the denominator. If we were able to sample from p(Œ∏|y), then we could
generate Œ∏(1) , . . . , Œ∏(S) ‚àº i.i.d. p(Œ∏|y) and obtain Monte Carlo approximations
to posterior quantities, such as

10 Nonconjugate priors and Metropolis-Hastings algorithms

Œ≤3
‚àí0.2
‚àí0.4

0

0.0

1

2

p( Œ≤2|y)
0.4
0.8

p( Œ≤3|y)
3 4 5

6

0.0

7

1.2

174

‚àí1

0

Œ≤2

1

2

‚àí0.6

‚àí0.2
Œ≤3

0.2 0.4

‚àí0.5

0.5

Œ≤2

1.5

Fig. 10.2. Grid-based approximations to p(Œ≤2 |X, y), p(Œ≤3 |X, y) and p(Œ≤2 , Œ≤3 |X, y).

E[g(Œ∏)|y] ‚âà

S
1X
g(Œ∏(s) ).
S s=1

But what if we cannot sample directly from p(Œ∏|y)? In terms of approximating the posterior distribution, the critical thing is not that we have i.i.d.
samples from p(Œ∏|y) but rather that we are able to construct a large collection
of Œ∏-values, {Œ∏(1) , . . . , Œ∏(S) }, whose empirical distribution approximates p(Œ∏|y).
Roughly speaking, for any two different values Œ∏a and Œ∏b we need
#{Œ∏(s) ‚Äôs in the collection = Œ∏a }
p(Œ∏a |y)
‚âà
.
(s)
p(Œ∏b |y)
#{Œ∏ ‚Äôs in the collection = Œ∏b }
Let‚Äôs think intuitively about how we might construct such a collection.
Suppose we have a working collection {Œ∏(1) , . . . , Œ∏(s) } to which we would like
to add a new value Œ∏(s+1) . Let‚Äôs consider adding a value Œ∏‚àó which is nearby
Œ∏(s) . Should we include Œ∏‚àó in the set or not? If p(Œ∏‚àó |y) > p(Œ∏(s) |y) then we
want more Œ∏‚àó ‚Äôs in the set than Œ∏(s) ‚Äôs. Since Œ∏(s) is already in the set, then it
seems we should include Œ∏‚àó as well. On the other hand, if p(Œ∏‚àó |y) < p(Œ∏(s) |y)
then it seems we should not necessarily include Œ∏‚àó . So perhaps our decision
to include Œ∏‚àó or not should be based on a comparison of p(Œ∏‚àó |y) to p(Œ∏(s) |y).
Fortunately, this comparison can be made even if we cannot compute p(Œ∏|y):
r=

p(Œ∏‚àó |y)
p(y|Œ∏‚àó )p(Œ∏‚àó )
p(y)
p(y|Œ∏‚àó )p(Œ∏‚àó )
=
=
.
p(y)
p(Œ∏(s) |y)
p(y|Œ∏(s) )p(Œ∏(s) )
p(y|Œ∏(s) )p(Œ∏(s) )

(10.1)

Having computed r, how should we proceed?
If r > 1:
Intuition: Since Œ∏(s) is already in our set, we should include Œ∏‚àó as it
has a higher probability than Œ∏(s) .
Procedure: Accept Œ∏‚àó into our set, i.e. set Œ∏(s+1) = Œ∏‚àó .
If r < 1:

10.2 The Metropolis algorithm

175

Intuition: The relative frequency of Œ∏-values in our set equal to Œ∏‚àó
compared to those equal to Œ∏(s) should be p(Œ∏‚àó |y)/p(Œ∏(s) |y) = r. This
means that for every instance of Œ∏(s) , we should have only a ‚Äúfraction‚Äù
of an instance of a Œ∏‚àó value.
Procedure: Set Œ∏(s+1) equal to either Œ∏‚àó or Œ∏(s) , with probability r
and 1 ‚àí r respectively.
This is the basic intuition behind the famous Metropolis algorithm. The
Metropolis algorithm proceeds by sampling a proposal value Œ∏‚àó nearby the
current value Œ∏(s) using a symmetric proposal distribution J(Œ∏‚àó |Œ∏(s) ). Symmetric here means that J(Œ∏b |Œ∏a ) = J(Œ∏a |Œ∏b ), i.e. the probability of proposing
Œ∏‚àó = Œ∏b given that Œ∏(s) = Œ∏a is equal to the probability of proposing Œ∏‚àó = Œ∏a
given that Œ∏(s) = Œ∏b . Usually J(Œ∏‚àó |Œ∏(s) ) is very simple, with samples from
J(Œ∏‚àó |Œ∏(s) ) being near Œ∏(s) with high probability. Examples include
‚Ä¢ J(Œ∏‚àó |Œ∏(s) ) = uniform(Œ∏(s) ‚àí Œ¥, Œ∏(s) + Œ¥) ;
‚Ä¢ J(Œ∏‚àó |Œ∏(s) ) = normal(Œ∏(s) , Œ¥ 2 ) .
The value of the parameter Œ¥ is generally chosen to make the approximation
algorithm run efficiently, as will be discussed in more detail shortly.
Having obtained a proposal value Œ∏‚àó , we add either it or a copy of Œ∏(s) to
our set, depending on the ratio r = p(Œ∏‚àó |y)/p(Œ∏(s) |y). Specifically, given Œ∏(s) ,
the Metropolis algorithm generates a value Œ∏(s+1) as follows:
1. Sample Œ∏‚àó ‚àº J(Œ∏|Œ∏(s) );
2. Compute the acceptance ratio
r=
3. Let
Œ∏(s+1) =



p(Œ∏‚àó |y)
p(y|Œ∏‚àó )p(Œ∏‚àó )
=
.
(s)
p(Œ∏ |y)
p(y|Œ∏(s) )p(Œ∏(s) )
Œ∏‚àó with probability min(r, 1)
Œ∏(s) with probability 1 ‚àí min(r, 1).

Step 3 can be accomplished by sampling u ‚àº uniform(0, 1) and setting
Œ∏(s+1) = Œ∏‚àó if u < r and setting Œ∏(s+1) = Œ∏(s) otherwise.
Example: Normal distribution with known variance
Let‚Äôs try out the Metropolis algorithm for the conjugate normal model with
a known variance, a situation where we know the correct posterior distribution. Letting Œ∏ ‚àº normal(¬µ, œÑ 2 ) and {y1 , . . . , yn |Œ∏} ‚àº i.i.d. normal(Œ∏, œÉ 2 ), the
posterior distribution of Œ∏ is normal(¬µn , œÑn2 ) where
¬µn = y¬Ø

n/œÉ 2
1/œÑ 2
+
¬µ
n/œÉ 2 + 1/œÑ 2
n/œÉ 2 + 1/œÑ 2

œÑn2 = 1/(n/œÉ 2 + 1/œÑ 2 ).

176

10 Nonconjugate priors and Metropolis-Hastings algorithms

Suppose œÉ 2 = 1, œÑ 2 = 10, ¬µ = 5, n = 5 and y = (9.37, 10.18, 9.16, 11.60, 10.33).
For these data, ¬µn = 10.03 and œÑn2 = .20, and so p(Œ∏|y) = dnorm(10.03, .44).
Now suppose that for some reason we were unable to obtain the formula for
this posterior distribution and needed to use the Metropolis algorithm to approximate it. Based on this model and prior distribution, the acceptance ratio
comparing a proposed value Œ∏‚àó to a current value Œ∏(s) is
 Qn
 

‚àó
p(Œ∏‚àó |y)
dnorm(Œ∏‚àó , ¬µ, œÑ )
i=1 dnorm(yi , Œ∏ , œÉ)
Q
r=
=
√ó
.
n
(s) , œÉ)
p(Œ∏(s) |y)
dnorm(Œ∏(s) , ¬µ, œÑ )
i=1 dnorm(yi , Œ∏
In many cases, computing the ratio r directly can be numerically unstable, a
problem that often can be remedied by computing the logarithm of r:
log r =

n
X

[log dnorm(yi , Œ∏‚àó , œÉ) ‚àí log dnorm(yi , Œ∏(s) , œÉ)] +

i=1

log dnorm(Œ∏‚àó , ¬µ, œÑ ) ‚àí log dnorm(Œ∏(s) , ¬µ, œÑ ).
Keeping things on the log scale, the proposal is accepted if log u < log r, where
u is a sample from the uniform distribution on (0, 1).
The R-code below generates 10,000 iterations of the Metropolis algorithm,
starting at Œ∏(0) = 0 and using a normal proposal distribution, Œ∏(s+1) ‚àº normal
(Œ∏(s) , Œ¥ 2 ) with Œ¥ 2 = 2 .
s2 <‚àí1 ; t2 <‚àí10 ; mu<‚àí5
y<‚àíc ( 9 . 3 7 , 1 0 . 1 8 , 9 . 1 6 , 1 1 . 6 0 , 1 0 . 3 3 )
t h e t a <‚àí0 ; d e l t a 2 <‚àí2 ; S<‚àí10000 ; THETA<‚àíNULL ; s e t . s e e d ( 1 )
for ( s in 1: S)
{
t h e t a . s t a r <‚àírnorm ( 1 , t h e t a , s q r t ( d e l t a 2 ) )
l o g . r <‚àí( sum ( dnorm ( y , t h e t a . s t a r , s q r t ( s 2 ) , l o g=TRUE) ) +
dnorm ( t h e t a . s t a r , mu, s q r t ( t 2 ) , l o g=TRUE) )
( sum ( dnorm ( y , t h e t a , s q r t ( s 2 ) , l o g=TRUE) ) +
dnorm ( t h e t a , mu, s q r t ( t 2 ) , l o g=TRUE) )

‚àí

i f ( l o g ( r u n i f (1)) < l o g . r ) { t h e t a <‚àít h e t a . s t a r }
THETA<‚àíc (THETA, t h e t a )
}

The first panel of Figure 10.3 plots these 10,000 simulated values as a
function of iteration number. Although the value of Œ∏ starts nowhere near
the posterior mean of 10.03, it quickly arrives there after a few iterations. The
second panel gives a histogram of the 10,000 Œ∏-values, and includes a plot of the
normal(10.03, 0.20) density for comparison. Clearly the empirical distribution

177

0.0

5

6

0.2

7

Œ∏
8

9

density
0.4
0.6

10

0.8

11

10.2 The Metropolis algorithm

0

2000

4000 6000
iteration

8000

8.5 9.0 9.5

10.5

11.5

Œ∏

Fig. 10.3. Results from the Metropolis algorithm for the normal model.

of the simulated values is very close to the true posterior distribution. Will
this similarity between {Œ∏(1) , . . . , Œ∏(S) } and p(Œ∏|y) hold in general?
Output of the Metropolis algorithm
The Metropolis algorithm generates a dependent sequence {Œ∏(1) , Œ∏(2) , . . .} of
Œ∏-values. Since our procedure for generating Œ∏(s+1) depends only on Œ∏(s) , the
conditional distribution of Œ∏(s+1) given {Œ∏(1) , . . . , Œ∏(s) } also depends only on
Œ∏(s) and so the sequence {Œ∏(1) , Œ∏(2) , . . .} is a Markov chain.
Under some mild conditions the marginal sampling distribution of Œ∏(s) is
approximately p(Œ∏|y) for large s. Additionally, for any given numerical value
Œ∏a of Œ∏,
#{Œ∏‚Äôs in the sequence < Œ∏a }
lim
= p(Œ∏ < Œ∏a |y).
S‚Üí‚àû
S
Just as with the Gibbs sampler, this suggests we can approximate posterior
means, quantiles and other posterior quantities of interest using the empirical
distribution of {Œ∏(1) , . . . , Œ∏(S) }. However, our approximation to these quantities will depend on how well our simulated sequence actually approximates
p(Œ∏|y). Results from probability theory say that, in the limit as S ‚Üí ‚àû, the
approximation will be exact, but in practice we cannot run the Markov chain
forever. Instead, the standard practice in MCMC approximation, using either
the Metropolis algorithm or the Gibbs sampler, is as follows:
1. run algorithm until some iteration B for which it looks like the Markov
chain has achieved stationarity;
2. run the algorithm S more times, generating {Œ∏(B+1) , . . . , Œ∏(B+S) };
3. discard {Œ∏(1) , . . . , Œ∏(B) } and use the empirical distribution of {Œ∏(B+1) , . . .,
Œ∏(B+S) } to approximate p(Œ∏|y).

178

10 Nonconjugate priors and Metropolis-Hastings algorithms

15
10

15
10
0

100

300
iteration

500

5
0

5
0

0

5

Œ∏

10

15

The iterations up to and including B are called the ‚Äúburn-in‚Äù period, in which
the Markov chain moves from its initial value to a region of the parameter
space that has high posterior probability. If we have a good idea of where this
high probability region is, we can reduce the burn-in period by starting the
Markov chain there. For example, in the Metropolis algorithm above it would
have been better to start with Œ∏(1) = y¬Ø as we know that the posterior mode
will be near y¬Ø. However, starting with Œ∏(1) = 0 illustrates that the Metropolis
algorithm is able to move from a low posterior probability region to one of
high probability.
The Œ∏-values generated from an MCMC algorithm are statistically dependent. Recall from the discussion of MCMC diagnostics in Chapter 6 that the
higher the correlation, the longer it will take for the Markov chain to achieve
stationarity and the more iterations it will take to get a good approximation to
p(Œ∏|y). Roughly speaking, the amount of information we obtain about E[Œ∏|y]
from S positively correlated samples is less than the information we would obtain from S independent samples. The more correlated our Markov chain is,
the less information we get per iteration (recall the notion of ‚Äúeffective sample
size‚Äù from Section 6.6). In Gibbs sampling we do not have much control over
the correlation of the Markov chain, but with the Metropolis algorithm the
correlation can be adjusted by selecting an optimal value of Œ¥ in the proposal
distribution. By selecting Œ¥ carefully, we can decrease the correlation in the
Markov chain, leading to an increase in the rate of convergence, an increase
in the effective sample size of the Markov chain and an improvement in the
Monte Carlo approximation to the posterior distribution.

0

100

300
iteration

500

0

100

300
iteration

500

Fig. 10.4. Markov chains under three different proposal distributions. Going from
left to right, the values of Œ¥ 2 are 1/32, 2 and 64 respectively.

To illustrate this, we can rerun the Metropolis algorithm for the one-sample
normal problem using a range of Œ¥ values, including Œ¥ 2 ‚àà {1/32, 1/2, 2, 32,
64 }. Doing so results in lag-1 autocorrelations of (0.98, 0.77, 0.69, 0.84, 0.86)
for these five different Œ¥-values. Interestingly, the best Œ¥-value among these
five occurs in the middle of the set of values, and not at the extremes. The

10.3 The Metropolis algorithm for Poisson regression

179

reason why can be understood by inspecting each sequence. Figure 10.4 plots
the first 500 values for the sequences corresponding to Œ¥ 2 ‚àà {1/32, 2, 64}. In
the first panel where Œ¥ 2 = 1/32, the small proposal variance means that Œ∏‚àó
will be very close to Œ∏(s) , and so r ‚âà 1 for most proposed values. As a result,
Œ∏‚àó is accepted as the value of Œ∏(s+1) for 87% of the iterations. Although this
high acceptance rate keeps the chain moving, the moves are never very large
and so the Markov chain is highly correlated. One consequence of this is that
it takes a large number of iterations for the Markov chain to move from the
starting value of zero to the posterior mode of 10.03. At the other extreme,
the third plot in the figure shows the Markov chain for Œ¥ 2 = 64. In this case
the chain moves quickly to the posterior mode but once there it gets ‚Äústuck‚Äù
for long periods. This is because the variance of the proposal distribution is so
large that Œ∏‚àó is frequently very far away from the posterior mode. Proposals
in this Metropolis algorithm are accepted for only 5% of the iterations, and
so Œ∏(s+1) is set equal to Œ∏(s) 95% of the time, resulting in a highly correlated
Markov chain.
In order to construct a Markov chain with a low correlation we need a
proposal variance that is large enough so that the Markov chain can quickly
move around the parameter space, but not so large that the proposals end up
getting rejected most of the time. Among the proposal variances considered
for the data and normal model here, this balance was optimized with a Œ¥ 2 of
2, which gives an acceptance rate of 35%. In general, it is common practice to
first select a proposal distribution by implementing several short runs of the
Metropolis algorithm under different Œ¥-values until one is found that gives an
acceptance rate roughly between 20 and 50%. Once a reasonable value of Œ¥ is
selected, a longer more efficient Markov chain can be run. Alternatively, modified versions of the Metropolis algorithm can be constructed that adaptively
change the value of Œ¥ at the beginning of the chain in order to automatically
find a good proposal distribution.

10.3 The Metropolis algorithm for Poisson regression
Let‚Äôs implement the Metropolis algorithm for the Poisson regression model
introduced at the beginning of the chapter. Recall that the model is that Yi is
a sample from a Poisson distribution with a log-mean given by log E[Yi |xi ] =
Œ≤1 + Œ≤2 xi + Œ≤3 x2i , where xi is the age of the sparrow i. We will abuse notation
slightly by writing xi = (1, xi , x2i ) so that log E[Yi |xi ] = Œ≤ T xi . The prior
distribution we used in Section 10.1 was that the regression coefficients were
i.i.d. normal(0,100). Given a current value Œ≤ (s) and a value Œ≤ ‚àó generated from
J(Œ≤ ‚àó |Œ≤ (s) ), the acceptance ratio for the Metropolis algorithm is

180

10 Nonconjugate priors and Metropolis-Hastings algorithms

r=

p(Œ≤ ‚àó |X, y)

p(Œ≤ (s) |X, y)
Q3
Qn
‚àó
T ‚àó
j=1 dnorm(Œ≤j , 0, 10)
i=1 dpois(yi , xi Œ≤ )
= Qn
√ó
.
Q3
(s)
T (s) )
i=1 dpois(yi , xi Œ≤
j=1 dnorm(Œ≤j , 0, 10)

All that remains to implement the algorithm is to specify the proposal distribution for Œ∏‚àó . A convenient choice is a multivariate normal distribution with
mean Œ≤ (s) . In many problems, the posterior variance can be an efficient choice
of a proposal variance. Although we do not know the posterior variance before we run the Metropolis algorithm, it is often sufficient just to use a rough
approximation. In a normal regression problem, the posterior variance of Œ≤
will be close to œÉ 2 (XT X)‚àí1 , where œÉ 2 is the variance of Y . In our Poisson
regression, the model is that the log of Y has expectation equal to Œ≤ T x, so
let‚Äôs try a proposal variance of œÉ
ÀÜ 2 (XT X)‚àí1 where œÉ
ÀÜ 2 is the sample variance of
{log(y1 + 1/2), . . . , log(yn + 1/2)} (we use log(y + 1/2) instead of log y because
the latter would be ‚àí‚àû if y = 0). If this results in an acceptance rate that is
too high or too low, we can always adjust the proposal variance accordingly.
R-code to implement the Metropolis algorithm for a Poisson regression of
y on X is as follows:
data ( c h a p t e r 1 0 ) ; y<‚àíyX . sparrow [ , 1 ] ; X<‚àíyX . sparrow [ , ‚àí 1 ]
n<‚àíl e n g t h ( y ) ; p<‚àídim (X ) [ 2 ]
pmn . beta<‚àír e p ( 0 , p )
psd . beta<‚àír e p ( 1 0 , p )

#p r i o r e x p e c t a t i o n
#p r i o r var

var . prop<‚àí var ( l o g ( y +1/2))‚àó s o l v e ( t (X)%‚àó%X )
S<‚àí10000
beta<‚àír e p ( 0 , p ) ; acs <‚àí0
BETA<‚àím at r i x ( 0 , nrow=S , n c o l=p )
set . seed (1)

#p r o p o s a l var

for ( s in 1: S)
{
b e t a . p<‚àí t ( rmvnorm ( 1 , beta , var . prop ) )
l h r <‚àí sum ( d p o i s ( y , exp (X%‚àó%b e t a . p ) , l o g=T) ) ‚àí
sum ( d p o i s ( y , exp (X%‚àó%b e t a ) , l o g=T) ) +
sum ( dnorm ( b e t a . p , pmn . beta , psd . beta , l o g=T) ) ‚àí
sum ( dnorm ( beta , pmn . beta , psd . beta , l o g=T) )
i f ( l o g ( r u n i f (1)) < l h r ) { beta<‚àíb e t a . p ; acs<‚àía c s+1 }
BETA[ s ,]<‚àí b e t a
}

Applying this algorithm to the song sparrow data gives an acceptance
rate of about 43%. A plot of Œ≤3 versus iteration number appears in the first

0 2000

6000
iteration

10000

181

ACF
0.0 0.2 0.4 0.6 0.8 1.0

‚àí0.3

‚àí0.2

Œ≤3
‚àí0.1

0.0

ACF
0.0 0.2 0.4 0.6 0.8 1.0

10.4 Metropolis, Metropolis-Hastings and Gibbs

0

10

20
lag

30

40

0

5 10
20
lag/10

30

Fig. 10.5. Plot of the Markov chain in Œ≤3 along with autocorrelation functions.

panel of Figure 10.5. The algorithm moves quickly from the starting value
of Œ≤3 = 0 to a region closer to the posterior mode. The second panel of the
figure shows the autocorrelation function for Œ≤3 . We could possibly reduce
the autocorrelation by modifying the proposal variance and obtaining a new
Markov chain, although this Markov chain is perhaps sufficient to obtain a
good approximation to p(Œ≤|X, y). For example, the third panel of Figure 10.5
plots the autocorrelation function of every 10th value of Œ≤3 from the Markov
chain. This ‚Äúthinned‚Äù subsequence contains 1,000 of the 10,000 Œ≤3 values,
but these 1,000 values are nearly independent. This suggests we have nearly
the equivalent of 1,000 independent samples of Œ≤3 with which to approximate
the posterior distribution. To be more precise, we can calculate the effective
sample size as described in Section 6.6. The effective sample sizes for Œ≤1 ,
Œ≤2 and Œ≤3 are 818, 778 and 726 respectively. The adequacy of this Markov
chain is confirmed further in the first two panels of Figure 10.6, which plots
the MCMC approximations to the marginal posterior densities of Œ≤2 and Œ≤3 .
These densities are nearly identical to the ones obtained from the grid-based
approximation, which are shown in gray lines for comparison. Finally, the
third panel of the figure plots posterior quantiles of E[Y |x] for each age x,
which indicates the quadratic nature of reproductive success for this song
sparrow population.

10.4 Metropolis, Metropolis-Hastings and Gibbs
Recall that a Markov chain is a sequentially generated sequence {x(1) , x(2) , . . .}
such that the mechanism that generates x(s+1) can depend on the value of
x(s) but not on {x(s‚àí1) , x(s‚àí2) , . . . x(1) }. A more poetic way of putting this
is that for a Markov chain ‚Äúthe future depends on the present and not on
the past.‚Äù The Gibbs sampler and the Metropolis algorithm are both ways of
generating Markov chains that approximate a target probability distribution
p0 (x) for a potentially vector-valued random variable x. In Bayesian analysis,
x is typically a parameter or vector of parameters and p0 (x) is a posterior

10 Nonconjugate priors and Metropolis-Hastings algorithms

‚àí1

0

Œ≤2

1

2

0

0

0.0

1

2

p( Œ≤2|y)
0.4
0.8

p( Œ≤3|y)
3 4 5

number of offspring
1
2
3

6

7

1.2

182

‚àí0.6

‚àí0.2
Œ≤3

0.2 0.4

1

2

3

4

5

6

age

Fig. 10.6. The first two panels give the MCMC approximations to the posterior
marginal distributions of Œ≤2 and Œ≤3 in black, with the grid-based approximations in
gray. The third panel gives 2.5%, 50% and 97.5% posterior quantiles of exp(Œ≤ T x).

distribution, but the Gibbs sampler and Metropolis algorithm are both used
more broadly.
In this section we will show that these two algorithms are in fact special cases of a more general algorithm, called the Metropolis-Hastings algorithm. We will then describe why Markov chains generated by the MetropolisHastings algorithm are able to approximate a target probability distribution.
Since the Gibbs and Metropolis algorithms are special cases of MetropolisHastings, this implies that these two algorithms are also valid ways to approximate probability distributions.
10.4.1 The Metropolis-Hastings algorithm
We‚Äôll first consider a simple example where our target probability distribution
is p0 (u, v), a bivariate distribution for two random variables U and V . In the
one-sample normal problem, for example, we would have U = Œ∏, V = œÉ 2 and
p0 (u, v) = p(Œ∏, œÉ 2 |y).
Recall that the Gibbs sampler proceeds by iteratively sampling values of
U and V from their conditional distributions: Given x(s) = (u(s) , v (s) ), a new
value of x(s+1) is generated as follows:
1. update U : sample u(s+1) ‚àº p0 (u|v (s) );
2. update V : sample v (s+1) ‚àº p0 (v|u(s+1) ).
Alternatively, we could have first sampled v (s+1) ‚àº p0 (v|u(s) ) and then
u(s+1) ‚àº p0 (u|v (s+1) ).
In contrast, the Metropolis algorithm proposes changes to X = (U, V ) and
then accepts or rejects those changes based on p0 . In the Poisson regression
example the proposed vector differed from its current value at each element
of the vector, but this is not necessary. An alternative way to implement the
Metropolis algorithm is to propose and then accept or reject changes to one
element at a time:

10.4 Metropolis, Metropolis-Hastings and Gibbs

183

1. update U :
a) sample u‚àó ‚àº Ju (u|u(s) );
b) compute r = p0 (u‚àó , v (s) )/p0 (u(s) , v (s) );
c) set u(s+1) to u‚àó or u(s) with probability min(1, r) and max(0, 1 ‚àí r).
2. update V :
a) sample v ‚àó ‚àº Jv (v|v (s) );
b) compute r = p0 (u(s+1) , v ‚àó )/p0 (u(s+1) , v (s) );
c) set v (s+1) to v ‚àó or v (s) with probability min(1, r) and max(0, 1 ‚àí r).
Here, Ju and Jv are separate symmetric proposal distributions for U and V .
This Metropolis algorithm generates proposals from Ju and Jv and accepts them with some probability min(1, r). Similarly, each step of the Gibbs
sampler can be seen as generating a proposal from a full conditional distribution and then accepting it with probability 1. The Metropolis-Hastings
algorithm generalizes both of these approaches by allowing arbitrary proposal
distributions. The proposal distributions can be symmetric around the current
values, full conditional distributions, or something else entirely. A MetropolisHastings algorithm for approximating p0 (u, v) runs as follows:
1. update U :
a) sample u‚àó ‚àº Ju (u|u(s) , v (s) );
b) compute the acceptance ratio
r=

p0 (u‚àó , v (s) )
Ju (u(s) |u‚àó , v (s) )
√ó
;
p0 (u(s) , v (s) ) Ju (u‚àó |u(s) , v (s) )

c) set u(s+1) to u‚àó or u(s) with probability min(1, r) and max(0, 1 ‚àí r).
2. update V :
a) sample v ‚àó ‚àº Jv (v|u(s+1) , v (s) );
b) compute the acceptance ratio
r=

p0 (u(s+1) , v ‚àó )
Jv (v (s) |u(s+1) , v ‚àó )
√ó
;
(s+1)
(s)
p0 (u
, v ) Jv (v ‚àó |u(s+1) , v (s) )

c) set v (s+1) to v ‚àó or v (s) with probability min(1, r) and max(0, 1 ‚àí r).
In this algorithm the proposal distributions Ju and Jv are not required to be
symmetric. In fact, the only requirement is that they do not depend on U or V
values in our sequence previous to the most current values. This requirement
ensures that the sequence is a Markov chain.
The Metropolis-Hastings algorithm looks a lot like the Metropolis algorithm, except that the acceptance ratio contains an extra factor, the ratio of
the probability of generating the current value from the proposed to the probability of generating the proposed from the current. This can be viewed as a
‚Äúcorrection factor:‚Äù If a value u‚àó is much more likely to be proposed than the
current value u(s) , then we must down-weight the probability of accepting u‚àó
accordingly, otherwise the value u‚àó will be overrepresented in our sequence.

184

10 Nonconjugate priors and Metropolis-Hastings algorithms

That the Metropolis algorithm is a special case of the Metropolis-Hastings
algorithm is easy to see: If Ju is symmetric, meaning that J(ua |ub , v) =
J(ub |ua , v) for all possible ua , ub and v, then the correction factor in the
Metropolis-Hastings acceptance ratio is equal to 1 and the acceptance probability is the same as in the Metropolis algorithm. That the Gibbs sampler is a
type of Metropolis-Hastings algorithm is almost as easy to see. In the Gibbs
sampler the proposal distribution for U is the full conditional distribution of
U given V = v. If we use the full conditionals as our proposal distributions
in the Metropolis-Hastings algorithm, we have Ju (u‚àó |u(s) , v (s) ) = p0 (u‚àó |v (s) ).
The Metropolis-Hastings acceptance ratio is then
r=

p0 (u‚àó , v (s) )
Ju (u(s) |u‚àó , v (s) )
√ó
p0 (u(s) , v (s) ) Ju (u‚àó |u(s) , v (s) )

=

p0 (u‚àó , v (s) ) p0 (u(s) |v (s) )
p0 (u(s) , v (s) ) p0 (u‚àó |v (s) )

=

p0 (u‚àó |v (s) )p0 (v (s) ) p0 (u(s) |v (s) )
p0 (u(s) |v (s) )p0 (v (s) ) p0 (u‚àó |v (s) )

=

p0 (v (s) )
= 1,
p0 (v (s) )

and so if we propose a value from the full conditional distribution the acceptance probability is 1, and the algorithm is equivalent to the Gibbs sampler.
10.4.2 Why does the Metropolis-Hastings algorithm work?
A more general form of the Metropolis-Hastings algorithm is as follows: Given
a current value x(s) of X,
1. Generate x‚àó from Js (x‚àó |x(s) );
2. Compute the acceptance ratio
r=

p0 (x‚àó )
Js (x(s) |x‚àó )
√ó
;
p0 (x(s) ) Js (x‚àó |x(s) )

3. Sample u ‚àºuniform(0, 1). If u < r set x(s+1) = x‚àó , else set x(s+1) = x(s) .
Note that the proposal distribution may also depend on the iteration number
s. For example, the Metropolis-Hastings algorithm presented in the last section can be equivalently described by steps 1, 2 and 3 above by setting Js to
be equal to Ju for odd values of s and equal to Jv for even values. This makes
the algorithm alternately update values of U and V .
The primary restriction we place on Js (x‚àó |x(s) ) is that it does not depend
on values in the sequence previous to x(s) . This restriction ensures that the
algorithm generates a Markov chain. We also want to choose Js so that the
Markov chain is able to converge to the target distribution p0 . For example,

10.4 Metropolis, Metropolis-Hastings and Gibbs

185

we want to make sure that every value of x such that p0 (x) > 0 will eventually
be proposed (and so accepted some fraction of the time), regardless of where
we start the Markov chain. An example in which this is not the case is where
the values of X having non-zero probability are the integers, and Js (x‚àó |x(s) )
proposes x(s) ¬± 2 with equal probability. In this case the Metropolis-Hastings
algorithm produces a Markov chain, but the chain will only generate even
numbers if x(1) is even, and only odd number if x(1) is odd. This type of
Markov chain is called reducible, as the set of possible X-values can be divided
into non-overlapping sets (even and odd integers in this example), between
which the algorithm is unable to move. In contrast, we want our Markov chain
to be irreducible, that is, able to go from any one value of X to any other,
eventually.
Additionally, we will want Js to be such that the Markov chain is aperiodic
and recurrent. A value x is periodic with period k > 1 in a Markov chain if it
can only be visited every kth iteration. If x is periodic, then for every S there
are an infinite number of iterations s > S for which Pr(x(s) = x) = 0. Since
we want the distribution of x(s) to converge to p0 , we should make sure that if
p0 (x) > 0, then x is not periodic in our Markov chain. A Markov chain lacking
any periodic states is called aperiodic. Finally, if x(s) = x for some iteration
s, then this must mean that p0 (x) > 0. Therefore, we want our Markov chain
to be able to return to x from time to time as we run our chain (otherwise the
relative fraction of x‚Äôs in the chain will go to zero, even though p0 (x) > 0). A
value x is said to be recurrent if, when we continue to run the Markov chain
from x, we are guaranteed to eventually return to x. Clearly we want all of
the possible values of X to be recurrent in our Markov chain.
An irreducible, aperiodic and recurrent Markov chain is a very well behaved object. A theorem from probability theory says that the empirical distribution of samples generated from such a Markov chain will converge:
Theorem 2 (Ergodic Theorem) If {x(1) , x(2) , . . .} is an irreducible, aperiodic
and recurrent Markov chain, then there is a unique probability distribution œÄ
such that as s ‚Üí ‚àû,
(s)
‚Ä¢ Pr(x
P ‚àà(s)A) ‚ÜíRœÄ(A) for any set A;
1
‚Ä¢ S
g(x ) ‚Üí g(x)œÄ(x) dx.

The distribution œÄ is called the stationary distribution of the Markov chain.
It is called the stationary distribution because it has the following property:
If x(s) ‚àº œÄ,
and x(s+1) is generated from the Markov chain starting at x(s) ,
then Pr(x(s+1) ‚àà A) = œÄ(A).
In other words, if you sample x(s) from œÄ and then generate x(s+1) conditional
on x(s) from the Markov chain, then the unconditional distribution of x(s+1)
is œÄ. Once you are sampling from the stationary distribution, you are always
sampling from the stationary distribution.

186

10 Nonconjugate priors and Metropolis-Hastings algorithms

In most problems it is not too hard to construct Metropolis-Hastings algorithms that generate Markov chains that are irreducible, aperiodic and recurrent. For example, if p0 (x) is continuous, then using a normal proposal
distribution centered around the current value guarantees that Pr(x(s+1) ‚àà
A|x(s) = x) > 0 for every x, s and set A such that p0 (A) > 0. All of the
Metropolis-Hastings algorithms in this book generate Markov chains that are
irreducible, aperiodic and recurrent. As such, sequences of X-values generated from these algorithms can be used to approximate their stationary distributions. What is left to show is that the stationary distribution œÄ for a
Metropolis-Hastings algorithm is equal to the distribution p0 we wish to approximate.
‚ÄúProof ‚Äù that œÄ(x) = p0 (x)
The theorem above says that the stationary distribution of the MetropolisHastings algorithm is unique, and so if we show that p0 is a stationary distribution, we will have shown it is the stationary distribution. Our sketch of
a proof follows closely a proof from Gelman et al (2004) for the Metropolis
algorithm. In that proof and here, it is assumed for simplicity that X is a
discrete random variable. Suppose x(s) is sampled from the target distribution p0 , and then x(s+1) is generated from x(s) using the Metropolis-Hastings
algorithm. To show that p0 is the stationary distribution we need to show
that Pr(x(s+1) = x) = p0 (x).
Let xa and xb be any two values of X such that p0 (xa )Js (xb |xa ) ‚â•
p0 (xb )Js (xa |xb ). Then under the Metropolis-Hastings algorithm the probability that x(s) = xa and x(s+1) = xb is equal to the probability of
1. sampling x(s) = xa from p0 ;
2. proposing x‚àó = xb from Js (x‚àó |x(s) );
3. accepting x(s+1) = xb .
The probability of these three things occurring is their product:
Pr(x(s) = xa , x(s+1) = xb ) = p0 (xa ) √ó Js (xb |xa ) √ó

p0 (xb ) Js (xa |xb )
p0 (xa ) Js (xb |xa )

= p0 (xb )Js (xa |xb ) .
On the other hand, the probability that x(s) = xb and x(s+1) = xa is the
probability that xb is sampled from p0 , that xa is proposed from Js (x‚àó |x(s) )
and that xa is accepted as x(s+1) . But in this case the acceptance probability
is one because we assumed p0 (xa )Js (xb |xa ) ‚â• p0 (xb )Js (xa |xb ). This means
that Pr(x(s) = xb , x(s+1) = xa ) = p0 (xb )Js (xa |xb ).
The above two calculations have shown that the probability of observing
x(s) and x(s+1) to be xa and xb , respectively, is the same as observing them
to be xb and xa respectively, for any two values xa and xb . The final step of
the proof is to use this fact to derive the marginal probability Pr(x(s+1) = x):

10.5 Combining the Metropolis and Gibbs algorithms

Pr(x(s+1) = x) =

X

=

X

187

Pr(x(s+1) = x, x(s) = xa )

xa

Pr(x(s+1) = xa , x(s) = x)

xa

= Pr(x(s) = x)
This completes the proof that Pr(x(s+1) = x) = p0 (x) if Pr(x(s) = x) = p0 (x).

10.5 Combining the Metropolis and Gibbs algorithms
In complex models it is often the case that conditional distributions are available for some parameters but not for others. In these situations we can combine
Gibbs and Metropolis-type proposal distributions to generate a Markov chain
to approximate the joint posterior distribution of all of the parameters. In this
section we do this in the context of estimating the parameters in a regression
model for time-series data where the errors are temporally correlated. In this
case, full conditional distributions are available for the regression parameters
but not the parameter describing the dependence among the observations.
Example: Historical CO2 and temperature data
Analyses of ice cores from East Antarctica have allowed scientists to deduce
historical atmospheric conditions of the last few hundred thousand years (Petit et al, 1999). The first plot of Figure 10.7 plots time-series of temperature
and carbon dioxide concentration on a standardized scale (centered and scaled
to have a mean of zero and a variance of one). The data include 200 values
of temperature measured at roughly equal time intervals, with time between
consecutive measurements being approximately 2,000 years. For each value of
temperature there is a CO2 concentration value corresponding to a date that
is roughly 1,000 years previous to the temperature value, on average. Temperature is recorded in terms of its difference from current present temperature
in degrees Celsius, and CO2 concentration is recorded in parts per million by
volume.
The plot indicates that the temporal history of temperature and CO2
follow very similar patterns. The second plot in Figure 10.7 indicates that CO2
concentration at a given time point is predictive of temperature following that
time point. One way to quantify this is by fitting a linear regression model for
temperature (Y ) as a function of CO2 (x). Ordinary least squares regression
ÀÜ |x] = ‚àí23.02+0.08x with a nominal standard
gives an estimated model of E[Y
error of 0.0038 for the slope term. The validity of this standard error relies
on the error terms in the regression model being independent and identically
distributed, and standard confidence intervals further rely on the errors being
normally distributed.

temp
CO2

‚àí4e+05

‚àí3e+05

‚àí2e+05
year

‚àí1e+05

temperature difference (deg C)
‚àí8
‚àí4
0 2

10 Nonconjugate priors and Metropolis-Hastings algorithms

standardized measurement
‚àí2 ‚àí1 0
1
2
3

188

0e+00

‚óè

‚óè

‚óè

‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè ‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè

180

220
260
CO2(ppmv)

0

‚àí0.2

0.0

10

0.2

frequency
20
30

ACF
0.4 0.6

40

0.8

50

1.0

Fig. 10.7. Temperature and carbon dioxide data.

‚àí4

‚àí2

0
residual

2

4

0

5

10
lag

15

20

Fig. 10.8. Residual analysis for the least squares estimation.

These two assumptions are examined in the two residual diagnostic plots
in Figure 10.8. The first plot, a histogram of the residuals, indicates no serious deviation from non-normality. The second plot gives the autocorrelation
function of the residuals, and indicates a nontrivial correlation of 0.52 between residuals at consecutive time points. Such a positive correlation generally means that there is less information in the data, and less evidence for a
relationship between the two variables, than is assumed by the ordinary least
squares regression analysis.
10.5.1 A regression model with correlated errors
The ordinary regression model is

10.5 Combining the Metropolis and Gibbs algorithms

Ô£´

189

Ô£∂

Y1
Ô£¨ .. Ô£∑
Y = Ô£≠ . Ô£∏ ‚àº multivariate normal(XŒ≤, œÉ 2 I).
Yn
The diagnostic plots suggest that a more appropriate model for the ice core
data is one in which the error terms are not independent, but temporally
correlated. This means we must replace the covariance matrix œÉ 2 I in the ordinary regression model with a matrix Œ£ that can represent positive correlation
between sequential observations. One simple, popular class of covariance matrices for temporally correlated data are those having first-order autoregressive
structure:
Ô£´
Ô£∂
1
œÅ œÅ2 ¬∑ ¬∑ ¬∑ œÅn‚àí1
Ô£¨ œÅ
1 œÅ ¬∑ ¬∑ ¬∑ œÅn‚àí2 Ô£∑
Ô£¨ 2
Ô£∑
Ô£¨
Ô£∑
œÅ 1
Œ£ = œÉ 2 CœÅ = œÉ 2 Ô£¨ œÅ
Ô£∑
Ô£¨ ..
Ô£∑
..
..
Ô£≠ .
Ô£∏
.
.
œÅn‚àí1 œÅn‚àí2

1

Under this covariance matrix the variance of {Yi |Œ≤, xi } is œÉ 2 but the correlation between Yi and Yi+t is œÅt , which decreases to zero as the time difference
t becomes larger.
Having observed Y = y, the parameters to estimate in this model include Œ≤, œÉ 2 and œÅ. Using the multivariate normal and inverse-gamma prior
distributions of Section 9.2.1 for Œ≤ and œÉ 2 , it is left as an exercise to show
that
{Œ≤|X, y, œÉ 2 , œÅ} ‚àº multivariate normal(Œ≤ n , Œ£n ) , where
‚àí1 ‚àí1
2
Œ£n = (XT C‚àí1
œÅ X/œÉ + Œ£0 )

(10.2)

‚àí1
2
Œ≤ n = Œ£n (XT C‚àí1
œÅ y/œÉ + Œ£0 Œ≤ 0 ) , and

{œÉ 2 |X, y, Œ≤, œÅ} ‚àº inverse-gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSRœÅ ]/2) , where
SSRœÅ = (y ‚àí XŒ≤)T C‚àí1
œÅ (y ‚àí XŒ≤).
If Œ≤ 0 = 0 and Œ£0 has large diagonal entries, then Œ≤ n is very close to
‚àí1 T ‚àí1
(XT C‚àí1
X CœÅ y. If œÅ were known this would be the generalized least
œÅ X)
squares (GLS) estimate of Œ≤, a type of weighted least squares estimate that is
used when the error terms are not independent and identically distributed. In
such situations, both OLS and GLS provide unbiased estimates of Œ≤ but the
GLS estimate has a lower variance. Bayesian analysis using a model that accounts for the correlated errors provides parameter estimates that are similar
to those of GLS, so for convenience we will refer to our analysis as ‚ÄúBayesian
GLS.‚Äù
If we knew the value of œÅ we could use the Gibbs sampler to approximate
p(Œ≤, œÉ 2 |X, y, œÅ) by iteratively sampling from the full conditional distributions
given by the equations in 10.2. Of course œÅ is unknown and so we will need to

190

10 Nonconjugate priors and Metropolis-Hastings algorithms

estimate it as well with our Markov chain. Unfortunately the full conditional
distribution for œÅ will be nonstandard for most prior distributions, suggesting
that the Gibbs sampler is not applicable here and we may have to use a
Metropolis algorithm (although a discrete approximation to p(œÅ|X, y, Œ≤, œÉ 2 )
could be used).
It is in situations like this that the generality of the Metropolis-Hastings
algorithm comes in handy. Recall that in this algorithm we are allowed to
use different proposal distributions at each step. We can iteratively update Œ≤,
œÉ 2 and œÅ at different steps, making proposals with full conditional distributions for Œ≤ and œÉ 2 (Gibbs proposals) and a symmetric proposal distribution
for œÅ (a Metropolis proposal). Following the rules of the Metropolis-Hastings
algorithm, we accept with probability 1 any proposal coming from a full conditional distribution, whereas we have to calculate an acceptance probability
for proposals of œÅ. Given {Œ≤ (s) , œÉ 2(s) , œÅ(s) }, a Metropolis-Hastings algorithm
to generate a new set of parameter values is as follows:
1. Update Œ≤: Sample Œ≤ (s+1) ‚àº multivariate normal(Œ≤ n , Œ£n ), where Œ≤ n and
Œ£n depend on œÉ 2(s) and œÅ(s) .
2. Update œÉ 2 : Sample œÉ 2(s+1) ‚àº inverse-gamma([ŒΩ0 + n]/2, [ŒΩ0 œÉ02 + SSRœÅ ]/2),
where SSRœÅ depends on Œ≤ (s+1) and œÅ(s) .
3. Update œÅ:
a) Propose œÅ‚àó ‚àº uniform(œÅ(s) ‚àí Œ¥, œÅ(s) + Œ¥). If œÅ‚àó < 0 then reassign it to
be |œÅ‚àó |. If œÅ‚àó > 1 reassign it to be 2 ‚àí œÅ‚àó .
b) Compute the acceptance ratio
r=

p(y|X, Œ≤ (s+1) , œÉ 2(s+1) , œÅ‚àó )p(œÅ‚àó )
p(y|X, Œ≤ (s+1) , œÉ 2(s+1) , œÅ(s) )p(œÅ(s) )

and sample u ‚àº uniform(0,1). If u < r set œÅ(s+1) = œÅ‚àó , otherwise set
œÅ(s+1) = œÅ(s) .
The proposal distribution used in Step 3.a is called a reflecting random walk ,
which ensures that 0 < œÅ < 1. It is left as an exercise to show that this
proposal distribution is symmetric. We also leave it as an exercise to show
that the value of r given in Step 3.b is numerically equal to
p(Œ≤ (s+1) , œÉ 2(s+1) , œÅ‚àó |y, X)
p(Œ≤ (s+1) , œÉ 2(s+1) , œÅ(s) |y, X)

,

the ratio as given in the definition of the Metropolis algorithm.
While technically the steps above constitute ‚Äúthree iterations‚Äù of the
Metropolis-Hastings algorithm, it is convenient to group them together as
one. A sequence of Metropolis-Hastings steps in which each parameter is updated is often referred to as a scan of the algorithm.

10.5 Combining the Metropolis and Gibbs algorithms

191

10.5.2 Analysis of the ice core data

0.55

0.0

0.2

0.65

œÅ
0.75

ACF
0.4
0.6

0.8

0.85

1.0

We‚Äôll use diffuse prior distributions for the parameters, with Œ≤ 0 = 0, Œ£0 =
diag(1000), ŒΩ0 = 1 and œÉ02 = 1. Our prior for œÅ will be the uniform distribution on (0, 1). The first panel of Figure 10.9 plots the first 1,000 values
{œÅ(1) , . . . , œÅ(1000) } generated using the Metropolis-Hastings algorithm above.
The acceptance rate for these values is 0.322 which seems good, but the autocorrelation of the sequence, shown in the second panel, is very high. The
effective sample size for this correlated sequence of 1,000 œÅ-values is only 23,
indicating that we will need many more iterations of the algorithm to obtain
a decent approximation to the posterior distribution.

0

200

400
600
scan

800

1000

0

5

10

15
lag

20

25

30

Fig. 10.9. The first 1,000 values of œÅ generated from the Markov chain.

Suppose we were to generate 25,000 scans for a total of 25, 000 √ó 4 =
100, 000 parameter values. Storing and manipulating all of these values can
be tedious and somewhat unnecessary: Since the Markov chain is so highly
correlated, the values of œÅ(s) and œÅ(s+1) offer roughly the same information
about the posterior distribution. With this in mind, for highly correlated
Markov chains with moderate to large numbers of parameters we will often
only save a fraction of the scans of the Markov chain. This practice of throwing
away many iterations of a Markov chain is known as thinning. Figure 10.10
shows the thinned output of a 25,000-scan Markov chain for the ice core data,
in which only every 25th scan was saved. Thinning the output reduces it
down to a manageable 1,000 samples, having a much lower autocorrelation
than 1,000 sequential samples from an unthinned Markov chain.
The Monte Carlo approximation to the posterior density of Œ≤2 , the slope
parameter, appears in the first panel of Figure 10.11. The posterior mean of Œ≤2
is 0.028 and a posterior 95% quantile-based confidence interval is (0.01, 0.05),

10 Nonconjugate priors and Metropolis-Hastings algorithms

0.0

0.6

0.2

0.7

œÅ

0.8

ACF
0.4
0.6

0.8

0.9

1.0

192

0

200

400
600
scan/25

800

1000

0

5

10

15 20
lag/25

25

30

Fig. 10.10. Every 25th value of œÅ from a Markov chain of length 25,000.

indicating evidence that the relationship between CO2 and temperature is
positive. However, as indicated in the second plot this relationship seems
much weaker than that suggested by the OLS estimate of 0.08. For the OLS
estimation, the small number of data points with high y-values have a larger
amount of influence on the estimate of Œ≤. In contrast, the GLS model recognizes that many of these extreme points are highly correlated with one
another and down-weights their influence. We note that this ‚Äúweaker‚Äù regression coefficient is a result of the temporally correlated data, and not of the
particular prior distribution we used or the Bayesian approach in general. The
reader is encouraged to repeat the analysis with different prior distributions,
or to perform a non-Bayesian GLS estimation for comparison. In any case,
the data analysis indicates evidence of a relationship between temperature
measurements and the CO2 measurements that precede them in time.

10.6 Discussion and further references
The Metropolis algorithm was introduced by Metropolis et al (1953) in an
application to a problem in statistical physics. The algorithm was generalized by Hastings (1970), but it was not until the publication of Gelfand and
Smith (1990) that MCMC became widely used in the statistics community.
See Robert and Casella (2008) for a brief history of Monte Carlo and MCMC
methods.
A number of modifications and extensions of MCMC methods have appeared since the 1990s. One technique that is broadly applicable is automatic,
adaptive tuning of the proposal distribution in order to achieve good mixing
(Gilks et al, 1998; Haario et al, 2001). Not all adaptive algorithms will result

10.6 Discussion and further references

193

‚óè

GLS estimate
OLS estimate

0

‚àí8

‚àí6

temperature
‚àí4 ‚àí2

0

posterior marginal density
10
20
30

2

‚óè

0.00

0.02

Œ≤2

0.04

0.06

‚óè

‚óè
‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè ‚óè ‚óè

‚óè ‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè ‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚óè‚óè
‚óè‚óè‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

180

200

220

240
CO2

260

280

Fig. 10.11. Posterior distribution of the slope parameter Œ≤2 , along with the posterior mean regression line.

in chains that converge to the target distribution, but there are known conditions under which convergence is guaranteed (Atchad¬¥e and Rosenthal, 2005;
Roberts and Rosenthal, 2007).

11
Linear and generalized linear mixed effects
models

In Chapter 8 we learned about the concept of hierarchical modeling, a data
analysis approach that is appropriate when we have multiple measurements
within each of several groups. In that chapter, variation in the data was represented with a between-group sampling model for group-specific means, in
addition to a within-group sampling model to represent heterogeneity of observations within a group. In this chapter we extend the hierarchical model
to describe how relationships between variables may differ between groups.
This can be done with a regression model to describe within-group variation,
and a multivariate normal model to describe heterogeneity among regression
coefficients across the groups. We also cover estimation for hierarchical generalized linear models, which are hierarchical models that have a generalized
linear regression model representing within-group heterogeneity.

11.1 A hierarchical regression model
Let‚Äôs return to the math score data described in Section 8.4, which included
math scores of 10th grade children from 100 different large urban public high
schools. In Chapter 8 we estimated school-specific expected math scores, as
well as how these expected values varied from school to school. Now suppose we are interested in examining the relationship between math score
and another variable, socioeconomic status (SES), which was calculated from
parental income and education levels for each student in the dataset.
In Chapter 8 we quantified the between-school heterogeneity in expected
math score with a hierarchical model. Given the amount of variation we observed it seems possible that the relationship between math score and SES
might vary from school to school as well. A quick and easy way to assess this
possibility is to fit a linear regression model of math score as a function of
SES for each of the 100 schools in the dataset. To make the parameters more
interpretable we will center the SES scores within each school separately, so
that the sample average SES score within each school is zero. As a result, the
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 11,
c Springer Science+Business Media, LLC 2009


196

11 Linear and generalized linear mixed effects models

20
‚àí2 ‚àí1

0
1
SES

2

10

‚óè

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

5

‚óè
‚óè‚óè
‚óè
‚óè
‚óè ‚óè

10 15 20 25 30
sample size

slope
0
5

‚óè

‚àí5

math score
40
60

80

intercept
40 45 50 55 60 65

intercept of the regression line can be interpreted as the school-level average
math score.
‚óè

‚óè

‚óè
‚óè ‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè ‚óè‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè
‚óè ‚óè‚óè ‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè ‚óè‚óè ‚óè
‚óè ‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè
‚óè

5

10 15 20 25 30
sample size

Fig. 11.1. Least squares regression lines for the math score data, and plots of
estimates versus group sample size.

The first panel of Figure 11.1 plots least squares estimates of the regression
lines for the 100 schools, along with an average of these lines in black. A
large majority show an increase in expected math score with increasing SES,
although a few show a negative relationship. The second and third panels of
the figure relate the least squares estimates to sample size. Notice that schools
with the highest sample sizes have regression coefficients that are generally
close to the average, whereas schools with extreme coefficients are generally
those with low sample sizes. This phenomenon is reminiscent of what we
discussed in Section 8.4: The smaller the sample size for the group, the more
probable that unrepresentative data are sampled and an extreme least squares
estimate is produced. As in Chapter 8, our remedy to this problem will be
to stabilize the estimates for small sample size schools by sharing information
across groups, using a hierarchical model.
The hierarchical model in the linear regression setting is a conceptually
straightforward generalization of the normal hierarchical model from Chapter
8. We use an ordinary regression model to describe within-group heterogeneity
of observations, then describe between-group heterogeneity using a sampling
model for the group-specific regression parameters. Expressed symbolically,
our within-group sampling model is
Yi,j = Œ≤ Tj xi,j + i,j , {i,j } ‚àº i.i.d. normal(0, œÉ 2 ),

(11.1)

where xi,j is a p√ó1 vector of regressors for observation i in group j. Expressing
Y1,j , . . . , Ynj ,j as a vector Y j and combining x1,j , . . . , xnj ,j into an nj √ó p
matrix Xj , the within-group sampling model can be expressed equivalently
as Y j ‚àº multivariate normal(Xj Œ≤ j , œÉ 2 I), with the group-specific data vectors
Y 1 , . . . , Y m being conditionally independent given Œ≤ 1 , . . . , Œ≤ m and œÉ 2 .

11.1 A hierarchical regression model

197

The heterogeneity among the regression coefficients Œ≤ 1 , . . . , Œ≤ m will be described with a between-group sampling model. If we have no prior information
distinguishing the different groups we can model them as being exchangeable,
or (roughly) equivalently, as being i.i.d. from some distribution representing the sampling variability across groups. The normal hierarchical regression
model describes the across-group heterogeneity with a multivariate normal
model, so that
Œ≤ 1 , . . . , Œ≤ m ‚àº i.i.d. multivariate normal(Œ∏, Œ£).

(11.2)

A graphical representation of the hierarchical model appears in Figure 11.2,
which makes clear that the multivariate normal distribution for Œ≤ 1 , . . . , Œ≤ m
is not a prior distribution representing uncertainty about a fixed but unknown quantity. Rather, it is a sampling distribution representing heterogeneity among a collection of objects. The values of Œ∏ and Œ£ are fixed but
unknown parameters to be estimated.
Œ∏, Œ£
Œ≤1

Œ≤2

¬∑¬∑¬∑

Œ≤ m‚àí1

Y1

Y2

¬∑¬∑¬∑

Y m‚àí1 Y m

Œ≤m

œÉ2
Fig. 11.2. A graphical representation of the hierarchical normal regression model.

This hierarchical regression model is sometimes called a linear mixed effects model . This name is motivated by an alternative parameterization of
Equations 11.1 and 11.2. We can rewrite the between-group sampling model
as
Œ≤j = Œ∏ + Œ≥ j
Œ≥ 1 . . . , Œ≥ m ‚àº i.i.d. multivariate normal(0, Œ£).
Plugging this into our within-group regression model gives
Yi,j = Œ≤ Tj xi,j + i,j
= Œ∏ T xi,j + Œ≥ Tj xi,j + i,j .
In this parameterization Œ∏ is referred to as a fixed effect as it is constant
across groups, whereas Œ≥ 1 , . . . , Œ≥ m are called random effects, as they vary.
The name ‚Äúmixed effects model‚Äù comes from the fact that the regression
model contains both fixed and random effects. Although for our particular

198

11 Linear and generalized linear mixed effects models

example the regressors corresponding to the fixed and random effects are the
same, this does not have to be the case. A more general model would be
Yi,j = Œ∏ T xi,j + Œ≥ Tj z i,j + i,j , where xi,j and z i,j could be vectors of different
lengths which may or may not contain overlapping variables. In particular,
xi,j might contain regressors that are group specific, that is, constant across
all observations in the same group. Such variables are not generally included
in z i,j , as there would be no information in the data with which to estimate
the corresponding group-specific regression coefficients.
Given a prior distribution for (Œ∏, Œ£, œÉ 2 ) and having observed Y 1 =
y 1 , . . . , Y m = y m , a Bayesian analysis proceeds by computing the posterior
distribution p(Œ≤ 1 , . . . , Œ≤ m , Œ∏, Œ£, œÉ 2 |X1 , . . . , Xm , y 1 , . . . , y m ). If semiconjugate
prior distributions are used for Œ∏, Œ£ and œÉ 2 , then the posterior distribution
can be approximated quite easily with Gibbs sampling. The classes of semiconjugate prior distributions for Œ∏ and Œ£ are as in the multivariate normal
model discussed in Chapter 7. The prior we will use for œÉ 2 is the usual inversegamma distribution.
Œ∏ ‚àº multivariate normal(¬µ0 , Œõ0 )
Œ£ ‚àº inverse-Wishart(Œ∑0 , S‚àí1
0 )
2
œÉ ‚àº inverse-gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2)

11.2 Full conditional distributions
While computing the posterior distribution for so many parameters may seem
daunting, the calculations involved in computing the full conditional distributions have the same mathematical structure as models we have studied
in previous chapters. Once we have the full conditional distributions we can
iteratively sample from them to approximate the joint posterior distribution.
Full conditional distributions of Œ≤ 1 , . . . , Œ≤ m
Our hierarchical regression model shares information across groups via the
parameters Œ∏, Œ£ and œÉ 2 . As a result, conditional on Œ∏, Œ£, œÉ 2 the regression
coefficients Œ≤ 1 , . . . , Œ≤ m are independent. Referring to the graph in Figure 11.2,
from the perspective of a given Œ≤ j the model looks like an ordinary one-group
regression problem where the prior mean and variance for Œ≤ j are Œ∏ and Œ£.
This analogy is in fact correct, and the results of Section 9.2.1 show that
{Œ≤ j |y j , Xj , Œ∏, Œ£, œÉ 2 } has a multivariate normal distribution with
Var[Œ≤ j |y j , Xj , œÉ 2 , Œ∏, Œ£] = (Œ£ ‚àí1 + XTj Xj /œÉ 2 )‚àí1
E[Œ≤ j |y j , Xj , œÉ 2 , Œ∏, Œ£] = (Œ£ ‚àí1 + XTj Xj /œÉ 2 )‚àí1 (Œ£ ‚àí1 Œ∏ + XTj y j /œÉ 2 ).

11.2 Full conditional distributions

199

Full conditional distributions of Œ∏ and Œ£
Our sampling model for the Œ≤ j ‚Äôs is that they are i.i.d. samples from a multivariate normal population with mean Œ∏ and variance Œ£. Inference for the
population mean and variance of a multivariate normal population was covered in Chapter 7, in which we derived the full conditional distributions when
semiconjugate priors are used. There, we saw that the full conditional distribution of a population mean is multivariate normal with expectation equal to
a combination of the prior expectation and the sample mean, and precision
equal to the sum of the prior and data precisions. In the context of the hierarchical regression model, given Œ£ and our sample of regression coefficients
Œ≤ 1 , . . . , Œ≤ m , the full conditional distribution of Œ∏ is as follows:
{Œ∏|Œ≤ 1 , . . . , Œ≤ m , Œ£} ‚àº multivariate normal(¬µm , Œõm ) , where
‚àí1 ‚àí1
Œõm = (Œõ‚àí1
)
0 + mŒ£
‚àí1
¬Ø
¬µm = Œõm (Œõ0 ¬µ0 + mŒ£ ‚àí1 Œ≤)
¬Ø is the vector average 1 P Œ≤ . In Chapter 7 we also saw that the full
where Œ≤
j
m
conditional distribution of a covariance matrix is an inverse-Wishart distribution, with sum of squares matrix equal to the prior sum of squares S0 plus
the sum of squares from the sample:
{Œ£|Œ∏, Œ≤ 1 , . . . , Œ≤ m } ‚àº inverse-Wishart(Œ∑0 + m, [S0 + SŒ∏ ]‚àí1 ) , where
m
X
SŒ∏ =
(Œ≤ j ‚àí Œ∏)(Œ≤ j ‚àí Œ∏)T .
j=1

Note that SŒ∏ depends on Œ∏ and so must be recomputed each time Œ∏ is updated
in the Markov chain.
Full conditional distribution of œÉ 2
The parameter œÉ 2 represents the error variance, assumed to be common across
all groups. As such, conditional on Œ≤ 1 , . . . , Œ≤ m , the data provide information
about œÉ 2 via the sum of squared residuals from each group:
X
œÉ 2 ‚àº inverse-gamma([ŒΩ0 +
nj ]/2, [ŒΩ0 œÉ02 + SSR]/2) , where
SSR =

nj
m X
X

(yi,j ‚àí Œ≤ Tj xi,j )2 .

j=1 i=1

It is important to remember that SSR depends on the value of Œ≤ j , and so SSR
must be recomputed in each scan of the Gibbs sampler before œÉ 2 is updated.

200

11 Linear and generalized linear mixed effects models

11.3 Posterior analysis of the math score data

posterior density
0.4 0.6 0.8 1.0

70

1.2

80

To analyze the math score data we will use a prior distribution that is similar
in spirit to the unit information priors that were discussed in Chapter 9. For
example, we‚Äôll take ¬µ0 , the prior expectation of Œ∏, to be equal to the average of
the ordinary least squares regression estimates and the prior variance Œõ0 to be
their sample covariance. Such a prior distribution represents the information
of someone with unbiased but weak prior information. For example, a 95%
prior confidence interval for the slope parameter Œ∏2 under this prior is (3.86,8.60), which is quite a large range when considering what the extremes
of the interval imply in terms of average change in score per unit change in
SES score. Similarly, we will take the prior sum of squares matrix S0 to be
equal to the covariance of the least squares estimate, but we‚Äôll take the prior
degrees of freedom Œ∑0 to be p + 2 = 4, so that the prior distribution of Œ£
is reasonably diffuse but has an expectation equal to the sample covariance
of the least squares estimates. Finally, we‚Äôll take œÉ02 to be the average of the
within-group sample variance but set ŒΩ0 = 1.

20

0.0

30

0.2

40

math score
50 60

Œ∏2
~
Œ≤2

‚àí6

‚àí4

‚àí2 0
2
4
slope parameter

6

8

‚àí2

‚àí1

0
SES

1

2

Fig. 11.3. Relationship between SES and math score. The first panel plots the
posterior density of the expected slope Œ∏2 of a randomly sampled school, as well
as the posterior predictive distribution of a randomly sampled slope. The second
panel gives posterior expectations of the 100 school-specific regression lines, with
the average line given in black.

Running a Gibbs sampler for 10,000 scans and saving every 10th scan
produces a sequence of 1,000 values for each parameter, each sequence having
a fairly low autocorrelation. For example, the lag-10 autocorrelations of Œ∏1 and
Œ∏2 are -0.024 and 0.038. As usual, we can use these simulated values to make
Monte Carlo approximations to various posterior quantities of interest. For

11.4 Generalized linear mixed effects models

201

example, the first plot in Figure 11.3 shows the posterior distribution of Œ∏2 ,
the expected within-school slope parameter. A 95% quantile-based posterior
confidence interval for this parameter is (1.83, 2.96), which, compared to our
prior interval of (-3.86, 8.60), indicates a strong alteration in our information
about Œ∏2 .
The fact that Œ∏2 is extremely unlikely to be negative only indicates that the
population average of school-level slopes is positive. It does not indicate that
any given within-school slope cannot be negative. To clarify this distinction,
the posterior predictive distribution of Œ≤Àú2 , the slope for a to-be-sampled school,
is plotted in the same figure. Samples from this distribution can be generated
Àú (s) from a multivariate normal(Œ∏ (s) , Œ£ (s) ) distribution
by sampling a value Œ≤
for each scan s of the Gibbs sampler. Notice that this posterior predictive
distribution is much more spread out than the posterior distribution of Œ∏2 ,
reflecting the heterogeneity in slopes across schools. Using the Monte Carlo
approximation, we have Pr(Œ≤Àú2 < 0|y 1 , . . . , y m , X1 , . . . , Xm ) ‚âà 0.07, which is
small but not negligible.
The second panel in Figure 11.3 plots posterior expectations of the 100
school-specific regression lines, with the line given by the posterior mean of Œ∏
in black. Comparing this to the first panel of Figure 11.1 indicates how the
hierarchical model is able to share information across groups, shrinking extreme regression lines towards the across-group average. In particular, hardly
any of the slopes are negative when we share information across groups.

11.4 Generalized linear mixed effects models
As the name suggests, a generalized linear mixed effects model combines aspects of linear mixed effects models with those of generalized linear models
described in Chapter 10. Such models are useful when we have a hierarchical
data structure but the normal model for the within-group variation is not
appropriate. For example, if the variable Y were binary or a count, then more
appropriate models for within-group variation would be logistic or Poisson
regression models, respectively.
A basic generalized linear mixed model is as follows:
Œ≤ 1 , . . . , Œ≤ m ‚àº i.i.d. multivariate normal(Œ∏, Œ£)
nj
Y
p(y j |Xj , Œ≤ j , Œ≥) =
p(yi,j |Œ≤ Tj xi,j , Œ≥),
i=1

with observations from different groups also being conditionally independent.
In this formulation p(y|Œ≤ T x, Œ≥) is a density whose mean depends on Œ≤ T x, and
Œ≥ is an additional parameter often representing variance or scale. For example,
in the normal model p(y|Œ≤ T x, Œ≥) = dnorm(y, Œ≤ T x, Œ≥ 1/2 ) where Œ≥ represents
the variance. In the Poisson model p(y|Œ≤ T x) = dpois(exp{Œ≤ T x}), and there
is no Œ≥ parameter.

202

11 Linear and generalized linear mixed effects models

11.4.1 A Metropolis-Gibbs algorithm for posterior approximation
Estimation for the linear mixed effects model was straightforward because
the full conditional distribution of each parameter was standard, allowing for
the easy implementation of a Gibbs sampling algorithm. In contrast, for nonnormal generalized linear mixed models, typically only Œ∏ and Œ£ have standard
full conditional distributions. This suggests we use a Metropolis-Hastings algorithm to approximate the posterior distribution of the parameters, using
a combination of Gibbs steps for updating (Œ∏, Œ£) with a Metropolis step for
updating each Œ≤ j . In what follows we assume there is no Œ≥ parameter. If there
is such a parameter, it can be updated using a Gibbs step if a full conditional
distribution is available, and a Metropolis step if not.
Gibbs steps for (Œ∏, Œ£)
Just as in the linear mixed effects model, the full conditional distributions of Œ∏
and Œ£ depend only on Œ≤ 1 , . . . , Œ≤ m . This means that the form of p(y|Œ≤ T x) has
no effect on the full conditional distributions of Œ∏ and Œ£. Whether p(y|Œ≤ T x)
is a normal model, a Poisson model, or some other generalized linear model,
the full conditional distributions of Œ∏ and Œ£ will be the multivariate normal
and inverse-Wishart distributions described in Section 11.2.
Metropolis step for Œ≤ j
Updating Œ≤ j in a Markov chain can proceed by proposing a new value of Œ≤ ‚àój
based on the current parameter values and then accepting or rejecting it with
the appropriate probability. A standard proposal distribution in this situation
would be a multivariate normal distribution with mean equal to the current
(s)
(s)
value Œ≤ j and with some proposal variance Vj . In this case the Metropolis
step is as follows:
(s)

(s)

1. Sample Œ≤ ‚àój ‚àº multivariate normal(Œ≤ j , Vj ).
2. Compute the acceptance ratio
r=

p(y j |Xj , Œ≤ ‚àój )p(Œ≤ ‚àój |Œ∏ (s) , Œ£ (s) )
(s)

(s)

p(y j |Xj , Œ≤ j )p(Œ≤ j |Œ∏ (s) , Œ£ (s) )
(s+1)

3. Sample u ‚àº uniform(0,1). Set Œ≤ j
(s)

.
(s)

to Œ≤ ‚àój if u < r and to Œ≤ j

if u > r.

In many cases, setting Vj equal to a scaled version of Œ£ (s) produces a wellmixing Markov chain, although the task of finding the right scale might have
to proceed by trial and error.

11.4 Generalized linear mixed effects models

203

A Metropolis-Hastings approximation algorithm
Putting these steps together results in the following Metropolis-Hastings algorithm for approximating p(Œ≤ 1 , . . . , Œ≤ m , Œ∏, Œ£|X1 , . . . , Xm , y 1 , . . . , y m ): Given
current values at scan s of the Markov chain, we obtain new values as follows:
1. Sample Œ∏ (s+1) from its full conditional distribution.
2. Sample Œ£ (s+1) from its full conditional distribution.
3. For each j ‚àà {1, . . . , m},
a) propose a new value Œ≤ ‚àój ;
(s+1)

b) set Œ≤ j

(s)

equal to Œ≤ ‚àój or Œ≤ j

with the appropriate probability.

11.4.2 Analysis of tumor location data
(From Haigis et al (2004)) A certain population of laboratory mice experiences
a high rate of intestinal tumor growth. One item of interest to researchers
is how the rate of tumor growth varies along the length of the intestine. To
study this, the intestine of each of 21 sample mice was divided into 20 sections
and the number of tumors occurring in each section was recorded. The first
panel of Figure 11.4 shows 21 lines, each one representing the observed tumor
counts of each of the mice plotted against the fraction of the length along their
intestine. Although it is hard to tell from the figure, the lines for some mice
are consistently below the average (given in black), and others are consistently
above. This suggests that tumor counts are more similar within a mouse than
between mice, and a hierarchical model with mouse-specific effects may be
appropriate.
A natural model for count data such as these is a Poisson distribution
with a log-link. Letting Yx,j be mouse j‚Äôs tumor count at location x of their
intestine, we will model Yx,j as Poisson(efj (x) ), where fj is a smoothly varying
function of x ‚àà [0, 1]. A simple way to parameterize fj is as a polynomial, so
that fj (x) = Œ≤1,j + Œ≤2,j x + Œ≤3,j x2 + ¬∑ ¬∑ ¬∑ + Œ≤p,j xp‚àí1 for some maximum degree
p ‚àí 1. Such a parameterization allows us to represent each fj as a regression
on (1, x, x2 , . . . , xp‚àí1 ).
Averaging across the 21 mice gives an observed average tumor count y¬Øx
at each of the 20 locations x ‚àà (.05, .10, . . . , .95) along the intestine. This
average curve is plotted in the first panel of Figure 11.4 in black, and the log
of this curve is given in the second panel of the figure. Also in the second
panel are approximations of this curve using polynomials of degree 2, 3 and
4. The second- and third-degree approximations indicate substantial lack-offit, whereas the fourth-degree polynomial fits the log average tumor count
function rather well. For simplicity we‚Äôll model each fj as a fourth-degree
polynomial, although it is possible that a particular fj may be better fit with
a higher degree.
Our between-group sampling model for the Œ≤ j ‚Äôs will be as in the previous
section, so that Œ≤ 1 , . . . , Œ≤ m ‚àº i.i.d. multivariate normal(Œ∏, Œ£). Unconditional

log average number of tumors
‚àí1
0
1
2

11 Linear and generalized linear mixed effects models

number of tumors
5
10

15

204

3
4

44
4
3334
3

4
3
4222223
22
3
2
2
2
4
2
4
23
3
4
2
2 3
4
2 3
4
3
2 4
3
4
3
244
433
34
44
2
33
2

0

2
0.0

0.2

0.4
0.6
location

0.8

1.0

0.2

0.4
0.6
location

0.8

1.0

Fig. 11.4. Tumor count data. The first panel gives mouse-specific tumor counts as
a function of location in gray, with a population average in black. The second panel
gives quadratic, cubic and quartic polynomial fits to the log sample average tumor
count.

on Œ≤ j , the observations coming from a given mouse are statistically dependent as determined by Œ£. Estimating Œ£ in this mixed effects model allows us
to account for and describe potential within-mouse dependencies in the data.
The unknown parameters in this model are Œ∏ and Œ£ for which we need to
specify prior distributions. Using conjugate normal and inverse-Wishart prior
distributions, we need to specify ¬µ and Œõ0 for p(Œ∏) and Œ∑0 and S0 for p(Œ£).
Specifying reasonable values for this many parameters can be difficult, especially in the absence of explicit prior data. As an alternative, we‚Äôll take an
approach based on unit information priors, in which the prior distributions
for the parameters are weakly centered around estimates derived from the observed data. As mentioned before, such prior distributions might represent the
prior information of someone with a small amount of unbiased information.
Our unit information prior requires estimates of Œ∏ and Œ£, the population
mean and covariance of the Œ≤ j ‚Äôs. For each mouse we can obtain a prelimiÀú by regressing {log(y1,j + 1/n), . . . , log(yn,j + 1/n)}
nary ad hoc estimate Œ≤
j
on {x1 , . . . , x20 }, where xi = (1, xi , x2i , x3i , x4i )T for xi ‚àà (.05, .10, . . . , .95)
Àú using maximum likelihood estimates from
(alternatively, we could obtain Œ≤
j
a Poisson regression model). A unit-information type of prior distribution
for Œ∏ would
be a multivariate normal distribution with an expectation of
P
1
Àú
¬µ= m
Œ≤
j=1 j and a prior covariance matrix equal to the sample covariance
Àú
of the Œ≤ j ‚Äôs. We also set S0 equal to this sample covariance matrix, but set
Œ∑0 = p + 2 = 7, so that the prior expectation of Œ£ is equal to S0 but the prior
distribution is relatively diffuse.

11.4 Generalized linear mixed effects models

205

In terms of MCMC posterior approximation, recall from the steps outlined in the previous subsection that values of Œ∏ and Œ£ can be sampled from
their full conditional distributions. The full conditional distributions of the
Œ≤ j ‚Äôs, however, are not standard and so we‚Äôll propose changes to these parameters from distributions centered around their current values. After a bit of
(s)
trial and error, it turns out that a multivariate normal(Œ≤ j , Œ£ (s) /2) proposal
distribution yields an acceptance rate of about 31% and a reasonably wellmixing Markov chain. Running the Markov chain for 50,000 scans and saving
the values every 10th scan gives 5,000 approximate posterior samples for each
parameter. The effective sample sizes for the elements of Œ£ are all above 1,000
except for that of Œ£11 , which was about 950. The effective sample sizes for the
five Œ∏ parameters are (674, 1003, 1092, 1129, 1145). This roughly means that
our Monte‚àöCarlo standard error in approximating E[Œ∏1 |y 1 , . . . , y m , X], for example, is 674 = 25.96 times smaller than the posterior standard error of Œ∏1 .
Of course, we can reduce the Monte Carlo standard error to be as small as we
want by running the Markov chain for more iterations. R-code for generating
this Markov chain appears below:
## data
data ( c h a p t e r 1 1 )
Y<‚àíXY. tumor$Y ; X<‚àíXY. tumor$X ; m<‚àídim (Y ) [ 1 ] ; p<‚àídim (X ) [ 2 ]
## p r i o r s
BETA<‚àíNULL
f o r ( j i n 1 :m)
{
BETA<‚àír b i n d (BETA, lm ( l o g (Y[ j ,]+1/20)Àú ‚àí1+X[ , , j ] ) $ c o e f )
}
mu0<‚àía p p ly (BETA, 2 , mean )
S0<‚àícov (BETA) ; eta0 <‚àíp+2
iL0<‚àíiSigma<‚àís o l v e ( S0 )
## MCMC
THETA. post <‚àíNULL ; s e t . s e e d ( 1 )
for ( s in 1:50000)
{
##update t h e t a
Lm<‚àís o l v e ( i L 0 + m‚àó iSigma )
mum<‚àíLm%‚àó%( i L 0%‚àó%mu0 + iSigma%‚àó%a pply (BETA, 2 , sum ) )
t h e t a <‚àít ( rmvnorm ( 1 ,mum,Lm) )
##
##update Sigma
mtheta<‚àím at r i x ( t h e t a ,m, p , byrow=TRUE)
iSigma<‚àír w i s h ( 1 , e t a 0+m,
s o l v e ( S0+t (BETA‚àímtheta)%‚àó%(BETA‚àímtheta ) ) )
##

206

11 Linear and generalized linear mixed effects models

##update b e t a
Sigma<‚àís o l v e ( iSigma ) ; dSigma<‚àíd e t ( Sigma )
f o r ( j i n 1 :m)
{
b e t a . p<‚àít ( rmvnorm ( 1 ,BETA[ j , ] , . 5 ‚àó Sigma ) )
l r <‚àísum ( d p o i s (Y[ j , ] , exp (X[ , , j ]%‚àó%b e t a . p ) , l o g=TRUE ) ‚àí
d p o i s (Y[ j , ] , exp (X[ , , j ]%‚àó%BETA[ j , ] ) , l o g=TRUE ) ) +
ldmvnorm ( t ( b e t a . p ) , t h e t a , Sigma ,
iSigma=iSigma , dSigma=dSigma ) ‚àí
ldmvnorm ( t (BETA[ j , ] ) , t h e t a , Sigma ,
iSigma=iSigma , dSigma=dSigma )
i f ( l o g ( r u n i f (1)) < l r ) { BETA[ j ,]<‚àí b e t a . p }
}
##
##s t o r e some output
i f ( s%%10==0){THETA. post <‚àír b i n d (THETA. post , t ( t h e t a ) ) }
##

0.0 0.2 0.4 0.6 0.8 1.0
location

15
0

5

10

15
0

0

5

10

number of tumors
5
10
15

}

0.0 0.2 0.4 0.6 0.8 1.0
location

0.0 0.2 0.4 0.6 0.8 1.0
location

Fig. 11.5. 2.5, 50 and 97.5% quantiles for exp(Œ∏ T x), exp(Œ≤ T x) and {Y |x}.

The three panels in Figure 11.5 show posterior distributions for a variety
of quantities. The first panel gives 2.5%, 50% and 97.5% posterior quantiles
for exp(Œ∏ T x). The second panel gives the same quantiles for the posterior
predictive distribution of exp(Œ≤ T x). The difference in the width of the confidence bands is due to the estimated across-mouse heterogeneity. If there were
no across-mouse heterogeneity then Œ£ would be zero, each Œ≤ j would be equal
to Œ∏ and the plots in the first two panels of the figure would be identical.
Finally, the third panel gives 2.5%, 50% and 97.5% quantiles of the posterior
predictive distribution of {Y |x} for each of the 20 values of x. The difference

11.5 Discussion and further references

207

between this plot and the one in the second panel is due to the variability
of a Poisson random variable Y around its expected value exp(Œ≤ T x). The
widening confidence bands of the three plots in this figure describe cumulative sources of uncertainty: The first panel shows the uncertainty in the fixed
but unknown value of Œ∏. The second panel shows this uncertainty in addition
to the uncertainty due to across-mouse heterogeneity. Finally, the third panel
includes both of these sources of uncertainty as well as that due to fluctuations of a mouse‚Äôs observed tumor counts around its own expected tumor
count function. Understanding these different sources of uncertainty can be
very relevant to inference and decision making: For example, if we want to
predict the observed tumor count distribution of a new mouse, we should use
the confidence bands in the third panel, whereas the bands in the first panel
would be appropriate if we just wanted to describe the uncertainty in the fixed
value of Œ∏.

11.5 Discussion and further references
Posterior approximation via MCMC for hierarchical models can suffer from
poor mixing. One reason for this is that many of the parameters in the model
are highly correlated, and generating them one at a time in the Gibbs sampler
can lead to a high degree of autocorrelation. For example, Œ∏ and the Œ≤ j ‚Äôs are
positively correlated, and so an extreme value of Œ∏ at one iteration can lead
to extreme values of the Œ≤ j ‚Äôs when they get updated, especially if the amount
of within-group data is low. This in turn leads to an extreme value of Œ∏ at the
next iteration. Section 15.4 of Gelman et al (2004) provides a detailed discussion of several alternative Gibbs sampling strategies for improving mixing for
hierarchical models. Improvements also can be made by careful reparameterizations of the model (Gelfand et al, 1995; Papaspiliopoulos et al, 2007).

12
Latent variable methods for ordinal data

Many datasets include variables whose distributions cannot be represented
by the normal, binomial or Poisson distributions we have studied thus far.
For example, distributions of common survey variables such as age, education
level and income generally cannot be accurately described by any of the abovementioned sampling models. Additionally, such variables are often binned into
ordered categories, the number of which may vary from survey to survey. In
such situations, interest often lies not in the scale of each individual variable,
but rather in the associations between the variables: Is the relationship between two variables positive, negative or zero? What happens if we ‚Äúaccount‚Äù
for a third variable? For normally distributed data these types of questions
can be addressed with the multivariate normal and linear regression models
of Chapters 7 and 9. In this chapter we extend these models to situations
where the data are not normal, by expressing non-normal random variables
as functions of unobserved, ‚Äúlatent‚Äù normally distributed random variables.
Multivariate normal and linear regression models then can be applied to the
latent data.

12.1 Ordered probit regression and the rank likelihood
Suppose we are interested in describing the relationship between the educational attainment and number of children of individuals in a population.
Additionally, we might suspect that an individual‚Äôs educational attainment
may be influenced by their parent‚Äôs education level. The 1994 General Social
Survey provides data on variables DEG, CHILD and PDEG for a sample of
individuals in the United States, where DEGi indicates the highest degree
obtained by individual i, CHILDi is their number of children and PDEGi is
the binary indicator of whether or not either parent of i obtained a college
degree. Using these data, we might be tempted to investigate the relationship
between the variables with a linear regression model:
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 12,
c Springer Science+Business Media, LLC 2009


210

12 Latent variable methods for ordinal data

DEGi = Œ≤1 + Œ≤2 √ó CHILDi + Œ≤3 √ó PDEGi + Œ≤4 √ó CHILDi √ó PDEGi + i ,

probability
0.20

0.0

0.00

0.1

0.10

probability
0.2
0.3

0.4

0.30

0.5

where we assume that 1 , . . . , n ‚àº i.i.d. normal(0, œÉ 2 ). However, such a model
would be inappropriate for a couple of reasons. Empirical distributions of DEG
and CHILD for a sample of 1,002 males in the 1994 workforce are shown in
Figure 12.1. The value of DEG is recorded as taking a value in {1, 2, 3, 4, 5}
corresponding to the highest degree of the respondent being no degree, high
school degree, associate‚Äôs degree, bachelor‚Äôs degree, or graduate degree.

1

2

3
DEG

4

5

0

1

2

3

4 5
CHILD

6

7

8

Fig. 12.1. Two ordinal variables having non-normal distributions.

Since the variable DEG takes on only a small set of discrete values, the
normality assumption of the residuals will certainly be violated. But perhaps
more importantly, the regression model imposes a numerical scale to the data
that is not really present: A bachelor‚Äôs degree is not ‚Äútwice as much‚Äù as a
high school degree, and an associate‚Äôs degree is not ‚Äútwo less‚Äù than a graduate
degree. There is an order to the categories in the sense that a graduate degree
is ‚Äúhigher‚Äù than a bachelor‚Äôs degree, but otherwise the scale of DEG is not
meaningful.
Variables for which there is a logical ordering of the sample space are
known as ordinal variables. With this definition, the discrete variables DEG
and CHILD are ordinal variables, as are ‚Äúcontinuous‚Äù variables like height or
weight. However, CHILD, height and weight are variables that are measured
on meaningful numerical scales, whereas DEG is not. In this chapter we will
use the term ‚Äúordinal‚Äù to refer to any variable for which there is a logical
ordering of the sample space. We will use the term ‚Äúnumeric‚Äù to refer to
variables that have meaningful numerical scales, and ‚Äúcontinuous‚Äù if a variable
can have a value that is (roughly) any real number in an interval. For example,

12.1 Ordered probit regression and the rank likelihood

211

DEG is ordinal but not numeric, whereas CHILD is ordinal, numeric and
discrete. Variables like height or weight are ordinal, numeric and continuous.
12.1.1 Probit regression
Linear or generalized linear regression models, which assume a numeric scale
to the data, may be appropriate for variables like CHILD, height or weight,
but are not appropriate for non-numeric ordinal variables like DEG. However,
it is natural to think of many ordinal, non-numeric variables as arising from
some underlying numeric process. For example, the severity of a disease might
be described ‚Äúlow‚Äù, ‚Äúmoderate‚Äù or ‚Äúhigh‚Äù, although we imagine a patient‚Äôs
actual condition lies within a continuum. Similarly, the amount of effort a
person puts into formal education may lie within a continuum, but a survey
may only record a rough, categorized version of this variable, such as DEG.
This idea motivates a modeling technique known as ordered probit regression,
in which we relate a variable Y to a vector of predictors x via a regression in
terms of a latent variable Z. More precisely, the model is
1 , . . . , n ‚àº i.i.d. normal(0, 1)
Zi = Œ≤ T xi + i
Yi = g(Zi ),

(12.1)
(12.2)

where Œ≤ and g are unknown parameters. For example, to model the conditional distribution of DEG given CHILD and PDEG we would let Yi be DEGi
and let xi = (CHILDi , PDEGi , CHILDi √óPDEGi ). The regression coefficients
Œ≤ describe the relationship between the explanatory variables and the unobserved latent variable Z, and the function g relates the value of Z to the
observed variable Y . The function g is taken to be non-decreasing, so that
we can interpret small and large values of Z as corresponding to small and
large values of Y . This also means that the sign of a regression coefficient Œ≤j
indicates whether Y is increasing or decreasing in xj .
Notice that in this probit regression model we have taken the variance
of 1 , . . . , n to be one. This is because the scale of the distribution of Y can
already be represented by g, as g is allowed to be any non-decreasing function.
Similarly, g can represent the location of the distribution of Y , and so we do
not need to include an intercept term in the model.
If the sample space for Y takes on K values, say {1, . . . , K}, then the
function g can be described with only K ‚àí 1 ordered parameters g1 < g2 <
¬∑ ¬∑ ¬∑ < gk‚àí1 as follows:
y = g(z) = 1 if ‚àí‚àû = g0 < z < g1
= 2 if
g1 < z < g2
..
.
= K if

gK‚àí1 < z < gK = ‚àû .

(12.3)

212

12 Latent variable methods for ordinal data

The values {g1 , . . . , gK‚àí1 } can be thought of as ‚Äúthresholds,‚Äù so that moving z past a threshold moves y into the next highest category. The unknown parameters in the model include the regression coefficients Œ≤ and the
thresholds g1 , . . . , gK‚àí1 . If we use normal prior distributions for these quantities, the joint posterior distribution of {Œ≤, g1 , . . . , gK‚àí1 , Z1 , . . . , Zn } given
Y = y = (y1 , . . . , yn ) can be approximated using a Gibbs sampler.
Full conditional distribution of Œ≤
Given Y = y, Z = z, and g = (g1 . . . , gK‚àí1 ), the full conditional distribution of Œ≤ depends only on z and satisfies p(Œ≤|y, z, g) ‚àù p(Œ≤)p(z|Œ≤).
Just as in ordinary regression, a multivariate normal prior distribution for
Œ≤ gives a multivariate normal posterior distribution. For example, if we use
Œ≤ ‚àº multivariate normal(0, n(XT X)‚àí1 ), then p(Œ≤|z) is multivariate normal
with
n
Var[Œ≤|z] =
(XT X)‚àí1 , and
n+1
n
E[Œ≤|z] =
(XT X)‚àí1 XT z.
n+1
Full conditional distribution of Z
The full conditional distributions of the Zi ‚Äôs are only slightly more complicated. Under the sampling model, the conditional distribution of Zi given Œ≤
is Zi ‚àº normal(Œ≤ T xi , 1). Given g, observing Yi = yi tells us that Zi must lie
in the interval (gyi ‚àí1 , gyi ). Letting a = gyi ‚àí1 and b = gyi , the full conditional
distribution of Zi given {Œ≤, y, g} is then
p(zi |Œ≤, y, g) ‚àù dnorm(zi , Œ≤ T xi , 1) √ó Œ¥(a,b) (zi ).
This is the density of a constrained normal distribution. To sample a value x
from a normal(¬µ, œÉ 2 ) distribution constrained to the interval (a, b), we perform
the following two steps:
1. sample u ‚àº uniform(Œ¶[(a ‚àí ¬µ)/œÉ], Œ¶[(b ‚àí ¬µ)/œÉ])
2. set x = ¬µ + œÉŒ¶‚àí1 (u)
where Œ¶ and Œ¶‚àí1 are the cdf and inverse-cdf of the standard normal distribution (given by pnorm and qnorm in R). Code to sample from the full
conditional distribution of Zi is as follows:
ez<‚àí t ( b e t a)%‚àó%X[ i , ]
a<‚àímax(‚àí I n f , g [ y [ i ] ‚àí 1 ] , na . rm=TRUE)
b<‚àímin ( g [ y [ i ] ] , I n f , na . rm=TRUE)
u<‚àír u n i f ( 1 , pnorm ( a‚àíe z ) , pnorm ( b‚àíe z ) )
z [ i ]<‚àí e z + qnorm ( u )

The added complexity in assigning a and b in the above code is to deal with
the special cases g0 = ‚àí‚àû and gK = ‚àû.

12.1 Ordered probit regression and the rank likelihood

213

Full conditional distribution of g
Suppose the prior distribution for g is some arbitrary density p(g). Given
Y = y and Z = z, we know from Equation 12.3 that gk must be higher
than all zi ‚Äôs for which yi = k and lower than all zi ‚Äôs for which yi = k + 1.
Letting ak = max{zi : yi = k} and bk = min{zi : yi = k + 1} the full
conditional distribution of g is then proportional to p(g) but constrained to
the set {g : ak < gk < bk }. For example, if p(g) is proportional to the product
QK‚àí1
k=1 dnorm(gk , ¬µk , œÉk ) but constrained so that g1 < ¬∑ ¬∑ ¬∑ < gk‚àí1 , then the
full conditional density of gk is a normal(¬µk , œÉk2 ) density constrained to the
interval (ak , bk ). R-code to sample from the full conditional distribution of gk
is given below:
a<‚àímax( z [ y==k ] )
b<‚àímin ( z [ y==k +1])
u<‚àír u n i f ( 1 , pnorm ( ( a‚àímu [ k ] ) / s i g [ k ] ) , pnorm ( ( b‚àímu [ k ] ) / s i g [ k ] ) )
g [ k]<‚àí mu [ k ] + s i g [ k ] ‚àó qnorm ( u )

Example: Educational attainment

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè

6

7
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

prior
posterior

5

‚óè
‚óè

PDEG=0
PDEG=1

‚óè

density
3
4

2
1
z

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè

‚óè

‚óè

0

‚àí3

0

‚óè
‚óè
‚óè
‚óè

2

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

1

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

3
‚àí1

‚óè
‚óè

‚àí2

4

Some researchers suggest that having children reduces opportunities for educational attainment (Moore and Waite, 1977). Here we examine this hypothesis in a sample of males in the labor force (meaning not retired, not
in school and not in an institution), obtained from the 1994 General Social
Survey. For 959 of the 1,002 survey respondents we have complete data on
the variables DEG, CHILD and PDEG described above. Letting Yi = DEGi

0

2

4
6
number of children

8

‚àí0.4

‚àí0.2

0.0
Œ≤3

Fig. 12.2. Results from the probit regression analysis.

0.2

0.4

214

12 Latent variable methods for ordinal data

and xi = (CHILDi , PDEGi , CHILDi √ó PDEGi ), we will estimate the parameters in the ordered probit regression model using
prior distributions of Œ≤ ‚àº
QK‚àí1
multivariate normal(0, n(XT X)‚àí1 ) and p(g) ‚àù k=1 dnorm(gk , 0, 100) but
constrained so that g1 < ¬∑ ¬∑ ¬∑ < gK‚àí1 . We‚Äôll approximate the corresponding
posterior distribution of {Œ≤, Z, g} with a Gibbs sampler consisting of 25,000
scans. Saving parameter values every 25th scan results in 1,000 values for each
parameter with which to approximate the posterior distribution. The posterior
mean regression line for people without a college-educated parent (xi,2 = 0)
is E[Z|y, x1 , x2 = 0] = ‚àí0.024 √ó x1 while the regression line for people with a
college-educated parent is E[Z|y, x1 , x2 = 1] = 0.818 + 0.054 √ó x1 . These lines
are shown in the first panel of Figure 12.2, along with the value of Z that
was obtained in the last scan of the Gibbs sampler. The lines suggest that
for people whose parents did not go to college, the number of children they
have is indeed weakly negatively associated with their educational outcome.
However, the opposite seems to be true among people whose parents went to
college. The posterior distribution of Œ≤3 is given in the second panel of the figure, along with the prior distribution for comparison. The 95% quantile-based
posterior confidence interval for Œ≤3 is (-0.026,0.178), which contains zero but
still represents a reasonable amount of evidence that the slope for the x2 = 1
group is larger than that of the x2 = 0 group.
12.1.2 Transformation models and the rank likelihood
The analysis of the educational attainment data above required us to specify a
prior distribution for Œ≤ and the transformation g(z), as specified by the vector
g of K ‚àí 1 threshold parameters. While simple default prior distributions for
Œ≤ exist (such as Zellner‚Äôs g-prior), the same is not true for g. Coming up with
a prior distribution for g that represents actual prior information seems like a
difficult task. Of course, this task is much harder if the number of categories
K is large. For example, the incomes (INC) of the subjects in the 1994 GSS
dataset were each recorded as belonging to one of 21 ordered categories, so that
a regression in which Yi = INCi would require that g includes 20 parameters.
Estimation and prior specification for such a large number of parameters can
be difficult.
Fortunately there is an alternative approach to estimating Œ≤ that does not
require us to estimate the function g(z). Note that if the Zi ‚Äôs were observed
directly, then we could ignore Equation (12.2) of the model and we would
be left with an ordinary regression problem without having to estimate the
transformation g(z). Unfortunately we do not observe the Zi ‚Äôs directly, but
there is information in the data about the Zi ‚Äôs that does not require us to
specify g(z): Since we know that g is non-decreasing, we do know something
about the order of the Zi ‚Äôs. For example, if our observed data are such that
y1 > y2 , then since yi = g(Zi ), we know that g(Z1 ) > g(Z2 ). Since g is
non-decreasing, this means that we know Z1 > Z2 . In other words, having
observed Y = y, we know that the Zi ‚Äôs must lie in the set

12.1 Ordered probit regression and the rank likelihood

215

R(y) = {z ‚àà Rn : zi1 < zi2 if yi1 < yi2 }.
Since the distribution of the Zi ‚Äôs does not depend on g, the probability that
Z ‚àà R(y) for a given y also does not depend on the unknown function g. This
suggests that we base our posterior inference on the knowledge that Z ‚àà R(y).
Our posterior distribution for Œ≤ in this case is given by
p(Œ≤|Z ‚àà R(y)) ‚àù p(Œ≤) √ó Pr(Z ‚àà R(y)|Œ≤)
Z
n
Y
= p(Œ≤) √ó
dnorm(zi , Œ≤ T xi , 1) dzi .
R(y) i=1

As a function of Œ≤, the probability Pr(Z ‚àà R(y)|Œ≤) is known as the rank
likelihood . For continuous y-variables this likelihood was introduced by Pettitt
(1982) and its theoretical properties were studied by Bickel and Ritov (1997).
It is called a rank likelihood because for continuous data it contains the same
information about y as knowing the ranks of {y1 , . . . , yn }, i.e. which one has
the highest value, which one has the second highest value, etc. If Y is discrete
then observing R(y) is not exactly the same as knowing the ranks, but for
simplicity we will still refer to Pr(Z ‚àà R(y)|Œ≤) as the rank likelihood, whether
or not Y is discrete or continuous. The important thing to note is that for any
ordinal outcome variable Y (non-numeric, numeric, discrete or continuous),
information about Œ≤ can be obtained from Pr(Z ‚àà R(y)|Œ≤) without having
to specify g(z).
For any given Œ≤ the value of Pr(Z ‚àà R(y)|Œ≤) involves a very complicated
integral that is difficult to compute. However, by estimating Z simultaneously
with Œ≤ we can obtain an estimate of Œ≤ without ever having to numerically
compute Pr(Z ‚àà R(y)|Œ≤). The joint posterior distribution of {Œ≤, Z} can be
approximated by using Gibbs sampling, alternately sampling from full conditional distributions. The full conditional distribution of Œ≤ is very easy: Given
a current value z of Z, the full conditional density p(Œ≤|Z = z, Z ‚àà R(y)) reduces to p(Œ≤|Z = z) because knowing the value of Z is more informative than
knowing just that Z lies in the set R(y). A multivariate normal prior distribution for Œ≤ then results in a multivariate normal full conditional distribution,
as before. The full conditional distributions of the Zi ‚Äôs are also straightforward to derive. Let‚Äôs consider the full conditional distribution of Zi given
{Œ≤, Z ‚àà R(y), z ‚àíi }, where z ‚àíi denotes the values of all of the Z‚Äôs except Zi .
Conditional on Œ≤, Zi is normal(Œ≤ T xi , 1). Conditional on {Œ≤, Z ‚àà R(y), z ‚àíi },
the density of Zi is proportional to a normal density but constrained by the
fact that Z ‚àà R(y). Let‚Äôs recall the nature of this constraint: yi < yj implies
Zi < Zj , and yi > yj implies Zi > Zj . This means that Zi must lie in the
following interval:
max{zj : yj < yi } < Zi < min{zj : yi < yj } .
Letting a and b denote the numerical values of the lower and upper endpoints
of this interval, the full conditional distribution of Zi is then

216

12 Latent variable methods for ordinal data

p(zi |Œ≤, Z ‚àà R(y), z ‚àíi ) ‚àù dnorm(zi , Œ≤ T xi , 1) √ó Œ¥(a,b) (zi ).
This full conditional distribution is exactly the same as that of Zi in the ordered probit model, except that now the constraints on Zi are determined
directly by the current value Z ‚àíi , instead of on the threshold variables. As
such, sampling from this full conditional distribution is very similar to sampling from the analogous distribution in the probit regression model:
ez<‚àí t ( b e t a)%‚àó%X[ i , ]
a<‚àímax( z [ y<y [ i ] ] )
b<‚àímin ( z [ y [ i ]<y ] )
u<‚àír u n i f ( 1 , pnorm ( a‚àíe z ) , pnorm ( b‚àíe z ) )
z [ i ]<‚àí e z + qnorm ( u )

‚àí0.15

‚àí0.05 0.05
Œ≤1

2
0

0

0.0

2

4

1.0

4

density
6 8

2.0

6

3.0

10 12

Not surprisingly, for the educational attainment data the posterior distribution of Œ≤ based on the rank likelihood is very similar to the one based on
the full ordered probit model. The three panels of Figure 12.3 indicate that
the marginal posterior densities of Œ≤1 , Œ≤2 and Œ≤3 are nearly identical under
the two models. In general, if K is small and n is large, we expect the two
methods to behave similarly. However, the rank likelihood approach is applicable to a wider array of datasets since with this approach, Y is allowed to be
any type of ordinal variable, discrete or continuous. The drawback to using
the rank likelihood is that it does not provide us with inference about g(z),
which describes the relationship between the latent and observed variables. If
this parameter is of interest, then the rank likelihood is not appropriate, but
if interest lies only in Œ≤, then the rank likelihood provides a simple alternative
to the ordered probit model.

0.4

0.8
Œ≤2

1.2

‚àí0.1

0.1
Œ≤3

0.2

0.3

Fig. 12.3. Marginal posterior distributions of (Œ≤1 , Œ≤2 , Œ≤3 ), under the ordinal probit
regression model (in gray) and the rank likelihood (in black).

12.2 The Gaussian copula model

217

12.2 The Gaussian copula model
The regression model above is somewhat limiting because it only describes
the conditional distribution of one variable given the others. In general, we
may be interested in the relationships among all of the variables in a dataset.
If the variables were approximately jointly normally distributed, or at least
were all measured on a meaningful numerical scale, then we could describe
the relationships among the variables with the sample covariance matrix or a
multivariate normal model. However, such a model is inappropriate for nonnumeric ordinal variables like INC, DEG and PDEG. To accommodate variables such as these we can extend the ordered probit model above to a latent,
multivariate normal model that is appropriate for all types of ordinal data,
both numeric and non-numeric. Letting Y 1 , . . . , Y n be i.i.d. random samples
from a p-variate population, our latent normal model is
Z 1 , . . . , Z n ‚àº i.i.d. multivariate normal(0, Œ® )
Yi,j = gj (Zi,j ) ,

(12.4)
(12.5)

where g1 , . . . , gp are non-decreasing functions and Œ® is a correlation matrix,
having diagonal entries equal to 1. In this model, the matrix Œ® represents the
joint dependencies among the variables and the functions g1 , . . . , gp represent
their marginal distributions. To see how the gj ‚Äôs represent the margins, let‚Äôs
calculate the marginal cdf Fj (y) of a continuous random variable Yi,j under
the model given by Equations 12.4 and 12.5. Recalling the definition of the
cdf, we have
Fj (y) = Pr(Yi,j ‚â§ y)
= Pr(gj (Zi,j ) ‚â§ y) , since Yi,j = gj (Zi,j )
= Pr(Zi,j ‚â§ gj‚àí1 (y))
= Œ¶(gj‚àí1 (y)),
where Œ¶(z) is the cdf of the standard normal distribution. The last line holds
because the diagonal entries of Œ® are all equal to 1, and so the marginal
distribution of each Zi,j is a standard normal distribution with cdf Œ¶(z).
The above calculations show that Fj (y) = Œ¶(gj‚àí1 (y)), indicating that the
marginal distributions of the Yj ‚Äôs are fully determined by the gj ‚Äôs and do
not depend on the matrix Œ® . A model having separate parameters for the
univariate marginal distributions and the multivariate dependencies is generally called a copula model . The model given by Equations 12.4 and 12.5,
where the dependence is described by a multivariate normal distribution, is
called the multivariate normal copula model . The term ‚Äúcopula‚Äù refers to the
method of ‚Äúcoupling‚Äù a model for multivariate dependence (such as the multivariate normal distribution) to a model for the marginal distributions of
the data. As shown above, a copula model separates the parameters for the
dependencies among the variables (Œ® ) from the parameters describing their

218

12 Latent variable methods for ordinal data

univariate marginal distributions (g1 , . . . , gp ). This separation comes in handy
if we are primarily interested in the dependencies among the variables and not
the univariate scales on which they were measured. In this case, the functions
g1 , . . . , gp are nuisance parameters and the parameter of interest is Œ® . Using
an extension of the rank likelihood described in the previous section, we will
be able to obtain a posterior distribution for Œ® without having to estimate or
specify prior distributions for the nuisance parameters g1 , . . . , gp .
12.2.1 Rank likelihood for copula estimation
The unknown parameters in the copula model are the matrix Œ® and the nondecreasing functions g1 , . . . , gp . Bayesian inference for all of these parameters
would require that we specify a prior for Œ® as well as p prior distributions
over the complicated space of arbitrary non-decreasing functions. If we are
not interested in g1 , . . . , gp then we can use a version of the rank likelihood
which quantifies information about Z 1 , . . . , Z n without having to specify these
nuisance parameters. Recall that since each gj is non-decreasing, observing the
n √ó p data matrix Y tells us that the matrix of latent variables Z must lie in
the set
R(Y) = {Z : zi1 ,j < zi2 ,j if yi1 ,j < yi2 ,j }.
(12.6)
The probability of this event, Pr(Z ‚àà R(Y)|Œ® ), does not depend on g1 , . . . , gp .
As a function of Œ® , Pr(Z ‚àà R(Y)|Œ® ) is called the rank likelihood for the
multivariate normal copula model. Computing the likelihood for a given value
of Œ® is very difficult, but as with the regression model in Section 12.1.2 we can
make an MCMC approximation to p(Œ®, Z|Z ‚àà R(Y)) using Gibbs sampling,
provided we use a prior for Œ® based on the inverse-Wishart distribution.
A parameter-expanded prior distribution for Œ®
Unfortunately there is no simple conjugate class of prior distributions for our
correlation matrix Œ® . As an alternative, let‚Äôs consider altering Equation 12.4
to be
Z 1 , . . . , Z n ‚àº i.i.d. multivariate normal(0, Œ£),
where Œ£ is an arbitrary covariance matrix, not restricted to be a correlation
matrix like Œ® . In this case a natural prior distribution for Œ£ would be an
inverse-Wishart distribution, which would give an inverse-Wishart full conditional distribution and thus make posterior inference available via Gibbs
sampling. However, careful inspection of the rank likelihood indicates that
it does not provide us with a complete estimate of Œ£. Specifically, the rank
likelihood contains only information about the relative ordering among the
Zi,j ‚Äôs, and no information about their scale. For example, if Z1,j and Z2,j
are two i.i.d. samples from a normal(0, œÉj2 ) distribution, then the probability
that Z1,j < Z2,j does not depend on œÉj2 . For this reason we say that the diagonal entries of Œ£ are non-identifiable in this model, meaning that the rank

12.2 The Gaussian copula model

219

likelihood provides no information about what the diagonal should be. In a
Bayesian analysis, the posterior distribution of any non-identifiable parameter
is determined by the prior distribution, and so in some sense the posterior distribution of such a parameter is not of interest. However, to each covariance
matrix Œ£ there corresponds a unique correlation matrix Œ® , obtained by the
function
q
Œ® = h(Œ£) = {œÉi,j / œÉi2 œÉj2 }.
The value of Œ® is identifiable from the rank likelihood, and so one estimation approach for the Gaussian copula model is to reparameterize the model
in terms of a non-identifiable covariance matrix Œ£, but focus our posterior
inference on the identifiable correlation matrix Œ® = h(Œ£). This technique of
modeling in terms of a non-identifiable parameter in order to simplify calculations is referred to as parameter expansion (Liu and Wu, 1999), and has been
used in the context of modeling multivariate ordinal data by Hoff (2007) and
Lawrence et al (2008).
To summarize, we will base our posterior distribution on
Œ£ ‚àº inverse-Wishart(ŒΩ0 , S‚àí1
0 )
Z 1 , . . . , Z n ‚àº i.i.d. multivariate normal(0, Œ£)
Yi,j = gj (Zi,j ),

(12.7)

but our estimation and inference will be restricted to Œ® = h(Œ£). Interestingly,
the posterior distribution for Œ® obtained from this prior and model is exactly
the same as that which would be obtained from the following:
Œ£
Œ®
Z 1, . . . , Z n
Yi,j

‚àº inverse-Wishart(ŒΩ0 , S‚àí1
0 )
= h(Œ£)
‚àº i.i.d. multivariate normal(0, Œ® )
= gj (Zi,j ).

(12.8)

In other words, the non-identifiable model described in Equation 12.7 gives the
same posterior distribution for Œ® as the identifiable model in Equation 12.8 in
which the prior distribution for Œ® is defined by {Œ£ ‚àº inverse-Wishart(ŒΩ0 , S‚àí1
0 ) ,
Œ® = h(Œ£)}. The only difference is that the Gibbs sampling scheme for Equation 12.7 is easier to formulate. The equivalence of these two models relies on
the scale invariance of the rank likelihood, and so will not generally hold for
other types of models involving correlation matrices.
Full conditional distribution of Œ£
If the prior distribution for Œ£ is inverse-Wishart(ŒΩ0 , S‚àí1
0 ), then, as described
in Section 7.3, the full conditional distribution of Œ£ is inverse-Wishart as well.
We review this fact here by first noting that the probability density of the n√óp
matrix Z can be written as

220

12 Latent variable methods for ordinal data

p(Z|Œ£) =

n
Y

1
(2œÄ)‚àíp/2 |Œ£|‚àí1/2 exp{‚àí z i Œ£ ‚àí1 z i }
2
i=1

= (2œÄ)‚àínp/2 |Œ£|‚àín/2 exp{‚àítr(ZT ZŒ£ ‚àí1 )/2},
where ‚Äútr(A)‚Äù stands for the trace of matrix A, which is the sum of the
diagonal elements of A. The full conditional distribution p(Œ£|Z, Z ‚àà R(Y)) =
p(Œ£|Z) is then given by
p(Œ£|Z) ‚àù p(Œ£) √ó p(Z|Œ£)
‚àù |Œ£|‚àí(ŒΩ0 +p+1)/2 exp{‚àítr(S0 Œ£ ‚àí1 )/2} √ó |Œ£|‚àín/2 exp{‚àítr(ZT ZŒ£ ‚àí1 )/2}
= |Œ£|‚àí([ŒΩ0 +n]+p+1)/2 exp{‚àítr([S0 + ZT Z]Œ£ ‚àí1 /2}
which is proportional to an inverse-Wishart(ŒΩ0 + n, [S0 + ZT Z]‚àí1 ) density.
Full conditional distribution of Z
Recall from Section 5 of Chapter 7 that if Z is a random multivariate
normal(0, Œ£) vector, then the conditional distribution of Zj , given the other
elements Z ‚àíj = z ‚àíj , is a univariate normal distribution with mean and variance given by
E[Zj |Œ£, z ‚àíj ] = Œ£j,‚àíj (Œ£‚àíj,‚àíj )‚àí1 z ‚àíj
Var[Zj |Œ£, z ‚àíj ] = Œ£j,j ‚àí Œ£j,‚àíj (Œ£‚àíj,‚àíj )‚àí1 Œ£‚àíj,j ,
where Œ£j,‚àíj refers to the jth row of Œ£ with the jth column removed, and
Œ£‚àíj,‚àíj refers to Œ£ with both the jth row and column removed. If in addition
we condition on the information that Z ‚àà R(Y), then we know that
max{zk,j : yk,j < yi,j } < Zi,j < min{zk,j : yi,j < yk,j }.
These two pieces of information imply that the full conditional distribution
of Zi,j is a constrained normal distribution, which can be sampled from using
the procedure described in the previous section and in the following R-code:
Sz<‚àí Sigma [ j ,‚àí j ]%‚àó% s o l v e ( Sigma[‚àí j ,‚àí j ] )
sz<‚àí s q r t ( Sigma [ j , j ] ‚àí S j c%‚àó%Sigma[‚àí j , j ] )
ez<‚àí Z [ i ,‚àí j ]%‚àó%t ( S j c )
a<‚àímax( Z [ Y[ i , j ]>Y[ , j ]
b<‚àímin ( Z [ Y[ i , j ]<Y[ , j ]

, j ] , na . rm=TRUE)
, j ] , na . rm=TRUE)

u<‚àír u n i f ( 1 , pnorm ( ( a‚àíe z ) / s z ) ,
Z [ i , j ]<‚àí e z + s z ‚àóqnorm ( u )

pnorm ( ( b‚àíe z ) / s z )

)

12.2 The Gaussian copula model

221

Missing data
The expression na.rm=TRUE in the above code allows for the possibility
of missing data. Instead of throwing out the rows of the data matrix that
contain some missing values, we would like to use all of the data we can. If
the values are missing-at-random as described in Section 7.5, then they can
simply be treated as unknown parameters and their values imputed using the
Gibbs sampler. For the Gaussian copula model, this imputation happens at
the level of the latent variables. For example, suppose that variable j for case
i is not recorded, i.e. yi,j is not available. As described above, the full conditional distribution of Zi,j given Z i,‚àíj is normal. If yi,j were observed then the
conditional distribution of Zi,j would be a constrained normal, as observing
yi,j imposes a constraint on the allowable values of Zi,j . But if yi,j is missing then no such constraint is imposed, and the full conditional distribution
of Zi,j is simply the original unconstrained normal distribution. The R-code
above handles this as follows: If Yi,j is missing, then Z[ Y[i , j]>Y[,j ] , j ]
is a vector of missing values. The option na.rm=TRUE removes all of these
missing values, so a is the maximum of an empty set, which is defined to be
‚àí‚àû. Similarly, b will be set to ‚àû.
Example: Social mobility data
The results of the probit regression of DEG on the variables PDEG and
CHILD in the last section indicate that the educational level of an individual
is related to that of their parents. In this section we analyze this further, by
examining the joint relationships among respondent-specific variables DEG,
CHILD, INC along with analogous parent-specific variables PDEG, PCHILD
and PINC. In this data analysis PDEG is a five-level categorical variable with
the same levels as DEG, recording the highest degree of the respondent‚Äôs
mother or father. PCHILD is the number of siblings of the respondent, and
so is roughly the number of children of the respondent‚Äôs parents. The variable
PINC is a five-level ordered categorical variable recording the respondent‚Äôs
parent‚Äôs financial status when the respondent was 16 years of age. Finally,
we also include AGE, the respondent‚Äôs age in years. Although not of primary
interest, heterogeneity in a person‚Äôs income, number of children and degree
category is likely to be related to age.
Using an inverse-Wishart(p + 2, (p + 2) √ó I) prior distribution for Œ£ having
a prior mean of E[Œ£] = I, we can implement a Gibbs sampling algorithm using
the full conditional distributions described above. Iterating the algorithm for
25,000 scans, saving parameter values every 25th scan, gives a total of 1,000
values of each parameter with which to approximate the posterior distribution
of Œ® = h(Œ£). The Monte Carlo estimate of the posterior mean of Œ® is

222

12 Latent variable methods for ordinal data

Ô£´

Ô£∂
1.00 0.48 0.29 0.13 0.17 ‚àí0.05 0.34
Ô£¨ 0.48 1.00 ‚àí0.04 0.20 0.46 ‚àí0.21 0.05 Ô£∑
Ô£¨
Ô£∑
Ô£¨ 0.29 ‚àí0.04 1.00 ‚àí0.15 ‚àí0.25 0.22 0.59 Ô£∑
Ô£¨
Ô£∑
Ô£∑
E[Œ® |y 1 , . . . , y n ] = Ô£¨
Ô£¨ 0.13 0.20 ‚àí0.15 1.00 0.44 ‚àí0.22 ‚àí0.13 Ô£∑ ,
Ô£¨ 0.17 0.46 ‚àí0.25 0.44 1.00 ‚àí0.29 ‚àí0.23 Ô£∑
Ô£¨
Ô£∑
Ô£≠ ‚àí0.05 ‚àí0.21 0.22 ‚àí0.22 ‚àí0.29 1.00 0.12 Ô£∏
0.34 0.05 0.59 ‚àí0.13 ‚àí0.23 0.12 1.00
where the columns and rows are, in order, INC, DEG, CHILD, PINC, PDEG,
PCHILD and AGE. We also may be interested in the ‚Äúregression coefficients‚Äù
Œ≤ j|‚àíj = Œ®j,‚àíj (Œ®‚àíj,‚àíj )‚àí1 , which for each variable j is a vector of length j ‚àí 1
that describes how the conditional mean of Zj depends on the remaining
variables Z‚àíj . Figure 12.4 summarizes the posterior distributions of each Œ≤ j|‚àíj
(except for that of AGE) as follows: A 95% quantile-based confidence interval
is obtained for each Œ≤ j,k . If the confidence interval does not contain zero, a
line between variables j and k is drawn, with a ‚Äú+‚Äù or a ‚Äú‚àí‚Äù indicating the
sign of the posterior median. If the interval does contain zero, no line is drawn
between the variables.
Such a graph is sometimes referred to as a dependence graph, which summarizes the conditional dependencies among the variables. Roughly speaking,
two variables in the graph are conditionally independent given the other variables if there is no line between them. More precisely, the absence of a line
indicates the lack of strong evidence of a conditional dependence. For example, although there is a positive marginal dependence between INC and PINC,
the graph indicates that there is little evidence of any conditional dependence,
given the other variables.

+
DEG

PDEG

+

+
‚àí
‚àí

‚àí

PINC

‚àí

INC

+
+

CHILD

PCHILD

Fig. 12.4. Reduced conditional dependence graph for the GSS data.

12.3 Discussion and further references

223

12.3 Discussion and further references
Normally distributed latent variables are often used to induce dependence
among a set of non-normal observed variables. For example, Chib and Winkelmann (2001) present a model for a vector of correlated count data in which
each component is a Poisson random variable with a mean depending on a
component-specific latent variable. Dependence among the count variables is
induced by modeling the vector of latent variables with a multivariate normal
distribution. Similar approaches are proposed by Dunson (2000) and described
in Chapter 8 of Congdon (2003). Pitt et al (2006) discuss Bayesian inference
for Gaussian copula models when the margins are known parametric families,
and Quinn (2004) presents a factor analysis model for mixed continuous and
discrete outcomes, in which the continuous variables are treated parametrically.
Pettitt (1982) develops the rank likelihood to estimate parameters in a
latent normal regression model, allowing the transformation from the latent
data to continuous observed data to be treated nonparametrically. Hoff (2007)
extends this type of likelihood to accommodate both continuous and discrete
ordinal data, and provides a Gibbs sampler for parameter estimation in a
semiparametric Gaussian copula model.
The rank likelihood is based on the marginal distribution of the ranks,
and so is called a marginal likelihood. Marginal likelihoods are typically constructed so that they use the information in the data that depends only on
the parameters of interest, and do not use any information that depends on
nuisance parameters. Marginal likelihoods do not generally provide efficient
estimation, as they throw away part of the information in the data. However, they can turn a very difficult semiparametric estimation problem into
essentially a parametric one. The use of marginal likelihoods in the context
of Bayesian estimation is discussed in Monahan and Boos (1992).

Exercises

Chapter 2
2.1 Marginal and conditional probability: The social mobility data from Section 2.5 gives a joint probability distribution on (Y1 , Y2 )= (father‚Äôs occupation, son‚Äôs occupation). Using this joint distribution, calculate the
following distributions:
a) the marginal probability distribution of a father‚Äôs occupation;
b) the marginal probability distribution of a son‚Äôs occupation;
c) the conditional distribution of a son‚Äôs occupation, given that the father
is a farmer;
d) the conditional distribution of a father‚Äôs occupation, given that the
son is a farmer.
2.2 Expectations and variances: Let Y1 and Y2 be two independent random
variables, such that E[Yi ] = ¬µi and Var[Yi ] = œÉi2 . Using the definition of
expectation and variance, compute the following quantities, where a1 and
a2 are given constants:
a) E[a1 Y1 + a2 Y2 ] , Var[a1 Y1 + a2 Y2 ];
b) E[a1 Y1 ‚àí a2 Y2 ] , Var[a1 Y1 ‚àí a2 Y2 ].
2.3 Full conditionals: Let X, Y, Z be random variables with joint density (discrete or continuous) p(x, y, z) ‚àù f (x, z)g(y, z)h(z). Show that
a) p(x|y, z) ‚àù f (x, z), i.e. p(x|y, z) is a function of x and z;
b) p(y|x, z) ‚àù g(y, z), i.e. p(y|x, z) is a function of y and z;
c) X and Y are conditionally independent given Z.
2.4 Symbolic manipulation: Prove the following form of Bayes‚Äô rule:
Pr(E|Hj ) Pr(Hj )
Pr(Hj |E) = PK
k=1 Pr(E|Hk ) Pr(Hk )
where E is any event and {H1 , . . . , HK } form a partition. Prove this using
only axioms P1-P3 from this chapter, by following steps a)-d) below:
a) Show that Pr(Hj |E) Pr(E) = Pr(E|Hj ) Pr(Hj ).
P.D. Hoff, A First Course in Bayesian Statistical Methods,
Springer Texts in Statistics, DOI 10.1007/978-0-387-92407-6 BM2,
c Springer Science+Business Media, LLC 2009


226

2.5

2.6

2.7

2.8

Exercises

b) Show that Pr(E) = Pr(E ‚à© H1 ) + Pr(E ‚à© {‚à™K
k=2 Hk }).
PK
c) Show that Pr(E) = k=1 Pr(E ‚à© Hk ).
d) Put it all together to show Bayes‚Äô rule, as described above.
Urns: Suppose urn H is filled with 40% green balls and 60% red balls, and
urn T is filled with 60% green balls and 40% red balls. Someone will flip
a coin and then select a ball from urn H or urn T depending on whether
the coin lands heads or tails, respectively. Let X be 1 or 0 if the coin lands
heads or tails, and let Y be 1 or 0 if the ball is green or red.
a) Write out the joint distribution of X and Y in a table.
b) Find E[Y ]. What is the probability that the ball is green?
c) Find Var[Y |X = 0], Var[Y |X = 1] and Var[Y ]. Thinking of variance as
measuring uncertainty, explain intuitively why one of these variances
is larger than the others.
d) Suppose you see that the ball is green. What is the probability that
the coin turned up tails?
Conditional independence: Suppose events A and B are conditionally independent given C, which is written A‚ä•B|C. Show that this implies that
Ac ‚ä•B|C, A‚ä•B c |C, and Ac ‚ä•B c |C, where Ac means ‚Äúnot A.‚Äù Find an
example where A‚ä•B|C holds but A‚ä•B|C c does not hold.
Coherence of bets: de Finetti thought of subjective probability as follows:
Your probability p(E) for event E is the amount you would be willing to
pay or charge in exchange for a dollar on the occurrence of E. In other
words, you must be willing to
‚Ä¢ give p(E) to someone, provided they give you $1 if E occurs;
‚Ä¢ take p(E) from someone, and give them $1 if E occurs.
Your probability for the event E c =‚Äúnot E‚Äù is defined similarly.
a) Show that it is a good idea to have p(E) ‚â§ 1.
b) Show that it is a good idea to have p(E) + p(E c ) = 1.
Interpretations of probability: One abstract way to define probability is
via measure theory, in that Pr(¬∑) is simply a ‚Äúmeasure‚Äù that assigns mass
to various events. For example, we can ‚Äúmeasure‚Äù the number of times a
particular event occurs in a potentially infinite sequence, or we can ‚Äúmeasure‚Äù our information about the outcome of an unknown event. The above
two types of measures are combined in de Finetti‚Äôs theorem, which tells
us that an exchangeable model for an infinite binary sequence Y1 , Y2 , . . .
is equivalent to modeling the sequence as conditionally i.i.d. given a parameter Œ∏, where Pr(Œ∏ < c) represents our information that the long-run
frequency of 1‚Äôs is less than c. With this in mind, discuss the different
ways in which probability could be interpreted in each of the following
scenarios. Avoid using the word ‚Äúprobable‚Äù or ‚Äúlikely‚Äù when describing
probability. Also discuss the different ways in which the events can be
thought of as random.
a) The distribution of religions in Sri Lanka is 70% Buddhist, 15% Hindu,
8% Christian, and 7% Muslim. Suppose each person can be identified

Exercises

227

by a number from 1 to K on a census roll. A number x is to be
sampled from {1, . . . , K} using a pseudo-random number generator
on a computer. Interpret the meaning of the following probabilities:
i. Pr(person x is Hindu);
ii. Pr(x = 6452859);
iii. Pr(Person x is Hindu|x=6452859).
b) A quarter which you got as change is to be flipped many times. Interpret the meaning of the following probabilities:
i. Pr(Œ∏, the long-run relative frequency of heads, equals 1/3);
ii. Pr(the first coin flip will result in a heads);
iii. Pr(the first coin flip will result in a heads | Œ∏ = 1/3).
c) The quarter above has been flipped, but you have not seen the outcome. Interpret Pr(the flip has resulted in a heads).

Chapter 3
3.1 Sample survey: Suppose we are going to sample 100 individuals from
a county (of size much larger than 100) and ask each sampled person
whether they support policy Z or not. Let Yi = 1 if person i in the sample
supports the policy, and Yi = 0 otherwise.
a) Assume Y1 , . . . , Y100 are, conditional on Œ∏, i.i.d. binary random variables with expectation Œ∏. Write down the joint distribution of Pr(Y1 =
y1 , .P
. . , Y100 = y100 |Œ∏) in a compact form. Also write down the form of
Pr( Yi = y|Œ∏).
b) For the moment, suppose you believed that Œ∏ ‚àà {0.0, 0.1, . . . , 0.9, 1.0}.
P100
Given
i=1 Yi = 57, compute
P that the results of the survey were
Pr( Yi = 57|Œ∏) for each of these 11 values of Œ∏ and plot these probabilities as a function of Œ∏.
c) Now suppose you originally had no prior information to believe one of
these Œ∏-values over another, and so Pr(Œ∏ = 0.0) = Pr(Œ∏ = 0.1)
Pn= ¬∑ ¬∑ ¬∑ =
Pr(Œ∏ = 0.9) = Pr(Œ∏ = 1.0). Use Bayes‚Äô rule to compute p(Œ∏| i=1 Yi =
57) for each Œ∏-value. Make a plot of this posterior distribution as a
function of Œ∏.
d) Now suppose you allow Œ∏ to be any value in the interval [0, 1]. Using
the uniform prior P
density for Œ∏, so that p(Œ∏) = 1, plot the posterior
n
density p(Œ∏) √ó Pr( i=1 Yi = 57|Œ∏) as a function of Œ∏.
e) As discussed in this chapter, the posterior distribution of Œ∏ is beta(1 +
57, 1 + 100 ‚àí 57). Plot the posterior density as a function of Œ∏. Discuss
the relationships among all of the plots you have made for this exercise.
3.2 Sensitivity analysis: It is sometimes useful to express the parameters a
and b in a beta distribution in terms of Œ∏0 = a/(a + b) and n0 = a + b,
so that a = Œ∏0 n0 and b = (1 ‚àí Œ∏0 )n0 . Reconsidering the sample survey
data in Exercise 3.1, for each combination of Œ∏0 ‚àà {0.1, 0.2, . . . , 0.9} and
n0 ‚àà {1, 2, 8, 16, 32} find the corresponding a, b values and compute Pr(Œ∏ >

228

Exercises

P
0.5| Yi = 57) using a beta(a, b) prior distribution for Œ∏. Display the
results with a contour plot, and discuss how the plot could be used to
explain to someone whether
P100 or not they should believe that Œ∏ > 0.5,
based on the data that i=1 Yi = 57.
3.3 Tumor counts: A cancer laboratory is estimating the rate of tumorigenesis
in two strains of mice, A and B. They have tumor count data for 10 mice
in strain A and 13 mice in strain B. Type A mice have been well studied,
and information from other laboratories suggests that type A mice have
tumor counts that are approximately Poisson-distributed with a mean of
12. Tumor count rates for type B mice are unknown, but type B mice are
related to type A mice. The observed tumor counts for the two populations
are
y A = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6);
y B = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7).
a) Find the posterior distributions, means, variances and 95% quantilebased confidence intervals for Œ∏A and Œ∏B , assuming a Poisson sampling
distribution for each group and the following prior distribution:
Œ∏A ‚àº gamma(120,10), Œ∏B ‚àº gamma(12,1), p(Œ∏A , Œ∏B ) = p(Œ∏A )√óp(Œ∏B ).
b) Compute and plot the posterior expectation of Œ∏B under the prior distribution Œ∏B ‚àº gamma(12√ón0 , n0 ) for each value of n0 ‚àà {1, 2, . . . , 50}.
Describe what sort of prior beliefs about Œ∏B would be necessary in order for the posterior expectation of Œ∏B to be close to that of Œ∏A .
c) Should knowledge about population A tell us anything about population B? Discuss whether or not it makes sense to have p(Œ∏A , Œ∏B ) =
p(Œ∏A ) √ó p(Œ∏B ).
3.4 Mixtures of beta priors: Estimate the probability Œ∏ of teen recidivism
based on a study in which there were n = 43 individuals released from
incarceration and y = 15 re-offenders within 36 months.
a) Using a beta(2,8) prior for Œ∏, plot p(Œ∏), p(y|Œ∏) and p(Œ∏|y) as functions
of Œ∏. Find the posterior mean, mode, and standard deviation of Œ∏.
Find a 95% quantile-based confidence interval.
b) Repeat a), but using a beta(8,2) prior for Œ∏.
c) Consider the following prior distribution for Œ∏:
p(Œ∏) =

1 Œì (10)
[3Œ∏(1 ‚àí Œ∏)7 + Œ∏7 (1 ‚àí Œ∏)],
4 Œì (2)Œì (8)

which is a 75-25% mixture of a beta(2,8) and a beta(8,2) prior distribution. Plot this prior distribution and compare it to the priors in a)
and b). Describe what sort of prior opinion this may represent.
d) For the prior in c):
i. Write out mathematically p(Œ∏) √ó p(y|Œ∏) and simplify as much as
possible.

Exercises

229

ii. The posterior distribution is a mixture of two distributions you
know. Identify these distributions.
iii. On a computer, calculate and plot p(Œ∏) √ó p(y|Œ∏) for a variety of Œ∏
values. Also find (approximately) the posterior mode, and discuss
its relation to the modes in a) and b).
e) Find a general formula for the weights of the mixture distribution in
d)ii, and provide an interpretation for their values.
3.5 Mixtures of conjugate priors: Let p(y|œÜ) = c(œÜ)h(y) exp{œÜt(y)} be an
exponential family model and let p1 (œÜ), . . . pK (œÜ) be K different members
of the conjugate class of prior densities
PKgiven in Section 3.3. A mixture of
conjugate priors is given
by
p
Àú
(Œ∏)
=
k=1 wk pk (Œ∏), where the wk ‚Äôs are all
P
greater than zero and
wk = 1 (see also Diaconis and Ylvisaker (1985)).
a) Identify the general form of the posterior distribution of Œ∏, based on
n i.i.d. samples from p(y|Œ∏) and the prior distribution given by pÀú.
b) Repeat a) but in the special case that p(y|Œ∏) = dpois(y, Œ∏) and
p1 , . . . , pK are gamma densities.
3.6 Exponential family expectations: Let p(y|œÜ) = c(œÜ)h(y) exp{œÜt(y)} be an
exponential family model.
a) Take
derivatives with respect to œÜ of both sides of the equation
R
p(y|œÜ) dy = 1 to show that E[t(Y )|œÜ] = ‚àíc0 (œÜ)/c(œÜ).
b) Let p(œÜ) ‚àù c(œÜ)n0 en0 t0 œÜ be the prior distribution for œÜ. Calculate
dp(œÜ)/dœÜ and, using the fundamental theorem of calculus, discuss
what must be true so that E[‚àíc(œÜ)/c(œÜ)] = t0 .
3.7 Posterior prediction: Consider a pilot study in which n1 = 15 children
enrolled in special education classes were randomly selected and tested
for a certain type of learning disability. In the pilot study, y1 = 2 children
tested positive for the disability.
a) Using a uniform prior distribution, find the posterior distribution of
Œ∏, the fraction of students in special education classes who have the
disability. Find the posterior mean, mode and standard deviation of
Œ∏, and plot the posterior density.
Researchers would like to recruit students with the disability to participate in a long-term study, but first they need to make sure they can recruit
enough students. Let n2 = 278 be the number of children in special education classes in this particular school district, and let Y2 be the number
of students with the disability.
b) Find Pr(Y2 = y2 |Y1 = 2), the posterior predictive distribution of Y2 ,
as follows:
i. Discuss what assumptions are needed about the joint distribution
of (Y1 , Y2 ) such that the following is true:
Z 1
Pr(Y2 = y2 |Y1 = 2) =
Pr(Y2 = y2 |Œ∏)p(Œ∏|Y1 = 2) dŒ∏ .
0

ii. Now plug in the forms for Pr(Y2 = y2 |Œ∏) and p(Œ∏|Y1 = 2) in the
above integral.

230

Exercises

iii. Figure out what the above integral must be by using the calculus
result discussed in Section 3.1.
c) Plot the function Pr(Y2 = y2 |Y1 = 2) as a function of y2 . Obtain the
mean and standard deviation of Y2 , given Y1 = 2.
d) The posterior mode and the MLE (maximum likelihood estimate; see
Exercise 3.14) of Œ∏, based on data from the pilot study, are both
ÀÜ and find the mean
Œ∏ÀÜ = 2/15. Plot the distribution Pr(Y2 = y2 |Œ∏ = Œ∏),
ÀÜ Compare these results to
and standard deviation of Y2 given Œ∏ = Œ∏.
the plots and calculations in c) and discuss any differences. Which
distribution for Y2 would you use to make predictions, and why?
3.8 Coins: Diaconis and Ylvisaker (1985) suggest that coins spun on a flat
surface display long-run frequencies of heads that vary from coin to coin.
About 20% of the coins behave symmetrically, whereas the remaining
coins tend to give frequencies of 1/3 or 2/3.
a) Based on the observations of Diaconis and Ylvisaker, use an appropriate mixture of beta distributions as a prior distribution for Œ∏, the
long-run frequency of heads for a particular coin. Plot your prior.
b) Choose a single coin and spin it at least 50 times. Record the number
of heads obtained. Report the year and denomination of the coin.
c) Compute your posterior for Œ∏, based on the information obtained in
b).
d) Repeat b) and c) for a different coin, but possibly using a prior for
Œ∏ that includes some information from the first coin. Your choice of
a new prior may be informal, but needs to be justified. How the results from the first experiment influence your prior for the Œ∏ of the
second coin may depend on whether or not the two coins have the
same denomination, have a similar year, etc. Report the year and
denomination of this coin.
3.9 Galenshore distribution: An unknown quantity Y has a Galenshore(a, Œ∏)
distribution if its density is given by
p(y) =

2 2a 2a‚àí1 ‚àíŒ∏2 y2
Œ∏ y
e
Œì (a)

for y > 0, Œ∏ > 0 and a > 0. Assume for now that a is known. For this
density,
Œì (a + 1/2)
a
E[Y ] =
, E[Y 2 ] = 2 .
Œ∏Œì (a)
Œ∏
a) Identify a class of conjugate prior densities for Œ∏. Plot a few members
of this class of densities.
b) Let Y1 , . . . , Yn ‚àº i.i.d. Galenshore(a, Œ∏). Find the posterior distribution
of Œ∏ given Y1 , . . . , Yn , using a prior from your conjugate class.
c) Write down p(Œ∏a |Y1 , . . . , Yn )/p(Œ∏b |Y1 , . . . , Yn ) and simplify. Identify a
sufficient statistic.
d) Determine E[Œ∏|y1 , . . . , yn ].

Exercises

231

e) Determine the form of the posterior predictive density p(Àú
y |y1 . . . , yn ).
3.10 Change of variables: Let œà = g(Œ∏), where g is a monotone function of Œ∏,
and let h be the inverse of g so that Œ∏ = h(œà). If pŒ∏ (Œ∏) is the probability
density of Œ∏, then the probability density of œà induced by pŒ∏ is given by
dh
pœà (œà) = pŒ∏ (h(œà)) √ó | dœà
|.
a) Let Œ∏ ‚àº beta(a, b) and let œà = log[Œ∏/(1 ‚àí Œ∏)]. Obtain the form of pœà
and plot it for the case that a = b = 1.
b) Let Œ∏ ‚àº gamma(a, b) and let œà = log Œ∏. Obtain the form of pœà and
plot it for the case that a = b = 1.
3.12 Jeffreys‚Äô prior: Jeffreys (1961) suggested a default rule for generating a
prior distribution of a parameter
Œ∏ in a sampling model p(y|Œ∏). Jeffreys‚Äô
p
prior is given by pJ (Œ∏) ‚àù I(Œ∏) , where I(Œ∏) = ‚àíE[‚àÇ 2 log p(Y |Œ∏)/‚àÇŒ∏2 |Œ∏]
is the Fisher information.
a) Let Y ‚àº binomial(n, Œ∏). Obtain Jeffreys‚Äô prior distribution pJ (Œ∏) for
this model.
b) Reparameterize the binomial sampling model with œà = log Œ∏/(1 ‚àí Œ∏),
so that p(y|œà) = ny eœày (1 + eœà )‚àín . Obtain Jeffreys‚Äô prior distribution
pJ (œà) for this model.
c) Take the prior distribution from a) and apply the change of variables
formula from Exercise 3.10 to obtain the induced prior density on œà.
This density should be the same as the one derived in part b) of this
exercise. This consistency under reparameterization is the defining
characteristic of Jeffrey‚Äôs‚Äô prior.
3.13 Improper Jeffreys‚Äô prior: Let Y ‚àº Poisson(Œ∏).
a) Apply Jeffreys‚Äô procedure to this model, and compare the result to the
family of gamma densities. Does Jeffreys‚Äô procedure
p produce an actual
probability density for Œ∏? In other words, can I(Œ∏) be proportional
to an actual probability density for Œ∏ ‚àà (0, ‚àû)?
p
b) Obtain the form of the function f (Œ∏, y) =
I(Œ∏) √ó p(y|Œ∏). What
probability
density
for
Œ∏
is
f
(Œ∏,
y)
proportional
to? Can we think of
R
f (Œ∏, y)/ f (Œ∏, y)dŒ∏ as a posterior density of Œ∏ given Y = y?
3.14 Unit information prior: Let Y1 , . . . , Yn ‚àº i.i.d. p(y|Œ∏). Having observed
the values Y1 = y1 , . . . , Yn = yn , the log likelihood is given by l(Œ∏|y) =
P
log p(yi |Œ∏), and the value Œ∏ÀÜ of Œ∏ that maximizes l(Œ∏|y) is called the
maximum likelihood estimator . The negative of the curvature of the loglikelihood, J(Œ∏) = ‚àí‚àÇ 2 l(Œ∏|y)/‚àÇŒ∏2 , describes the precision of the MLE Œ∏ÀÜ
and is called the observed Fisher information. For situations in which it
is difficult to quantify prior information in terms of a probability distribution, some have suggested that the ‚Äúprior‚Äù distribution be based on
the likelihood, for example, by centering the prior distribution around the
ÀÜ To deal with the fact that the MLE is not really prior information,
MLE Œ∏.
the curvature of the prior is chosen so that it has only ‚Äúone nth‚Äù as much
information as the likelihood, so that ‚àí‚àÇ 2 log p(Œ∏)/‚àÇŒ∏2 = J(Œ∏)/n. Such a
prior is called a unit information prior (Kass and Wasserman, 1995; Kass

232

Exercises

and Raftery, 1995), as it has as much information as the average amount
of information from a single observation. The unit information prior is
not really a prior distribution, as it is computed from the observed data.
However, it can be roughly viewed as the prior information of someone
with weak but accurate prior information.
ÀÜ
a) Let Y1 , . . . , Yn ‚àº i.i.d. binary(Œ∏). Obtain the MLE Œ∏ÀÜ and J(Œ∏)/n.
b) Find a probability density pU (Œ∏) such that log pU (Œ∏) = l(Œ∏|y)/n +
c, where c is a constant that does not depend on Œ∏. Compute the
information ‚àí‚àÇ 2 log pU (Œ∏)/‚àÇŒ∏2 of this density.
c) Obtain a probability density for Œ∏ that is proportional to pU (Œ∏) √ó
p(y1 , . . . , yn |Œ∏). Can this be considered a posterior distribution for Œ∏?
d) Repeat a), b) and c) but with p(y|Œ∏) being the Poisson distribution.

Chapter 4
4.1 Posterior comparisons: Reconsider the sample survey in Exercise 3.1. Suppose you are interested in comparing the rate of support in that county to
the rate in another county. Suppose that a survey of sample size 50 was
done in the second county, and the total number of people in the sample
who supported the policy was 30. Identify the posterior distribution of Œ∏2
assuming a uniform prior. Sample 5,000 values of each of Œ∏1 and Œ∏2 from
their posterior distributions and estimate Pr(Œ∏1 < Œ∏2 |the data and prior).
4.2 Tumor count comparisons: Reconsider the tumor count data in Exercise
3.3:
a) For the prior distribution given in part a) of that exercise, obtain
Pr(Œ∏B < Œ∏A |y A , y B ) via Monte Carlo sampling.
b) For a range of values of n0 , obtain Pr(Œ∏B < Œ∏A |y A , y B ) for Œ∏A ‚àº
gamma(120, 10) and Œ∏B ‚àº gamma(12√ón0 , n0 ). Describe how sensitive
the conclusions about the event {Œ∏B < Œ∏A } are to the prior distribution
on Œ∏B .
c) Repeat parts a) and b), replacing the event {Œ∏B < Œ∏A } with the event
{YÀúB < YÀúA }, where YÀúA and YÀúB are samples from the posterior predictive distribution.
4.3 Posterior predictive checks: Let‚Äôs investigate the adequacy of the Poisson model for the tumor count data. Following the example in Section
(1)
(1000)
(s)
4.4, generate posterior predictive datasets y A , . . . , y A
. Each y A is a
(s)
sample of size nA = 10 from the Poisson distribution with parameter Œ∏A ,
(s)
Œ∏A is itself a sample from the posterior distribution p(Œ∏A |y A ), and y A is
the observed data.
(s)
a) For each s, let t(s) be the sample average of the 10 values of y A ,
(s)
divided by the sample standard deviation of y A . Make a histogram
(s)
of t and compare to the observed value of this statistic. Based on
this statistic, assess the fit of the Poisson model for these data.

Exercises

233

b) Repeat the above goodness of fit evaluation for the data in population
B.
4.4 Mixtures of conjugate priors: For the posterior density from Exercise 3.4:
a) Make a plot of p(Œ∏|y) or p(y|Œ∏)p(Œ∏) using the mixture prior distribution
and a dense sequence of Œ∏-values. Can you think of a way to obtain
a 95% quantile-based posterior confidence interval for Œ∏? You might
want to try some sort of discrete approximation.
b) To sample a random variable z from the mixture distribution wp1 (z)+
(1 ‚àí w)p0 (z), first toss a w-coin and let x be the outcome (this can be
done in R with x<‚àírbinom(1,1,w) ). Then if x = 1 sample z from p1 ,
and if x = 0 sample z from p0 . Using this technique, obtain a Monte
Carlo approximation of the posterior distribution p(Œ∏|y) and a 95%
quantile-based confidence interval, and compare them to the results
in part a).
4.5 Cancer deaths: Suppose for a set of counties i ‚àà {1, . . . , n} we have information on the population size Xi = number of people in 10,000s, and Yi =
number of cancer fatalities. One model for the distribution of cancer fatalities is that, given the cancer rate Œ∏, they are independently distributed
with Yi ‚àº Poisson(Œ∏Xi ).
a) Identify the posterior distribution of Œ∏ given data (Y1 , X1 ), . . . , (Yn , Xn )
and a gamma(a, b) prior distribution.
The file cancer_react.dat contains 1990 population sizes (in 10,000s)
and number of cancer fatalities for 10 counties in a Midwestern state
that are near nuclear reactors. The file cancer_noreact.dat contains the
same data on counties in the same state that are not near nuclear reactors.
Consider these data as samples from two populations of counties: one is
the population of counties with no neighboring reactors and a fatality rate
of Œ∏1 deaths per 10,000, and the other is a population of counties having
nearby reactors and a fatality rate of Œ∏2 . In this exercise we will model
beliefs about the rates as independent and such that Œ∏1 ‚àº gamma(a1 , b1 )
and Œ∏2 ‚àº gamma(a2 , b2 ).
b) Using the numerical values of the data, identify the posterior distributions for Œ∏1 and Œ∏2 for any values of (a1 , b1 , a2 , b2 ).
c) Suppose cancer rates from previous years have been roughly Œ∏Àú = 2.2
per 10,000 (and note that most counties are not near reactors).
For each of the following three prior opinions, compute E[Œ∏1 |data],
E[Œ∏2 |data], 95% quantile-based posterior intervals for Œ∏1 and Œ∏2 , and
Pr(Œ∏2 > Œ∏1 |data). Also plot the posterior densities (try to put p(Œ∏1 |data)
and p(Œ∏2 |data) on the same plot). Comment on the differences across
posterior opinions.
i. Opinion 1: (a1 = a2 = 2.2 √ó 100, b1 = b2 = 100). Cancer rates for
both types of counties are similar to the average rates across all
counties from previous years.

234

Exercises

ii. Opinion 2: (a1 = 2.2 √ó 100, b1 = 100, a2 = 2.2, b1 = 1). Cancer
rates in this year for nonreactor counties are similar to rates in
previous years in nonreactor counties. We don‚Äôt have much information on reactor counties, but perhaps the rates are close to
those observed previously in nonreactor counties.
iii. Opinion 3: (a1 = a2 = 2.2, b1 = b2 = 1). Cancer rates in this year
could be different from rates in previous years, for both reactor
and nonreactor counties.
d) In the above analysis we assumed that population size gives no information about fatality rate. Is this reasonable? How would the analysis
have to change if this is not reasonable?
e) We encoded our beliefs about Œ∏1 and Œ∏2 such that they gave no information about each other (they were a priori independent). Think
about why and how you might encode beliefs such that they were a
priori dependent.
4.6 Non-informative prior distributions: Suppose for a binary sampling problem we plan on using a uniform, or beta(1,1), prior for the population
proportion Œ∏. Perhaps our reasoning is that this represents ‚Äúno prior information about Œ∏.‚Äù However, some people like to look at proportions on
Œ∏
the log-odds scale, that is, they are interested in Œ≥ = log 1‚àíŒ∏
. Via Monte
Carlo sampling or otherwise, find the prior distribution for Œ≥ that is induced by the uniform prior for Œ∏. Is the prior informative about Œ≥?
4.7 Mixture models: After a posterior analysis on data from a population of
squash plants, it was determined that the total vegetable weight of a given
plant could be modeled with the following distribution:
p(y|Œ∏, œÉ 2 ) = .31dnorm(y, Œ∏, œÉ) + .46dnorm(2Œ∏1 , 2œÉ) + .23dnorm(y, 3Œ∏1 , 3œÉ)
where the posterior distributions of the parameters have been calculated
as 1/œÉ 2 ‚àº gamma(10, 2.5), and Œ∏|œÉ 2 ‚àº normal(4.1, œÉ 2 /20).
a) Sample at least 5,000 y values from the posterior predictive distribution.
b) Form a 75% quantile-based confidence interval for a new value of Y .
c) Form a 75% HPD region for a new Y as follows:
i. Compute estimates of the posterior density of Y using the density
command in R, and then normalize the density values so they sum
to 1.
ii. Sort these discrete probabilities in decreasing order.
iii. Find the first probability value such that the cumulative sum of
the sorted values exceeds 0.75. Your HPD region includes all values
of y which have a discretized probability greater than this cutoff.
Describe your HPD region, and compare it to your quantile-based
region.
d) Can you think of a physical justification for the mixture sampling
distribution of Y ?

Exercises

235

4.8 More posterior predictive checks: Let Œ∏A and Œ∏B be the average number of children of men in their 30s with and without bachelor‚Äôs degrees,
respectively.
a) Using a Poisson sampling model, a gamma(2,1) prior for each Œ∏ and the
data in the files menchild30bach.dat and menchild30nobach.dat,
obtain 5,000 samples of YÀúA and YÀúB from the posterior predictive distribution of the two samples. Plot the Monte Carlo approximations to
these two posterior predictive distributions.
b) Find 95% quantile-based posterior confidence intervals for Œ∏B ‚àíŒ∏A and
YÀúB ‚àíYÀúA . Describe in words the differences between the two populations
using these quantities and the plots in a), along with any other results
that may be of interest to you.
c) Obtain the empirical distribution of the data in group B. Compare
this to the Poisson distribution with mean Œ∏ÀÜ = 1.4. Do you think the
Poisson model is a good fit? Why or why not?
d) For each of the 5,000 Œ∏B -values you sampled, sample nB = 218 Poisson
random variables and count the number of 0s and the number of 1s
in each of the 5,000 simulated datasets. You should now have two
sequences of length 5,000 each, one sequence counting the number of
people having zero children for each of the 5,000 posterior predictive
datasets, the other counting the number of people with one child.
Plot the two sequences against one another (one on the x-axis, one
on the y-axis). Add to the plot a point marking how many people in
the observed dataset had zero children and one child. Using this plot,
describe the adequacy of the Poisson model.

Chapter 5
5.1 Studying: The files school1.dat, school2.dat and school3.dat contain
data on the amount of time students from three high schools spent on
studying or homework during an exam period. Analyze data from each of
these schools separately, using the normal model with a conjugate prior
distribution, in which {¬µ0 = 5, œÉ02 = 4, Œ∫0 = 1, ŒΩ0 = 2} and compute or
approximate the following:
a) posterior means and 95% confidence intervals for the mean Œ∏ and
standard deviation œÉ from each school;
b) the posterior probability that Œ∏i < Œ∏j < Œ∏k for all six permutations
{i, j, k} of {1, 2, 3};
c) the posterior probability that YÀúi < YÀúj < YÀúk for all six permutations
{i, j, k} of {1, 2, 3}, where YÀúi is a sample from the posterior predictive
distribution of school i.
d) Compute the posterior probability that Œ∏1 is bigger than both Œ∏2 and
Œ∏3 , and the posterior probability that YÀú1 is bigger than both YÀú2 and
YÀú3 .

236

Exercises

5.2 Sensitivity analysis: Thirty-two students in a science classroom were
randomly assigned to one of two study methods, A and B, so that
nA = nB = 16 students were assigned to each method. After several
weeks of study, students were examined on the course material with an
exam designed to give an average score of 75 with a standard deviation of
10. The scores for the two groups are summarized by {¬Ø
yA = 75.2, sA = 7.3}
and {¬Ø
yB = 77.5, sb = 8.1}. Consider independent, conjugate normal prior
distributions for each of Œ∏A and Œ∏B , with ¬µ0 = 75 and œÉ02 = 100 for
both groups. For each (Œ∫0 , ŒΩ0 ) ‚àà {(1,1),(2,2),(4,4),(8,8),(16,16),(32,32)}
(or more values), obtain Pr(Œ∏A < Œ∏B |y A , y B ) via Monte Carlo sampling.
Plot this probability as a function of (Œ∫0 = ŒΩ0 ). Describe how you might
use this plot to convey the evidence that Œ∏A < Œ∏B to people of a variety
of prior opinions.
5.3 Marginal distributions: Given observations Y1 , . . . , Yn ‚àº i.i.d. normal(Œ∏, œÉ 2 )
and using the conjugate prior distribution for Œ∏ and œÉ 2 , derive the formula
for p(Œ∏|y1 , . . . , yn ), the marginal posterior distribution of Œ∏, conditional
on the data but marginal over œÉ 2 . Check your work by comparing your
formula to a Monte Carlo estimate of the marginal distribution, using
some values of Y1 , . . . , Yn , ¬µ0 , œÉ02 , ŒΩ0 and Œ∫0 that you choose. Also derive
p(Àú
œÉ 2 |y1 , . . . , yn ), where œÉ
Àú 2 = 1/œÉ 2 is the precision.
5.4 Jeffreys‚Äô prior: For sampling models expressed in terms of a p-dimensional
p
vector œà, Jeffreys‚Äô prior (Exercise 3.11) is defined as pJ (œà) ‚àù |I(œà)|,
where |I(œà)| is the determinant of the p √ó p matrix I(œà) having entries
I(œà)k,l = ‚àíE[‚àÇ 2 log p(Y |œà)/‚àÇœàk ‚àÇœàl ].
a) Show that Jeffreys‚Äô prior for the normal model is pJ (Œ∏, œÉ 2 ) ‚àù (œÉ 2 )‚àí3/2 .
b) Let y = (y1 , . . . , yn ) be the observed values of an i.i.d. sample from a
normal(Œ∏, œÉ 2 ) population. Find a probability density pJ (Œ∏, œÉ 2 |y) such
that pJ (Œ∏, œÉ 2 |y) ‚àù pJ (Œ∏, œÉ 2 )p(y|Œ∏, œÉ 2 ). It may be convenient to write
this joint density as pJ (Œ∏|œÉ 2 , y) √ó pJ (œÉ 2 |y). Can this joint density be
considered a posterior density?
5.5 Unit information prior: Obtain a unit information prior for the normal
model as follows:
a) Reparameterize the normal modelP
as p(y|Œ∏, œà), where œà = 1/œÉ 2 . Write
out the log likelihood l(Œ∏, œà|y) = log p(yi |Œ∏, œà) in terms of Œ∏ and œà.
b) Find a probability density pU (Œ∏, œà) so that log pU (Œ∏, œà) = l(Œ∏, œà|y)/n
+
a constant that does P
not depend on Œ∏ or œà. Hint: Write
Pc, where2 c isP
(yi ‚àí Œ∏) as (yi ‚àí y¬Ø + y¬Ø ‚àí Œ∏)2 = (yi ‚àí y¬Ø)2 + n(Œ∏ ‚àí y¬Ø)2 , and recall
that log pU (Œ∏, œà) = log pU (Œ∏|œà) + log pU (œà).
c) Find a probability density pU (Œ∏, œà|y) that is proportional to pU (Œ∏, œà)√ó
p(y1 , . . . , yn |Œ∏, œà). It may be convenient to write this joint density as
pU (Œ∏|œà, y) √ó pU (œà|y). Can this joint density be considered a posterior
density?

Exercises

237

Chapter 6
6.1 Poisson population comparisons: Let‚Äôs reconsider the number of children
data of Exercise 4.8. We‚Äôll assume Poisson sampling models for the two
groups as before, but now we‚Äôll parameterize Œ∏A and Œ∏B as Œ∏A = Œ∏, Œ∏B =
Œ∏ √ó Œ≥. In this parameterization, Œ≥ represents the relative rate Œ∏B /Œ∏A . Let
Œ∏ ‚àº gamma(aŒ∏ , bŒ∏ ) and let Œ≥ ‚àº gamma(aŒ≥ , bŒ≥ ).
a) Are Œ∏A and Œ∏B independent or dependent under this prior distribution? In what situations is such a joint prior distribution justified?
b) Obtain the form of the full conditional distribution of Œ∏ given y A , y B
and Œ≥.
c) Obtain the form of the full conditional distribution of Œ≥ given y A , y B
and Œ∏.
d) Set aŒ∏ = 2 and bŒ∏ = 1. Let aŒ≥ = bŒ≥ ‚àà {8, 16, 32, 64, 128}. For each of
these five values, run a Gibbs sampler of at least 5,000 iterations and
obtain E[Œ∏B ‚àíŒ∏A |y A , y B ]. Describe the effects of the prior distribution
for Œ≥ on the results.
6.2 Mixture model: The file glucose.dat contains the plasma glucose concentration of 532 females from a study on diabetes (see Exercise 7.6).
a) Make a histogram or kernel density estimate of the data. Describe
how this empirical distribution deviates from the shape of a normal
distribution.
b) Consider the following mixture model for these data: For each study
participant there is an unobserved group membership variable Xi
which is equal to 1 or 2 with probability p and 1 ‚àí p. If Xi = 1
then Yi ‚àº normal(Œ∏1 , œÉ12 ), and if Xi = 2 then Yi ‚àº normal(Œ∏2 , œÉ22 ). Let
p ‚àº beta(a, b), Œ∏j ‚àº normal(¬µ0 , œÑ02 ) and 1/œÉj ‚àº gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2)
for both j = 1 and j = 2. Obtain the full conditional distributions of
(X1 , . . . , Xn ), p, Œ∏1 , Œ∏2 , œÉ12 and œÉ22 .
c) Setting a = b = 1, ¬µ0 = 120, œÑ02 = 200, œÉ02 = 1000 and ŒΩ0 = 10,
implement the Gibbs sampler for at least 10,000 iterations. Let
(s)
(s) (s)
(s)
(s) (s)
Œ∏(1) = min{Œ∏1 , Œ∏2 } and Œ∏(2) = max{Œ∏1 , Œ∏2 }. Compute and plot
(s)

(s)

the autocorrelation functions of Œ∏(1) and Œ∏(2) , as well as their effective
sample sizes.
d) For each iteration s of the Gibbs sampler, sample a value x ‚àº
(s)
2(s)
binary(p(s) ), then sample YÀú (s) ‚àº normal(Œ∏x , œÉx ). Plot a histogram or kernel density estimate for the empirical distribution of
YÀú (1) , . . . , YÀú (S) , and compare to the distribution in part a). Discuss
the adequacy of this two-component mixture model for the glucose
data.
6.3 Probit regression: A panel study followed 25 married couples over a period of five years. One item of interest is the relationship between divorce
rates and the various characteristics of the couples. For example, the researchers would like to model the probability of divorce as a function of

238

Exercises

age differential, recorded as the man‚Äôs age minus the woman‚Äôs age. The
data can be found in the file divorce.dat. We will model these data with
probit regression, in which a binary variable Yi is described in terms of
an explanatory variable xi via the following latent variable model:
Zi = Œ≤xi + i
Yi = Œ¥(c,‚àû) (Zi ),
where Œ≤ and c are unknown coefficients, 1 , . . . , n ‚àº i.i.d. normal(0, 1)
and Œ¥(c,‚àû) (z) = 1 if z > c and equals zero otherwise.
a) Assuming Œ≤ ‚àº normal(0, œÑŒ≤2 ) obtain the full conditional distribution
p(Œ≤|y, x, z, c).
b) Assuming c ‚àº normal(0, œÑc2 ), show that p(c|y, x, z, Œ≤) is a constrained
normal density, i.e. proportional to a normal density but constrained
to lie in an interval. Similarly, show that p(zi |y, x, z ‚àíi , Œ≤, c) is proportional to a normal density but constrained to be either above c or
below c, depending on yi .
c) Letting œÑŒ≤2 = œÑc2 = 16 , implement a Gibbs sampling scheme that approximates the joint posterior distribution of Z, Œ≤, and c (a method
for sampling from constrained normal distributions is outlined in Section 12.1.1). Run the Gibbs sampler long enough so that the effective
sample sizes of all unknown parameters are greater than 1,000 (including the Zi ‚Äôs). Compute the autocorrelation function of the parameters
and discuss the mixing of the Markov chain.
d) Obtain a 95% posterior confidence interval for Œ≤, as well as Pr(Œ≤ >
0|y, x).

Chapter 7
7.1 Jeffreys‚Äô prior: For the multivariate normal model, Jeffreys‚Äô rule for generating a prior distribution on (Œ∏, Œ£) gives pJ (Œ∏, Œ£) ‚àù |Œ£|‚àí(p+2)/2 .
a) Explain why the function pJ cannot actually be a probability density
for (Œ∏, Œ£).
b) Let pJ (Œ∏, Œ£|y 1 , . . . , y n ) be the probability density that is proportional
to pJ (Œ∏, Œ£)√óp(y 1 , . . . , y n |Œ∏, Œ£). Obtain the form of pJ (Œ∏, Œ£|y 1 , . . . , y n ),
pJ (Œ∏|Œ£, y 1 , . . . , y n ) and pJ (Œ£|y 1 , . . . , y n ).
7.2 Unit information prior: Letting Œ® = Œ£ ‚àí1 , show that a unit information
prior for (Œ∏, Œ® ) is given by Œ∏|Œ® P
‚àº multivariate normal(¬Ø
y , Œ® ‚àí1 ) and Œ® ‚àº
‚àí1
T
Wishart(p + 1, S ), where S = (y i ‚àí y¬Ø)(y i ‚àí y¬Ø) /n. This can be done
by mimicking the procedure outlined in Exercise 5.6 as follows:
a) Reparameterize the multivariate normal model in terms of the precision matrix Œ® = Œ£ ‚àí1 . Write out the resulting log likelihood,
and find a probability density pU (Œ∏, Œ® ) = pU (Œ∏|Œ® )pU (Œ® ) such that
log p(Œ∏, Œ® ) = l(Œ∏, Œ® |Y)/n + c, where c does not depend on Œ∏ or Œ® .

Exercises

239

P T
Hint: Write (y i ‚àí Œ∏) as (y i ‚àí y¬Ø + y¬ØP
‚àí Œ∏), and note that
ai Bai can
be written as tr(AB), where A = ai aTi .
b) Let pU (Œ£) be the inverse-Wishart density induced by pU (Œ® ). Obtain a
density pU (Œ∏, Œ£|y 1 , . . . , y n ) ‚àù pU (Œ∏|Œ£)pU (Œ£)p(y 1 , . . . , y n |Œ∏, Œ£). Can
this be interpreted as a posterior distribution for Œ∏ and Œ£?
7.3 Australian crab data: The files bluecrab.dat and orangecrab.dat contain measurements of body depth (Y1 ) and rear width (Y2 ), in millimeters,
made on 50 male crabs from each of two species, blue and orange. We will
model these data using a bivariate normal distribution.
a) For each of the two species, obtain posterior distributions of the population mean Œ∏ and covariance matrix Œ£ as follows: Using the semiconjugate prior distributions for Œ∏ and Œ£, set ¬µ0 equal to the sample
mean of the data, Œõ0 and S0 equal to the sample covariance matrix
and ŒΩ0 = 4. Obtain 10,000 posterior samples of Œ∏ and Œ£. Note that
this ‚Äúprior‚Äù distribution loosely centers the parameters around empirical estimates based on the observed data (and is very similar to the
unit information prior described in the previous exercise). It cannot
be considered as our true prior distribution, as it was derived from
the observed data. However, it can be roughly considered as the prior
distribution of someone with weak but unbiased information.
b) Plot values of Œ∏ = (Œ∏1 , Œ∏2 )0 for each group and compare. Describe any
size differences between the two groups.
c) From each covariance matrix obtained from the Gibbs sampler, obtain the corresponding correlation coefficient. From these values, plot
posterior densities of the correlations œÅblue and œÅorange for the two
groups. Evaluate differences between the two species by comparing
these posterior distributions. In particular, obtain an approximation
to Pr(œÅblue < œÅorange |y blue , y orange ). What do the results suggest about
differences between the two populations?
7.4 Marriage data: The file agehw.dat contains data on the ages of 100 married couples sampled from the U.S. population.
a) Before you look at the data, use your own knowledge to formulate a
semiconjugate prior distribution for Œ∏ = (Œ∏h , Œ∏w )T and Œ£, where Œ∏h , Œ∏w
are mean husband and wife ages, and Œ£ is the covariance matrix.
b) Generate a prior predictive dataset of size n = 100, by sampling (Œ∏, Œ£)
from your prior distribution and then simulating Y 1 , . . . , Y n ‚àº i.i.d.
multivariate normal(Œ∏, Œ£). Generate several such datasets, make bivariate scatterplots for each dataset, and make sure they roughly represent your prior beliefs about what such a dataset would actually
look like. If your prior predictive datasets do not conform to your beliefs, go back to part a) and formulate a new prior. Report the prior
that you eventually decide upon, and provide scatterplots for at least
three prior predictive datasets.
c) Using your prior distribution and the 100 values in the dataset, obtain an MCMC approximation to p(Œ∏, Œ£|y 1 , . . . , y 100 ). Plot the joint

240

Exercises

posterior distribution of Œ∏h and Œ∏w , and also the marginal posterior
density of the correlation between Yh and Yw , the ages of a husband
and wife. Obtain 95% posterior confidence intervals for Œ∏h , Œ∏w and the
correlation coefficient.
d) Obtain 95% posterior confidence intervals for Œ∏h , Œ∏w and the correlation coefficient using the following prior distributions:
i. Jeffreys‚Äô prior, described in Exercise 7.1;
ii. the unit information prior, described in Exercise 7.2;
iii. a ‚Äúdiffuse prior‚Äù with ¬µ0 = 0, Œõ0 = 105 √ó I, S0 = 1000 √ó I and
ŒΩ0 = 3.
e) Compare the confidence intervals from d) to those obtained in c).
Discuss whether or not you think that your prior information is helpful
in estimating Œ∏ and Œ£, or if you think one of the alternatives in d)
is preferable. What about if the sample size were much smaller, say
n = 25?
7.5 Imputation: The file interexp.dat contains data from an experiment that
was interrupted before all the data could be gathered. Of interest was the
difference in reaction times of experimental subjects when they were given
stimulus A versus stimulus B. Each subject is tested under one of the two
stimuli on their first day of participation in the study, and is tested under
the other stimulus at some later date. Unfortunately the experiment was
interrupted before it was finished, leaving the researchers with 26 subjects
with both A and B responses, 15 subjects with only A responses and 17
subjects with only B responses.
2
2
a) Calculate empirical estimates of Œ∏A , Œ∏B , œÅ, œÉA
, œÉB
from the data using
the commands mean , cor and var . Use all the A responses to get
2
2
Œ∏ÀÜA and œÉ
ÀÜA
, and use all the B responses to get Œ∏ÀÜB and œÉ
ÀÜB
. Use only
the complete data cases to get œÅÀÜ.
b) For each person i with only an A response, impute a B response as
q
2 /ÀÜ
2.
yÀÜi,B = Œ∏ÀÜB + (yi,A ‚àí Œ∏ÀÜA )ÀÜ
œÅ œÉ
ÀÜB
œÉA
For each person i with only a B response, impute an A response as
q
2 /ÀÜ
2.
yÀÜi,A = Œ∏ÀÜA + (yi,B ‚àí Œ∏ÀÜB )ÀÜ
œÅ œÉ
ÀÜA
œÉB
You now have two ‚Äúobservations‚Äù for each individual. Do a paired
sample t-test and obtain a 95% confidence interval for Œ∏A ‚àí Œ∏B .
c) Using either Jeffreys‚Äô prior or a unit information prior distribution for
the parameters, implement a Gibbs sampler that approximates the
joint distribution of the parameters and the missing data. Compute
a posterior mean for Œ∏A ‚àí Œ∏B as well as a 95% posterior confidence
interval for Œ∏A ‚àí Œ∏B . Compare these results with the results from b)
and discuss.

Exercises

241

7.6 Diabetes data: A population of 532 women living near Phoenix, Arizona were tested for diabetes. Other information was gathered from these
women at the time of testing, including number of pregnancies, glucose
level, blood pressure, skin fold thickness, body mass index, diabetes pedigree and age. This information appears in the file azdiabetes.dat. Model
the joint distribution of these variables for the diabetics and non-diabetics
separately, using a multivariate normal distribution:
a) For both groups separately, use the following type of unit information
ÀÜ is the sample covariance matrix.
prior, where Œ£
ÀÜ
¬Ø , Œõ0 = Œ£;
i. ¬µ0 = y
ÀÜ
ii. S0 = Œ£, ŒΩ0 = p + 2 = 9 .
Generate at least 10,000 Monte Carlo samples for {Œ∏ d , Œ£d } and
{Œ∏ n , Œ£n }, the model parameters for diabetics and non-diabetics respectively. For each of the seven variables j ‚àà {1, . . . , 7}, compare the
marginal posterior distributions of Œ∏d,j and Œ∏n,j . Which variables seem
to differ between the two groups? Also obtain Pr(Œ∏d,j > Œ∏n,j |Y) for
each j ‚àà {1, . . . , 7}.
b) Obtain the posterior means of Œ£d and Œ£n , and plot the entries versus
each other. What are the main differences, if any?

Chapter 8
8.1 Components of variance: Consider the hierarchical model where
Œ∏1 , . . . , Œ∏m |¬µ, œÑ 2 ‚àº i.i.d. normal(¬µ, œÑ 2 )
y1,j , . . . , ynj ,j |Œ∏j , œÉ 2 ‚àº i.i.d. normal(Œ∏j , œÉ 2 ) .
For this problem, we will eventually compute the following:
Var[yi,j |Œ∏i , œÉ 2 ], Var[¬Ø
y¬∑,j |Œ∏i , œÉ 2 ], Cov[yi1 ,j , yi2 ,j |Œ∏j , œÉ 2 ]
2
Var[yi,j |¬µ, œÑ ], Var[¬Ø
y¬∑,j |¬µ, œÑ 2 ], Cov[yi1 ,j , yi2 ,j |¬µ, œÑ 2 ]
First, lets use our intuition to guess at the answers:
a) Which do you think is bigger, Var[yi,j |Œ∏i , œÉ 2 ] or Var[yi,j |¬µ, œÑ 2 ]? To
guide your intuition, you can interpret the first as the variability of
the Y ‚Äôs when sampling from a fixed group, and the second as the
variability in first sampling a group, then sampling a unit from within
the group.
b) Do you think Cov[yi1 ,j , yi2 ,j |Œ∏j , œÉ 2 ] is negative, positive, or zero? Answer the same for Cov[yi1 ,j , yi2 ,j |¬µ, œÑ ]. You may want to think about
what yi2 ,j tells you about yi1 ,j if Œ∏j is known, and what it tells you
when Œ∏j is unknown.
c) Now compute each of the six quantities above and compare to your
answers in a) and b).
d) Now assume we have a prior p(¬µ) for ¬µ. Using Bayes‚Äô rule, show that
p(¬µ|Œ∏1 , . . . , Œ∏m , œÉ 2 , œÑ 2 , y 1 , . . . , y m ) = p(¬µ|Œ∏1 , . . . , Œ∏m , œÑ 2 ).

242

Exercises

Interpret in words what this means.
8.2 Sensitivity analysis: In this exercise we will revisit the study from Exercise
5.2, in which 32 students in a science classroom were randomly assigned
to one of two study methods, A and B, with nA = nB = 16. After several
weeks of study, students were examined on the course material, and the
scores are summarized by {¬Ø
yA = 75.2, sA = 7.3}, {¬Ø
yB = 77.5, sb = 8.1}.
We will estimate Œ∏A = ¬µ + Œ¥ and Œ∏B = ¬µ ‚àí Œ¥ using the two-sample model
and prior distributions of Section 8.1.
a) Let ¬µ ‚àº normal(75, 100), 1/œÉ 2 ‚àº gamma(1, 100) and Œ¥ ‚àº normal(Œ¥0 , œÑ02 ).
For each combination of Œ¥0 ‚àà {‚àí4, ‚àí2, 0, 2, 4} and œÑ02 ‚àà {10, 50, 100, 500},
obtain the posterior distribution of ¬µ, Œ¥ and œÉ 2 and compute
i. Pr(Œ¥ < 0|Y);
ii. a 95% posterior confidence interval for Œ¥;
iii. the prior and posterior correlation of Œ∏A and Œ∏B .
b) Describe how you might use these results to convey evidence that
Œ∏A < Œ∏B to people of a variety of prior opinions.
8.3 Hierarchical modeling: The files school1.dat through school8.dat give
weekly hours spent on homework for students sampled from eight different
schools. Obtain posterior distributions for the true means for the eight
different schools using a hierarchical normal model with the following
prior parameters:
¬µ0 = 7, Œ≥02 = 5, œÑ02 = 10, Œ∑0 = 2, œÉ02 = 15, ŒΩ0 = 2 .
a) Run a Gibbs sampling algorithm to approximate the posterior distribution of {Œ∏, œÉ 2 , ¬µ, œÑ 2 }. Assess the convergence of the Markov chain,
and find the effective sample size for {œÉ 2 , ¬µ, œÑ 2 }. Run the chain long
enough so that the effective sample sizes are all above 1,000.
b) Compute posterior means and 95% confidence regions for {œÉ 2 , ¬µ, œÑ 2 }.
Also, compare the posterior densities to the prior densities, and discuss
what was learned from the data.
2
c) Plot the posterior density of R = œÉ2œÑ+œÑ 2 and compare it to a plot of the
prior density of R. Describe the evidence for between-school variation.
d) Obtain the posterior probability that Œ∏7 is smaller than Œ∏6 , as well as
the posterior probability that Œ∏7 is the smallest of all the Œ∏‚Äôs.
e) Plot the sample averages y¬Ø1 , . . . , y¬Ø8 against the posterior expectations
of Œ∏1 , . . . , Œ∏8 , and describe the relationship. Also compute the sample
mean of all observations and compare it to the posterior mean of ¬µ.

Chapter 9
9.1 Extrapolation: The file swim.dat contains data on the amount of time,
in seconds, it takes each of four high school swimmers to swim 50 yards.
Each swimmer has six times, taken on a biweekly basis.

Exercises

243

a) Perform the following data analysis for each swimmer separately:
i. Fit a linear regression model of swimming time as the response and
week as the explanatory variable. To formulate your prior, use the
information that competitive times for this age group generally
range from 22 to 24 seconds.
ii. For each swimmer j, obtain a posterior predictive distribution
for Yj‚àó , their time if they were to swim two weeks from the last
recorded time.
b) The coach of the team has to decide which of the four swimmers will
compete in a swimming meet in two weeks. Using your predictive distributions, compute Pr(Yj‚àó = max{Y1‚àó , . . . , Y4‚àó }|Y)) for each swimmer
j, and based on this make a recommendation to the coach.
9.2 Model selection: As described in Example 6 of Chapter 7, The file
azdiabetes.dat contains data on health-related variables of a population of 532 women. In this exercise we will be modeling the conditional
distribution of glucose level (glu) as a linear combination of the other
variables, excluding the variable diabetes.
a) Fit a regression model using the g-prior with g = n, ŒΩ0 = 2 and œÉ02 = 1.
Obtain posterior confidence intervals for all of the parameters.
b) Perform the model selection and averaging procedure described in Section 9.3. Obtain Pr(Œ≤j 6= 0|y), as well as posterior confidence intervals
for all of the parameters. Compare to the results in part a).
9.3 Crime: The file crime.dat contains crime rates and data on 15 explanatory variables for 47 U.S. states, in which both the crime rates
and the explanatory variables have been centered and scaled to have
variance 1. A description of the variables can be obtained by typing
library (MASS);?UScrime in R.
a) Fit a regression model y = XŒ≤+ using the g-prior with g = n, ŒΩ0 = 2
and œÉ02 = 1. Obtain marginal posterior means and 95% confidence
intervals for Œ≤, and compare to the least squares estimates. Describe
the relationships between crime and the explanatory variables. Which
variables seem strongly predictive of crime rates?
b) Lets see how well regression models can predict crime rates based on
the X-variables. Randomly divide the crime roughly in half, into a
training set {y tr , Xtr } and a test set {y te , Xte }
i. Using only the training set, obtain least squares regression coeffiÀÜ . Obtain predicted values for the test data by computing
cients Œ≤
ols
ÀÜ
ÀÜ ols = XP
ÀÜ ols versus y te and compute the prediction
y
te Œ≤ ols . Plot y
1
error nte (yi,te ‚àí yÀÜi,ols )2 .
ÀÜ
ii. Now obtain the posterior mean Œ≤
Bayes = E[Œ≤|y tr ] using the g-prior
described above and the training data only. Obtain predictions
ÀÜ
ÀÜ Bayes = Xtest Œ≤
for the test set y
Bayes . Plot versus the test data,
compute the prediction error, and compare to the OLS prediction
error. Explain the results.

244

Exercises

c) Repeat the procedures in b) many times with different randomly generated test and training sets. Compute the average prediction error
for both the OLS and Bayesian methods.

Chapter 10
10.1 Reflecting random walks: It is often useful in MCMC to have a proposal
distribution which is both symmetric and has support only on a certain
region. For example, if we know Œ∏ > 0, we would like our proposal distribution J(Œ∏1 |Œ∏0 ) to have support on positive Œ∏ values. Consider the following
proposal algorithm:
‚Ä¢ sample Œ∏Àú ‚àº uniform(Œ∏0 ‚àí Œ¥, Œ∏0 + Œ¥);
Àú
‚Ä¢ if Œ∏Àú < 0, set Œ∏1 = ‚àíŒ∏;
Àú
Àú
‚Ä¢ if Œ∏ ‚â• 0, set Œ∏1 = Œ∏.
Àú Show that the above algorithm draws samples
In other words, Œ∏1 = |Œ∏|.
from a symmetric proposal distribution which has support on positive
values of Œ∏. It may be helpful to write out the associated proposal density
J(Œ∏1 |Œ∏0 ) under the two conditions Œ∏0 ‚â§ Œ¥ and Œ∏0 > Œ¥ separately.
10.2 Nesting success: Younger male sparrows may or may not nest during a
mating season, perhaps depending on their physical characteristics. Researchers have recorded the nesting success of 43 young male sparrows
of the same age, as well as their wingspan, and the data appear in the
file msparrownest.dat. Let Yi be the binary indicator that sparrow i
successfully nests, and let xi denote their wingspan. Our model for Yi is
logit Pr(Yi = 1|Œ±, Œ≤, xi ) = Œ± + Œ≤xi , where the logit function is given by
logit Œ∏ = log[Œ∏/(1 ‚àí Œ∏)].
Qn
a) Write out the joint sampling distribution i=1 p(yi |Œ±, Œ≤, xi ) and simplify as much as possible.
b) Formulate a prior probability distribution over Œ± and Œ≤ by considering the range of Pr(Y = 1|Œ±, Œ≤, x) as x ranges over 10 to 15, the
approximate range of the observed wingspans.
c) Implement a Metropolis algorithm that approximates p(Œ±, Œ≤|y, x).
Adjust the proposal distribution to achieve a reasonable acceptance
rate, and run the algorithm long enough so that the effective sample
size is at least 1,000 for each parameter.
d) Compare the posterior densities of Œ± and Œ≤ to their prior densities.
e) Using output from the Metropolis algorithm, come up with a way to
make a confidence band for the following function fŒ±Œ≤ (x) of wingspan:
fŒ±Œ≤ (x) =

eŒ±+Œ≤x
,
1 + eŒ±+Œ≤x

where Œ± and Œ≤ are the parameters in your sampling model. Make a
plot of such a band.

Exercises

245

10.3 Tomato plants: The file tplant.dat contains data on the heights of ten
tomato plants, grown under a variety of soil pH conditions. Each plant
was measured twice. During the first measurement, each plant‚Äôs height
was recorded and a reading of soil pH was taken. During the second measurement only plant height was measured, although it is assumed that pH
levels did not vary much from measurement to measurement.
a) Using ordinary least squares, fit a linear regression to the data, modeling plant height as a function of time (measurement period) and pH
level. Interpret your model parameters.
b) Perform model diagnostics. In particular, carefully analyze the residuals and comment on possible violations of assumptions. In particular,
assess (graphically or otherwise) whether or not the residuals within a
plant are independent. What parts of your ordinary linear regression
model do you think are sensitive to any violations of assumptions you
may have detected?
c) Hypothesize a new model for your data which allows for observations
within a plant to be correlated. Fit the model using a MCMC approximation to the posterior distribution, and present diagnostics for your
approximation.
d) Discuss the results of your data analysis. In particular, discuss similarities and differences between the ordinary linear regression and the
model fit with correlated responses. Are the conclusions different?
10.4 Gibbs sampling: Consider the general Gibbs sampler for a vector of parameters œÜ. Suppose œÜ(s) is sampled from the target distribution p(œÜ)
and then œÜ(s+1) is generated using the Gibbs sampler by iteratively updating each component of the parameter vector. Show that
R the marginal
probability Pr(œÜ(s+1) ‚àà A) equals the target distribution A p(œÜ) dœÜ.
10.5 Logistic regression variable selection: Consider a logistic regression model
for predicting diabetes as a function of x1 = number of pregnancies, x2 =
blood pressure, x3 = body mass index, x4 = diabetes pedigree and x5 =
age. Using the data in azdiabetes.dat, center and scale each of the xvariables by subtracting the sample average and dividing by the sample
standard deviation for each variable. Consider a logistic regression model
of the form Pr(Yi = 1|xi , Œ≤, z) = eŒ∏i /(1 + eŒ∏i ) where
Œ∏i = Œ≤0 + Œ≤1 Œ≥1 xi,1 + Œ≤2 Œ≥2 xi,2 + Œ≤3 Œ≥3 xi,3 + Œ≤4 Œ≥4 xi,4 + Œ≤5 Œ≥5 xi,5 .
In this model, each Œ≥j is either 0 or 1, indicating whether or not variable
j is a predictor of diabetes. For example, if it were the case that Œ≥ =
(1, 1, 0, 0, 0), then Œ∏i = Œ≤0 + Œ≤1 xi,1 + Œ≤2 xi,2 . Obtain posterior distributions
for Œ≤ and Œ≥, using independent prior distributions for the parameters,
such that Œ≥j ‚àº binary(1/2), Œ≤0 ‚àº normal(0, 16) and Œ≤j ‚àº normal(0, 4) for
each j > 0.

246

Exercises

a) Implement a Metropolis-Hastings algorithm for approximating the
(s)
posterior distribution of Œ≤ and Œ≥. Examine the sequences Œ≤j and
(s)

(s)

Œ≤j √ó Œ≥j for each j and discuss the mixing of the chain.
b) Approximate the posterior probability of the top five most frequently
occurring values of Œ≥. How good do you think the MCMC estimates
of these posterior probabilities are?
c) For each j, plot posterior densities and obtain posterior means for
Œ≤j Œ≥j . Also obtain Pr(Œ≥j = 1|x, y).

Chapter 11
11.1 Full conditionals: Derive formally the full conditional distributions of
Œ∏, Œ£, œÉ 2 and the Œ≤ j ‚Äôs as given in Section 11.2.
11.2 Randomized block design: Researchers interested in identifying the optimal planting density for a type of perennial grass performed the following
randomized experiment: Ten different plots of land were each divided into
eight subplots, and planting densities of 2, 4, 6 and 8 plants per square meter were randomly assigned to the subplots, so that there are two subplots
at each density in each plot. At the end of the growing season the amount
of plant matter yield was recorded in metric tons per hectare. These data
appear in the file pdensity.dat. The researchers want to fit a model like
y = Œ≤1 + Œ≤2 x + Œ≤3 x2 + , where y is yield and x is planting density, but
worry that since soil conditions vary across plots they should allow for
some across-plot heterogeneity in this relationship. To accommodate this
possibility we will analyze these data using the hierarchical linear model
described in Section 11.1.
a) Before we do a Bayesian analysis we will get some ad hoc estimates
of these parameters via least squares regression. Fit the model y =
Œ≤1 +Œ≤2 x+Œ≤3 x2 + using OLS for each group, and make a plot showing
the heterogeneity of the least squares regression lines. From the least
squares coefficients find ad hoc estimates of Œ∏ and Œ£. Also obtain an
estimate of œÉ 2 by combining the information from the residuals across
the groups.
b) Now we will perform an analysis of the data using the following distributions as prior distributions:
ÀÜ ‚àí1 )
Œ£ ‚àí1 ‚àº Wishart(4, Œ£
ÀÜ Œ£)
ÀÜ
Œ∏ ‚àº multivariate normal(Œ∏,
2
2
œÉ ‚àº inverse ‚àí gamma(1, œÉ
ÀÜ )
ÀÜ Œ£,
ÀÜ œÉ
where Œ∏,
ÀÜ 2 are the estimates you obtained in a). Note that this
analysis is not combining prior information with information from
the data, as the‚Äúprior‚Äù distribution is based on the observed data.

Exercises

247

However, such an analysis can be roughly interpreted as the Bayesian
analysis of an individual who has weak but unbiased prior information.
c) Use a Gibbs sampler to approximate posterior expectations of Œ≤ for
each group j, and plot the resulting regression lines. Compare to the
regression lines in a) above and describe why you see any differences
between the two sets of regression lines.
d) From your posterior samples, plot marginal posterior and prior densities of Œ∏ and the elements of Œ£. Discuss the evidence that the slopes
or intercepts vary across groups.
e) Suppose we want to identify the planting density that maximizes average yield over a random sample of plots. Find the value xmax of x
that maximizes expected yield, and provide a 95% posterior predictive interval for the yield of a randomly sampled plot having planting
density xmax .
11.3 Hierarchical variances:. The researchers in Exercise 11.2 are worried that
the plots are not just heterogeneous in their regression lines, but also
in their variances. In this exercise we will consider the same hierarchical model as above except that the sampling variability within a group
is given by yi,j ‚àº normal(Œ≤1,j + Œ≤2,j xi,j + Œ≤3,j x2i,j , œÉj2 ), that is, the variances are allowed to differ across groups. As in Section 8.5, we will model
2
œÉ12 , . . . , œÉm
‚àº i.i.d. inverse gamma(ŒΩ0 /2, ŒΩ0 œÉ02 /2), with œÉ02 ‚àº gamma(2, 2)
and p(ŒΩ0 ) uniform on the integers {1, 2, . . . , 100}.
a) Obtain the full conditional distribution of œÉ02 .
b) Obtain the full conditional distribution of œÉj2 .
c) Obtain the full conditional distribution of Œ≤ j .
(s)

2
d) For two values ŒΩ0 and ŒΩ0‚àó of ŒΩ0 , obtain the ratio p(ŒΩ0‚àó |œÉ02 , œÉ12 , . . . , œÉm
)
(s) 2
2
2
divided by p(ŒΩ0 |œÉ0 , œÉ1 , . . . , œÉm ), and simplify as much as possible.
e) Implement a Metropolis-Hastings algorithm for obtaining the joint
posterior distribution of all of the unknown parameters. Plot values
of œÉ02 and ŒΩ0 versus iteration number and describe the mixing of the
Markov chain in terms of these parameters.
f) Compare the prior and posterior distributions of ŒΩ0 . Comment on any
evidence there is that the variances differ across the groups.
11.4 Hierarchical logistic regression: The Washington Assessment of Student
Learning (WASL) is a standardized test given to students in the state of
Washington. Letting j index the counties within the state of Washington
and i index schools within counties, the file mathstandard.dat includes
data on the following variables:
yi,j = the indicator that more than half the 10th graders in school i, j
passed the WASL math exam;
xi,j = the percentage of teachers in school i, j who have a masters
degree.
In this exercise we will construct an algorithm to approximate the posterior distribution of the parameters in a generalized linear mixed-effects

248

Exercises

model for these data. The model is a mixed effects version of logistic
regression:
yi,j ‚àº binomial(eŒ≥i,j /[1 + eŒ≥i,j ]), where Œ≥i,j = Œ≤0,j + Œ≤1,j xi,j
Œ≤ 1 , . . . , Œ≤ J ‚àº i.i.d. multivariate normal (Œ∏, Œ£), where Œ≤ j = (Œ≤0,j , Œ≤1,j )
a) The unknown parameters in the model include population-level parameters {Œ∏, Œ£} and the group-level parameters {Œ≤ 1 , . . . , Œ≤ m }. Draw
a diagram that describes the relationships between these parameters,
the data {yi,j , xi,j , i = 1 . . . , nj , j = 1, . . . , m}, and prior distributions.
b) Before we do a Bayesian analysis, we will get some ad hoc estimates
of these parameters via maximum likelihood: Fit a separate logistic
regression model for each group, possibly using the glm command
in R via beta. j <‚àí glm(y.jÀúX.j,family=binomial)$coef . Explain any
problems you have with obtaining estimates for each county. Plot
exp{Œ≤ÀÜ0,j + Œ≤ÀÜ1,j x}/(1 + exp{Œ≤ÀÜ0,j + Œ≤ÀÜ1,j x}) as a function of x for each
county and describe what you see. Using maximum likelihood estimates only from those counties with 10 or more schools, obtain ad
ÀÜ and Œ£
ÀÜ of Œ∏ and Œ£. Note that these estimates may not
hoc estimates Œ∏
be representative of patterns from schools with small sample sizes.
c) Formulate a unit information prior distribution for Œ∏ and Œ£ based on
ÀÜ Œ£)
ÀÜ and
the observed data. Specifically, let Œ∏ ‚àº multivariate normal(Œ∏,
‚àí1
‚àí1
ÀÜ
let Œ£ ‚àº Wishart(4, Œ£ ). Use a Metropolis-Hastings algorithm to
approximate the joint posterior distribution of all parameters.
d) Make plots of the samples of Œ∏ and Œ£ (5 parameters) versus MCMC
iteration number. Make sure you run the chain long enough so that
your MCMC samples are likely to be a reasonable approximation to
the posterior distribution.
e) Obtain posterior expectations of Œ≤ j for each group j, plot E[Œ≤0,j |y] +
E[Œ≤1,j |y]x as a function of x for each county, compare to the plot in
b) and describe why you see any differences between the two sets of
regression lines.
f) From your posterior samples, plot marginal posterior and prior densities of Œ∏ and the elements of Œ£. Include your ad hoc estimates from
b) in the plots. Discuss the evidence that the slopes or intercepts vary
across groups.
11.5 Disease rates: The number of occurrences of a rare, nongenetic birth defect
in a five-year period for six neighboring counties is y = (1, 3, 2, 12, 1, 1).
The counties have populations of x = (33, 14, 27, 90, 12, 17), given in thousands. The second county has higher rates of toxic chemicals (PCBs)
present in soil samples, and it is of interest to know if this town has a
high disease rate as well. We will use the following hierarchical model to
analyze these data:
‚Ä¢ Yi |Œ∏i , xi ‚àº Poisson(Œ∏i xi );
‚Ä¢ Œ∏1 , . . . , Œ∏6 |a, b ‚àº gamma(a, b);
‚Ä¢ a ‚àº gamma(1,1) ; b ‚àº gamma(10,1).

Exercises

249

a) Describe in words what the various components of the hierarchical
model represent in terms of observed and expected disease rates.
b) Identify the form of the conditional distribution of p(Œ∏1 , . . . , Œ∏6 |a, b, x,
y), and from this identify the full conditional distribution of the rate
for each county p(Œ∏i |Œ∏ ‚àíi , a, b, x, y).
c) Write out the ratio of the posterior densities comparing a set of proposal values (a‚àó , b‚àó , Œ∏) to values (a, b, Œ∏). Note the value of Œ∏, the
vector of county-specific rates, is unchanged.
d) Construct a Metropolis-Hastings algorithm which generates samples
of (a, b, Œ∏) from the posterior. Do this by iterating the following steps:
1. Given a current value (a, b, Œ∏), generate a proposal (a‚àó , b‚àó , Œ∏) by
sampling a‚àó and b‚àó from a symmetric proposal distribution centered around a and b, but making sure all proposals are positive (see Exercise 10.1). Accept the proposal with the appropriate
probability.
2. Sample new values of the Œ∏j ‚Äôs from their full conditional distributions.
Perform diagnostic tests on your chain and modify if necessary.
e) Make posterior inference on the infection rates using the samples from
the Markov chain. In particular,
i. Compute marginal posterior distributions of Œ∏1 , . . . , Œ∏6 and compare them to y1 /x1 , . . . y6 /x6 .
ii. Examine the posterior distribution of a/b, and compare it to the
corresponding prior distribution as well as to the average of yi /xi
across the six counties.
iii. Plot samples of Œ∏2 versus Œ∏j for each j 6= 2, and draw a 45 degree line on the plot as well. Also estimate Pr(Œ∏2 > Œ∏j |x, y) for
each j and Pr(Œ∏2 = max{Œ∏1 , . . . , Œ∏6 }|x, y). Interpret the results
of these calculations, and compare them to the conclusions one
might obtain if they just examined yj /xj for each county j.

Chapter 12
12.1 Rank regression: The 1996 General Social Survey gathered a wide variety of information on the adult U.S. population, including each survey
respondent‚Äôs sex, their self-reported frequency of religious prayer (on a
six-level ordinal scale), and the number of items correct out of 10 on a
short vocabulary test. These data appear in the file prayer.dat. Using
the rank regression procedure described in Section 12.1.2, estimate the
parameters in a regression model for Yi = prayer as a function of xi,1 =
sex of respondent (0-1 indicator of being female) and xi,2 = vocabulary
score, as well as their interaction xi,3 = xi,1 √ó xi,2 . Compare marginal
prior distributions of the three regression parameters to their posterior

250

Exercises

distributions, and comment on the evidence that the relationship between
prayer and score differs across the sexes.
12.2 Copula modeling: The file azdiabetes_alldata.dat contains data on
eight variables for 632 women in a study on diabetes (see Exercise 7.6
for a description of the variables). Data on subjects labeled 201-300 have
missing values for some variables, mostly for the skin fold thickness measurement.
a) Using only the data from subjects 1-200, implement the Gaussian
copula model for the eight variables in this dataset. Obtain
posterior

means and 95% posterior confidence intervals for all 82 = 28 parameters.
b) Now use the data from subjects 1-300, thus including data from subjects who are missing some variables. Implement the Gaussian copula
model and obtain posterior means and 95% posterior confidence intervals for all parameters. How do the results differ from those in a)?
12.3 Constrained normal: Let p(z) ‚àù dnorm(z, Œ∏, œÉ)√óŒ¥(a,b) (z), the normal density constrained to the interval (a, b). Prove that the inverse-cdf method
outlined in Section 12.1.1 generates a sample from this distribution.
12.4 Categorical data and the Dirichlet distribution: Consider again the data
on the number of children of men in their 30s from Exercise 4.8. These
data could be considered as categorical data, as each sample Y lies in the
discrete set {1, . . . , 8} (8 here actually denotes ‚Äú8 or more‚Äù children). Let
Œ∏ A = (Œ∏A,1 , . . . , Œ∏A,8 ) be the proportion in each of the eight categories
from the population of men with bachelor‚Äôs degrees, and let the vector Œ∏ B
be defined similarly for the population of men without bachelor‚Äôs degrees.
a) Write in a compact form the conditional probability given Œ∏ A of observing a particular sequence {yA,1 , . . . , yA,n1 } for a random sample
from the A population.
b) Identify the sufficient statistic. Show that the Dirichlet family of disaK ‚àí1
tributions, with densities of the form p(Œ∏|a) ‚àù Œ∏1a1 ‚àí1 √ó ¬∑ ¬∑ ¬∑ Œ∏K
, are
a conjugate class of prior distributions for this sampling model.
c) The function rdir () below samples from the Dirichlet distribution:
r d i r <‚àíf u n c t i o n ( nsamp=1 , a ) # a i s a v e c t o r
{
Z<‚àím at r i x ( rgamma ( l e n g t h ( a ) ‚àó nsamp , a , 1 ) ,
nsamp , l e n g t h ( a ) , byrow=T)
Z/ a p p ly ( Z , 1 , sum )
}

Using this function, generate 5,000 or more samples of Œ∏ A and Œ∏ B from
their posterior distributions. Using a Monte Carlo approximation, obtain and plot the posterior distributions of E[YA |Œ∏ A ] and E[YB |Œ∏ B ],
as well as of YÀúA and YÀúB .

Exercises

251

d) Compare the results above to those in Exercise 4.8. Perform the goodness of fit test from that exercise on this model, and compare to the
fit of the Poisson model.

Common distributions

The binomial distribution
A random variable X ‚àà {0, 1, . . . , n} has a binomial(n, Œ∏) distribution if Œ∏ ‚àà
[0, 1] and
 
n x
Pr(X = x|Œ∏, n) =
Œ∏ (1 ‚àí Œ∏)n‚àíx for x ‚àà {0, 1, . . . , n}.
x
For this distribution,
E[X|Œ∏] = nŒ∏,
Var[X|Œ∏] = nŒ∏(1 ‚àí Œ∏),
mode[X|Œ∏] = b(n + 1)Œ∏c,
p(x|Œ∏, n) =

dbinom(x,n,theta) .

If X1 ‚àº binomial(n1 , Œ∏) and X2 ‚àº binomial(n2 , Œ∏) are independent, then
X = X1 +X2 ‚àº binomial(n1 +n2 , Œ∏). When n = 1 this distribution is called the
binary or Bernoulli distribution. The binomial(n, Œ∏) model assumes that X is
(equal in distribution to) a sum of independent binary(Œ∏) random variables.

The beta distribution
A random variable X ‚àà [0, 1] has a beta(a, b) distribution if a > 0, b > 0 and
p(x|a, b) =

Œì (a + b) a‚àí1
x
(1 ‚àí x)b‚àí1
Œì (a)Œì (b)

for 0 ‚â§ x ‚â§ 1.

254

Common distributions

For this distribution,
E[X|a, b] =

a
,
a+b

ab
1
= E[X] √ó E[1 ‚àí X] √ó
,
2
(a + b + 1)(a + b)
a+b+1
a‚àí1
mode[X|a, b] =
if a > 1 and b > 1,
(a ‚àí 1) + (b ‚àí 1)
Var[X|a, b] =

p(x|a, b) = dbeta(x,a,b) .
The beta distribution is closely related to the gamma distribution. See the
paragraph on the gamma distribution below for details. A multivariate version
of the beta distribution is the Dirichlet distribution, described in Exercise 12.4.

The Poisson distribution
A random variable X ‚àà {0, 1, 2, . . .} has a Poisson(Œ∏) distribution if Œ∏ > 0 and
Pr(X = x|Œ∏) = Œ∏x e‚àíŒ∏ /x! for x ‚àà {0, 1, 2, . . .}.
For this distribution,
E[X|Œ∏] = Œ∏,
Var[X|Œ∏] = Œ∏,
mode[X|Œ∏] = bŒ∏c,
p(x|Œ∏) = dpois(x,theta) .
If X1 ‚àº Poisson(Œ∏1 ) and X2 ‚àº Poisson(Œ∏2 ) are independent, then X1 + X2 ‚àº
Poisson(Œ∏1 +Œ∏2 ). The Poisson family has a ‚Äúmean-variance relationship,‚Äù which
describes the fact that E[X|Œ∏] = Var[X|Œ∏] = Œ∏. If it is observed that a sample
mean is very different than the sample variance, then the Poisson model may
not be appropriate. If the variance is larger than the sample mean, then a
negative binomial model (Section 3.2.1) might be a better fit.

The gamma and inverse-gamma distributions
A random variable X ‚àà (0, ‚àû) has a gamma(a, b) distribution if a > 0, b > 0
and
ba a‚àí1 ‚àíbx
p(x|a, b) =
x
e
for x > 0.
Œì (a)
For this distribution,

Common distributions

255

E[X|a, b] = a/b,
Var[X|a, b] = a/b2 ,
mode[X|a, b] = (a ‚àí 1)/b if a ‚â• 1, 0 if 0 < a < 1 ,
p(x|a, b) = dgamma(x,a,b) .
If X1 ‚àº gamma(a1 , b) and X1 ‚àº gamma(a2 , b) are independent, then X1 +
X2 ‚àº gamma(a1 +a2 , b) and X1 /(X1 +X2 ) ‚àº beta(a1 , a2 ). If X ‚àº normal(0, œÉ 2 )
then X 2 ‚àº gamma(1/2, 1/[2œÉ 2 ]). The chi-square distribution with ŒΩ degrees
of freedom is the same as a gamma(ŒΩ/2, 1/2) distribution.
A random variable X ‚àà (0, ‚àû) has an inverse-gamma(a, b) distribution if
1/X has a gamma(a, b) distribution. In other words, if Y ‚àº gamma(a, b) and
X = 1/Y , then X ‚àº inverse-gamma(a, b). The density of X is
p(x|a, b) =

ba ‚àía‚àí1 ‚àíb/x
x
e
Œì (a)

for x > 0.

For this distribution,
E[X|a, b] = b/(a ‚àí 1) if a ‚â• 1, ‚àû if 0 < a < 1,
Var[X|a, b] = b2 /[(a ‚àí 1)2 (a ‚àí 2)] if a ‚â• 2, ‚àû if 0 < a < 2,
mode[X|a, b] = b/(a + 1).
Note that the inverse-gamma density is not simply the gamma density with
x replaced by 1/x: There is an additional factor of x‚àí2 due to the Jacobian
in the change-of-variables formula (see Exercise 10.3).

The univariate normal distribution
A random variable X ‚àà R has a normal(Œ∏, œÉ 2 ) distribution if œÉ 2 > 0 and
p(x|Œ∏, œÉ 2 ) = ‚àö

1
2œÄœÉ 2

1

e‚àí 2 (x‚àíŒ∏)

2

/œÉ 2

for ‚àí‚àû < x < ‚àû.

For this distribution,
E[X|Œ∏, œÉ 2 ] = Œ∏,
Var[X|Œ∏, œÉ 2 ] = œÉ 2 ,
mode[X|Œ∏, œÉ 2 ] = Œ∏,
p(x|Œ∏, œÉ 2 ) = dnorm(x,theta,sigma) .
Remember that R parameterizes things in terms of the standard deviation
œÉ, and not the variance œÉ 2 . If X1 ‚àº normal(Œ∏1 , œÉ12 ) and X2 ‚àº normal(Œ∏2 , œÉ22 )
are independent, then aX1 + bX2 + c ‚àº normal(aŒ∏1 + bŒ∏2 + c, a2 œÉ12 + b2 œÉ22 ).

256

Common distributions

A normal sampling model is often useful even if the underlying population
does not have a normal distribution. This is because statistical procedures
that assume a normal model will generally provide good estimates of the
population mean and variance, regardless of whether or not the population is
normal (see Section 5.5 for a discussion).

The multivariate normal distribution
A random vector X ‚àà Rp has a multivariate normal(Œ∏, Œ£) distribution if Œ£
is a positive definite p √ó p matrix and


1
p(x|Œ∏, Œ£) = (2œÄ)‚àíp/2 |Œ£|‚àí1/2 exp ‚àí (x ‚àí Œ∏)T Œ£ ‚àí1 (x ‚àí Œ∏)
for x ‚àà Rp .
2
For this distribution,
E[X|Œ∏, Œ£] = Œ∏,
Var[X|Œ∏, Œ£] = Œ£,
mode[X|Œ∏, Œ£] = Œ∏.
Just like the univariate normal distribution, if X 1 ‚àº normal(Œ∏ 1 , Œ£1 ) and
X 2 ‚àº normal(Œ∏ 2 , Œ£2 ) are independent, then aX 1 + bX 2 + c ‚àº normal(aŒ∏ 1 +
bŒ∏ 2 + c, a2 Œ£1 + b2 Œ£2 ). Marginal and conditional distributions of subvectors of X also have multivariate normal distributions: Let a ‚äÇ {1, . . . , p}
be a subset of variable indices, and let b = ac be the remaining indices.
Then X [a] ‚àº multivariate normal(Œ∏ [a] , Œ£[a,a] ) and {X [b] |X [a] } ‚àº multivariate
normal(Œ∏ b|a ,Œ£b|a ), where
Œ∏ b|a = Œ∏ [b] + Œ£[b,a] (Œ£[a,a] )‚àí1 (X [a] ‚àí Œ∏ [a] )
Œ£b|a = Œ£[b,b] ‚àí Œ£[b,a] (Œ£[a,a] )‚àí1 Œ£[a,b] .
Simulating a multivariate normal random variable can be achieved by a linear
transformation of a vector of i.i.d. standard normal random variables. If Z
is the vector with elements Z1 , . . . , Zp ‚àº i.i.d. normal(0, 1) and AAT = Œ£,
then X = Œ∏ + AZ ‚àº multivariate normal(Œ∏, Œ£). Usually A is the Choleski
factorization of Œ£. The following R-code will generate an n √ó p matrix such
that the rows are i.i.d. samples from a multivariate normal distribution:
Z<‚àím at r i x ( rnorm ( n‚àóp ) , nrow=n , n c o l=p )
X<‚àít ( t ( Z%‚àó%c h o l ( Sigma ) ) + c ( t h e t a ) )

Common distributions

257

The Wishart and inverse-Wishart distributions
A random p √ó p symmetric positive definite matrix X has a Wishart(ŒΩ, M)
distribution if the integer ŒΩ ‚â• p, M is a p √ó p symmetric positive definite
matrix and
p(X|ŒΩ, M) = [2ŒΩp/2 Œìp (ŒΩ/2)|M|ŒΩ/2 ]‚àí1 √ó |X|(ŒΩ‚àíp‚àí1)/2 etr(‚àíM‚àí1 X/2),
where
Qp
Œìp (ŒΩ/2) = œÄ p(p‚àí1)/4
j=1 Œì [(ŒΩ + 1 ‚àí j)/2], and
P
etr(A) = exp( aj,j ), the exponent of the sum of the diagonal elements.
For this distribution,
E[X|ŒΩ, M] = ŒΩM,
Var[Xi,j |ŒΩ, M] = ŒΩ √ó (m2i,j + mi,i mj,j ),
mode[X|ŒΩ, M] = (ŒΩ ‚àí p ‚àí 1)M.
The Wishart distribution is a multivariate version of the gamma distribution. Just as the sum of squares of i.i.d. univariate normal variables has a
gamma distribution, the sums of squares of i.i.d. multivariate normal vectors
has a Wishart distribution.
Specifically, if Y 1 , . . . , Y ŒΩ ‚àº i.i.d. multivariate
P
normal(0, M), then Y i Y Ti ‚àº Wishart(ŒΩ, M). This relationship can be used
to generate a Wishart-distributed random matrix:
Z<‚àím at r i x ( rnorm ( nu‚àóp ) , nrow=nu , n c o l=p )
Y<‚àíZ%‚àó%c h o l (M)
X<‚àít (Y)%‚àó%Y

# s t a n d a r d normal
# rows have cov=M
# Wishart matrix

A random p √ó p symmetric positive definite matrix X has an inverseWishart(ŒΩ, M) distribution if X‚àí1 has a Wishart(ŒΩ, M) distribution. In other
words, if Y ‚àº Wishart(ŒΩ, M) and X = Y‚àí1 , then X ‚àº inverse-Wishart(ŒΩ, M).
The density of X is
p(X|ŒΩ, M) = [2ŒΩp/2 Œìp (ŒΩ/2)|M|ŒΩ/2 ]‚àí1 √ó |X|‚àí(ŒΩ+p+1)/2 etr(‚àíM‚àí1 X‚àí1 /2).
For this distribution,
E[X|ŒΩ, M] = (ŒΩ ‚àí p ‚àí 1)‚àí1 M‚àí1 ,
mode[X|ŒΩ, M] = (ŒΩ + p + 1)‚àí1 M‚àí1 .
The second moments (i.e. the variances) of the elements of X are given in
Press (1972). Since we often use the inverse-Wishart distribution as a prior
distribution for a covariance matrix Œ£, it is sometimes useful to parameterize
the distribution in terms of S = M‚àí1 . Then if Œ£ ‚àº inverse-Wishart(ŒΩ, S‚àí1 ),
we have mode[X|ŒΩ, S] = (ŒΩ + p + 1)‚àí1 S. If Œ£0 were the most probable value

258

Common distributions

of Œ£ a priori, then we would set S = (ŒΩ0 + p + 1)Œ£0 , so that Œ£ ‚àº inverseWishart(ŒΩ, [(ŒΩ + p ‚àí 1)Œ£0 ]‚àí1 ) and mode[Œ£|ŒΩ, S] = Œ£0 .
For more on the Wishart distribution and its relationship to the multivariate
normal distribution, see Press (1972) or Mardia et al (1979).

References

Agresti A, Coull BA (1998) Approximate is better than ‚Äúexact‚Äù for interval
estimation of binomial proportions. Amer Statist 52(2):119‚Äì126
¬¥
Aldous DJ (1985) Exchangeability and related topics. In: Ecole
d‚Äô¬¥et¬¥e de
probabilit¬¥es de Saint-Flour, XIII‚Äî1983, Lecture Notes in Math., vol 1117,
Springer, Berlin, pp 1‚Äì198
Arcese P, Smith JNM, Hochachka WM, Rogers CM, Ludwig D (1992) Stability, regulation, and the determination of abundance in an insular song
sparrow population. Ecology 73(3):805‚Äì822
Atchad¬¥e YF, Rosenthal JS (2005) On adaptive Markov chain Monte Carlo
algorithms. Bernoulli 11(5):815‚Äì828
Bayarri MJ, Berger JO (2000) p values for composite null models. J Amer
Statist Assoc 95(452):1127‚Äì1142, 1157‚Äì1170, with comments and a rejoinder
by the authors
Bayes T (1763) An essay towards solving a problem in the doctrine of chances.
R Soc Lond Philos Trans 5(3):370‚Äì418
Berger JO (1980) Statistical decision theory: foundations, concepts, and methods. Springer-Verlag, New York, springer Series in Statistics
Berk KN (1978) Comparing subset regression procedures. Technometrics 20:1‚Äì
6
Bernardo JM, Smith AFM (1994) Bayesian theory. Wiley Series in Probability
and Mathematical Statistics: Probability and Mathematical Statistics, John
Wiley & Sons Ltd., Chichester
Besag J (1974) Spatial interaction and the statistical analysis of lattice systems. J Roy Statist Soc Ser B 36:192‚Äì236, with discussion by D. R. Cox, A.
G. Hawkes, P. Clifford, P. Whittle, K. Ord, R. Mead, J. M. Hammersley,
and M. S. Bartlett and with a reply by the author
Bickel PJ, Ritov Y (1997) Local asymptotic normality of ranks and covariates
in transformation models. In: Festschrift for Lucien Le Cam, Springer, New
York, pp 43‚Äì54

260

References

Box GEP, Draper NR (1987) Empirical model-building and response surfaces.
Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics, John Wiley & Sons Inc., New York
Bunke O, Milhaud X (1998) Asymptotic behavior of Bayes estimates under
possibly incorrect models. Ann Statist 26(2):617‚Äì644
Carlin BP, Louis TA (1996) Bayes and empirical Bayes methods for data analysis, Monographs on Statistics and Applied Probability, vol 69. Chapman
& Hall, London
Casella G (1985) An introduction to empirical Bayes data analysis. Amer
Statist 39(2):83‚Äì87
Chib S, Winkelmann R (2001) Markov chain Monte Carlo analysis of correlated count data. J Bus Econom Statist 19(4):428‚Äì435
Congdon P (2003) Applied Bayesian modelling. Wiley Series in Probability
and Statistics, John Wiley & Sons Ltd., Chichester
Cox RT (1946) Probability, frequency and reasonable expectation. Amer J
Phys 14:1‚Äì13
Cox RT (1961) The algebra of probable inference. The Johns Hopkins Press,
Baltimore, Md
Dawid AP, Lauritzen SL (1993) Hyper-Markov laws in the statistical analysis
of decomposable graphical models. Ann Statist 21(3):1272‚Äì1317
Diaconis P (1988) Recent progress on de Finetti‚Äôs notions of exchangeability.
In: Bayesian statistics, 3 (Valencia, 1987), Oxford Sci. Publ., Oxford Univ.
Press, New York, pp 111‚Äì125
Diaconis P, Freedman D (1980) Finite exchangeable sequences. Ann Probab
8(4):745‚Äì764
Diaconis P, Ylvisaker D (1979) Conjugate priors for exponential families. Ann
Statist 7(2):269‚Äì281
Diaconis P, Ylvisaker D (1985) Quantifying prior opinion. In: Bayesian statistics, 2 (Valencia, 1983), North-Holland, Amsterdam, pp 133‚Äì156, with discussion and a reply by Diaconis
Dunson DB (2000) Bayesian latent variable models for clustered mixed outcomes. J R Stat Soc Ser B Stat Methodol 62(2):355‚Äì366
Efron B (2005) Bayesians, frequentists, and scientists. J Amer Statist Assoc
100(469):1‚Äì5
Efron B, Morris C (1973) Stein‚Äôs estimation rule and its competitors‚Äîan
empirical Bayes approach. J Amer Statist Assoc 68:117‚Äì130
de Finetti B (1931) Funzione caratteristica di un fenomeno aleatorio. Atti
della R Academia Nazionale dei Lincei, Serie 6 Memorie, Classe di Scienze
Fisiche, Mathematice e Naturale 4:251‚Äì299
de Finetti B (1937) La pr¬¥evision : ses lois logiques, ses sources subjectives.
Ann Inst H Poincar¬¥e 7(1):1‚Äì68
Gelfand AE, Smith AFM (1990) Sampling-based approaches to calculating
marginal densities. J Amer Statist Assoc 85(410):398‚Äì409
Gelfand AE, Sahu SK, Carlin BP (1995) Efficient parameterisations for normal
linear mixed models. Biometrika 82(3):479‚Äì488

References

261

Gelman A, Hill J (2007) Data Analysis using Regression and Multilevel/Hierarchical Models. Cambridge University Press, New York
Gelman A, Rubin DB (1992) Inference from iterative simulation using multiple
sequences (Disc: P483-501, 503-511). Statistical Science 7:457‚Äì472
Gelman A, Meng XL, Stern H (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statist Sinica 6(4):733‚Äì807, with comments and a rejoinder by the authors
Gelman A, Carlin JB, Stern HS, Rubin DB (2004) Bayesian data analysis,
2nd edn. Texts in Statistical Science Series, Chapman & Hall/CRC, Boca
Raton, FL
Geman S, Geman D (1984) Stochastic relaxation, Gibbs distributions, and
the Bayesian restoration of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence 6:721‚Äì741
George EI, McCulloch RE (1993) Variable selection via Gibbs sampling. Journal of the American Statistical Association 88:881‚Äì889
Geweke J (1992) Evaluating the accuracy of sampling-based approaches to the
calculation of posterior moments (Disc: P189-193). In: Bernardo JM, Berger
JO, Dawid AP, Smith AFM (eds) Bayesian Statistics 4. Proceedings of the
Fourth Valencia International Meeting, Clarendon Press [Oxford University
Press], pp 169‚Äì188
Geyer CJ (1992) Practical Markov chain Monte Carlo (Disc: P483-503). Statistical Science 7:473‚Äì483
Gilks WR, Roberts GO, Sahu SK (1998) Adaptive Markov chain Monte Carlo
through regeneration. J Amer Statist Assoc 93(443):1045‚Äì1054
Grogan W, Wirth W (1981) A new American genus of predaceous midges
related to Palpomyia and Bezzia (Diptera: Ceratopogonidae). Proceedings
of the Biological Society of Washington 94:1279‚Äì1305
Guttman I (1967) The use of the concept of a future observation in goodnessof-fit problems. J Roy Statist Soc Ser B 29:83‚Äì100
Haario H, Saksman E, Tamminen J (2001) An adaptive metropolis algorithm.
Bernoulli 7(2):223‚Äì242
Haigis KM, Hoff PD, White A, Shoemaker AR, Halberg RB, Dove WF (2004)
Tumor regionality in the mouse intestine reflects the mechanism of loss of
apc function. PNAS 101(26):9769‚Äì9773
Hartigan JA (1966) Note on the confidence-prior of Welch and Peers. J Roy
Statist Soc Ser B 28:55‚Äì56
Hastings WK (1970) Monte Carlo sampling methods using Markov chains and
their applications. Biometrika 57:97‚Äì109
Hewitt E, Savage LJ (1955) Symmetric measures on Cartesian products. Trans
Amer Math Soc 80:470‚Äì501
Hoff PD (2007) Extending the rank likelihood for semiparametric copula estimation. Ann Appl Statist 1(1):265‚Äì283
Jaynes ET (2003) Probability theory. Cambridge University Press, Cambridge, the logic of science, Edited and with a foreword by G. Larry Bretthorst

262

References

Jeffreys H (1961) Theory of probability. Third edition, Clarendon Press, Oxford
Johnson VE (2007) Bayesian model assessment using pivotal quantities.
Bayesian Anal 2(4):719‚Äì733
Jordan MIe (1998) Learning in Graphical Models. Kluwer Academic Publishers Group
Kass RE, Raftery AE (1995) Bayes factors. Journal of the American Statistical
Association 90:773‚Äì795
Kass RE, Wasserman L (1995) A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion. J Amer Statist Assoc
90(431):928‚Äì934
Kass RE, Wasserman L (1996) The selection of prior distributions by formal
rules (Corr: 1998V93 p412). Journal of the American Statistical Association
91:1343‚Äì1370
Kelley TL (1927) The Interpretation of Educational Measurement. World
Book Company, New York
Key JT, Pericchi LR, Smith AFM (1999) Bayesian model choice: what and
why? In: Bayesian statistics, 6 (Alcoceber, 1998), Oxford Univ. Press, New
York, pp 343‚Äì370
Kleijn BJK, van der Vaart AW (2006) Misspecification in infinite-dimensional
Bayesian statistics. Ann Statist 34(2):837‚Äì877
Kuehl RO (2000) Design of Experiments: Statistical Principles of Research
Design and Analysis. Duxbury Press
Laplace PS (1995) A philosophical essay on probabilities, english edn. Dover
Publications Inc., New York, translated from the sixth French edition by
Frederick William Truscott and Frederick Lincoln Emory, With an introductory note by E. T. Bell
Lauritzen SL (1996) Graphical models, Oxford Statistical Science Series,
vol 17. The Clarendon Press Oxford University Press, New York, oxford
Science Publications
Lawrence E, Bingham D, Liu C, Nair V (2008) Bayesian inference for multivariate ordinal data using parameter expansion. Technometrics 50(2):182‚Äì
191
Leamer EE (1978) Specification searches. Wiley-Interscience [John Wiley &
Sons], New York, ad hoc inference with nonexperimental data, Wiley Series
in Probability and Mathematical Statistics
Letac G, Massam H (2007) Wishart distributions for decomposable graphs.
Ann Statist 35(3):1278‚Äì1323
Liang F, Paulo R, Molina G, Clyde MA, Berger JO (2008) Mixtures of g
Priors for Bayesian Variable Selection. Journal of the American Statistical
Association 103(481):410‚Äì423
Lindley DV, Smith AFM (1972) Bayes estimates for the linear model. J Roy
Statist Soc Ser B 34:1‚Äì41, with discussions by J. A. Nelder, V. D. Barnett,
C. A. B. Smith, T. Leonard, M. R. Novick, D. R. Cox, R. L. Plackett,
P. Sprent, J. B. Copas, D. V. Hinkley, E. F. Harding, A. P. Dawid, C.

References

263

Chatfield, S. E. Fienberg, B. M. Hill, R. Thompson, B. de Finetti, and O.
Kempthorne
Little RJ (2006) Calibrated Bayes: a Bayes/frequentist roadmap. Amer Statist
60(3):213‚Äì223
Liu JS, Wu YN (1999) Parameter expansion for data augmentation. J Amer
Statist Assoc 94(448):1264‚Äì1274
Logan JA (1983) A multivariate model for mobility tables. The American
Journal of Sociology 89(2):324‚Äì349
Lukacs E (1942) A characterization of the normal distribution. Ann Math
Statistics 13:91‚Äì93
Madigan D, Raftery AE (1994) Model selection and accounting for model uncertainty in graphical models using Occam‚Äôs window. Journal of the American Statistical Association 89:1535‚Äì1546
Mardia KV, Kent JT, Bibby JM (1979) Multivariate analysis. Academic Press
[Harcourt Brace Jovanovich Publishers], London, probability and Mathematical Statistics: A Series of Monographs and Textbooks
Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953)
Equation of state calculations by fast computing machines. Journal of
Chemical Physics 21(6):1087‚Äì1092
Monahan JF, Boos DD (1992) Proper likelihoods for Bayesian analysis.
Biometrika 79(2):271‚Äì278
Moore KA, Waite LJ (1977) Early childbearing and educational attainment.
Family Planning Perspectives 9(5):220‚Äì225
Papaspiliopoulos O, Roberts GO, Sk¬®
old M (2007) A general framework for
the parametrization of hierarchical models. Statist Sci 22(1):59‚Äì73
Petit J, , Jouzel J, Raynaud D, Barkov N, Barnola JM, Basile I, Bender M,
Chappellaz J, Davis M, Delayque G, Delmotte M, Kotlyakov V, Legrand M,
Lipenkov V, Lorius C, Pepin L, Ritz C, Saltzman E, Stievenard M (1999)
Climate and atmospheric history of the past 420,000 years from the vostok
ice core, antarctica. Nature 399:429‚Äì436
Pettitt AN (1982) Inference for the linear model using a likelihood based on
ranks. J Roy Statist Soc Ser B 44(2):234‚Äì243
Pitt M, Chan D, Kohn R (2006) Efficient Bayesian inference for Gaussian
copula regression models. Biometrika 93(3):537‚Äì554
Press SJ (1972) Applied multivariate analysis. Holt, Rinehart and Winston,
Inc., New York, series in Quantitative Methods for Decision-Making, International Series in Decision Processes
Press SJ (1982) Applied Multivariate Analysis: Using Bayesian and Frequentist Methods of Inference. Krieger Publishing Company, Inc.
Quinn K (2004) Bayesian Factor Analysis for Mixed Ordinal and Continuous
Responses. Political Analysis 12(4):338‚Äì353
Raftery AE, Lewis SM (1992) How many iterations in the Gibbs sampler? In:
Bernardo JM, Berger JO, Dawid AP, Smith AFM (eds) Bayesian Statistics
4. Proceedings of the Fourth Valencia International Meeting, Clarendon
Press [Oxford University Press], pp 763‚Äì773

264

References

Raftery AE, Madigan D, Hoeting JA (1997) Bayesian model averaging for
linear regression models. J Amer Statist Assoc 92(437):179‚Äì191
Raiffa H, Schlaifer R (1961) Applied statistical decision theory. Studies in
Managerial Economics, Division of Research, Graduate School of Business
Administration, Harvard University, Boston, Mass.
Rao JNK (1958) A characterization of the normal distribution. Ann Math
Statist 29:914‚Äì919
Ripley BD (1979) [Algorithm AS 137] Simulating spatial patterns: Dependent
samples from a multivariate density. Applied Statistics 28:109‚Äì112
Robert C, Casella G (2008) A history of markov chain monte carlo‚Äìsubjective
recollections from incomplete data arXiv:0808.2902 [stat.CO], arxiv:
0808.2902
Robert CP, Casella G (2004) Monte Carlo statistical methods, 2nd edn.
Springer Texts in Statistics, Springer-Verlag, New York
Roberts GO, Rosenthal JS (2007) Coupling and ergodicity of adaptive Markov
chain Monte Carlo algorithms. J Appl Probab 44(2):458‚Äì475
Rubin DB (1984) Bayesianly justifiable and relevant frequency calculations
for the applied statistician. Ann Statist 12(4):1151‚Äì1172
Rubinstein RY, Kroese DP (2008) Simulation and the Monte Carlo method,
2nd edn. Wiley Series in Probability and Statistics, Wiley-Interscience [John
Wiley & Sons], Hoboken, NJ
Savage LJ (1954) The foundations of statistics. John Wiley & Sons Inc., New
York
Savage LJ (1962) The foundations of statistical inference. Methuen & Co.
Ltd., London
Savage LJ (1972) The foundations of statistics, revised edn. Dover Publications Inc., New York
Severini TA (1991) On the relationship between Bayesian and non-Bayesian
interval estimates. J Roy Statist Soc Ser B 53(3):611‚Äì618
Smith JW, Everhart JE, Dickson WC, Knowler WC, Johannes RS (1988) Using the adap learning algorithm to forecast the onset of diabetes mellitus.
In: Greenes RA (ed) Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988), IEEE Computer Society Press,
pp 261‚Äì265
Stein C (1955) A necessary and sufficient condition for admissibility. Ann
Math Statist 26:518‚Äì522
Stein C (1956) Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In: Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954‚Äì1955, vol. I, University of California Press, Berkeley and Los Angeles, pp 197‚Äì206
Stein CM (1981) Estimation of the mean of a multivariate normal distribution.
Ann Statist 9(6):1135‚Äì1151
Sweeting TJ (1999) On the construction of Bayes-confidence regions. J R Stat
Soc Ser B Stat Methodol 61(4):849‚Äì861

References

265

Sweeting TJ (2001) Coverage probability bias, objective Bayes and the likelihood principle. Biometrika 88(3):657‚Äì675
Tibshirani R (1989) Noninformative priors for one parameter of many.
Biometrika 76(3):604‚Äì608
Tibshirani R (1996) Regression shrinkage and selection via the lasso. J Roy
Statist Soc Ser B 58(1):267‚Äì288
Welch BL, Peers HW (1963) On formulae for confidence points based on
integrals of weighted likelihoods. J Roy Statist Soc Ser B 25:318‚Äì329
White H (1982) Maximum likelihood estimation of misspecified models.
Econometrica 50(1):1‚Äì25
Zellner A (1986) On assessing prior distributions and Bayesian regression
analysis with g-prior distributions. In: Bayesian inference and decision techniques, Stud. Bayesian Econometrics Statist., vol 6, North-Holland, Amsterdam, pp 233‚Äì243

Index

admissible, 79
asymptotic coverage, 7, 41
autoregressive model, 189
backwards elimination, 162
Bayes factor, 16, 164
Bayes‚Äô rule, 2, 15, 225
Bayesian
coverage, 41
inference, 1
methods, 1
belief function, 13
beta distribution, 34, 253
bias, 80
binomial distribution, 18, 35, 253
categorical data, 250
central limit theorem, 68
change of variables, 231
Choleski factorization, 256
coherence of bets, 226
confidence regions and intervals, 7, 41
coverage, 41
highest posterior density, 42, 234
quantile-based, 42
conjugate prior distribution, 38, 51, 83
constrained normal distribution, 212,
238
copula model, 217
for missing data, 221
correlation, 106
correlation matrix, 120
covariance matrix
population, 105

sample, 109, 111
credible intervals, 52
cumulative distribution function (cdf),
19
de Finetti‚Äôs theorem, 29
density, 18
conditional, 23
joint, 23
marginal, 23
dependence graph, 222
Dirichlet distribution, 250
distribution
beta, 34, 253
binomial, 18, 35, 253
conditional, 23, 225
constrained normal, 212, 238
Dirichlet, 250
full conditional, 93, 225
gamma, 45, 254
inverse-gamma, 74, 254
inverse-Wishart, 110, 257
joint, 23
marginal, 23, 225
multivariate normal, 106, 256
negative binomial, 48
normal, 20, 67, 255
Poisson, 19, 43, 254
posterior, 2, 25
predictive, 40
prior, 2
uniform, 32
Wishart, 257

268

Index

effective sample size, 103
empirical Bayes, 146
Ergodic theorem, 185
exchangeability, 27
in hierarchical models, 131
random variables, 27
expectation, 21
exponential family, 51, 83, 229
Fisher information, 231
observed, 231
fixed effect, 147, 197
frequentist coverage, 41
full conditional distribution, 93, 225
gamma distribution, 45, 254
gamma function, 33
generalized least squares, 189
generalized linear model, 171
linear predictor, 172
link function
log link, 172
logit link, 173
logistic regression, 172
mixed effects model, 247
variable selection, 245
Poisson regression, 172
mixed effects model, 203
Gibbs sampler, 93
properties, 96, 245
with Metropolis algorithm, 187
graphical model, 123, 222
group comparisons
multiple groups, 130
two groups, 125
hierarchical data, 130
hierarchical model
fixed effect, 197
for population means, 132
for population variances, 143
logistic regression, 247
mixed effects model, 195
normal model, 132
normal regression, 197
Poisson regression, 203
Poisson-gamma model, 248
random effect, 197

highest posterior density (HPD)
region, see confidence regions and
intervals
i.i.d., 26
identifiability, lack of, 218
imputation, 115
independence
events, 17
random variables, 26
interquartile range, 22
inverse, of a matrix, 106
inverse-gamma distribution, 74, 254
inverse-Wishart distribution, 110, 257
Jeffreys‚Äô prior, see prior distribution
joint distribution, 23
lasso, 10
latent variable model, 211
likelihood, 231
maximum likelihood estimator, 231
rank, 214
linear mixed effects model, 197
linear regression, 149
g-prior, 157
Bayesian estimation, 154
complexity penalty, 166
generalized least squares, 189
hierarchical, 195, 197
model averaged estimate, 169
model averaging, 167
model selection, 160, 243
normal model, 151
ordinary least squares, 153
polynomial regression, 203
relationship to multivariate normal
model, 121
unit information prior, 156
weakly informative prior, 155
log-odds, 57
logistic regression, 172
mixed effects model, 247
variable selection, 245
logit function, 173
marginal likelihood, 223
Markov chain, 96
aperiodic, 185

Index

269

Ergodic theorem, 185
irreducible, 185
recurrent, 185
Markov chain Monte Carlo (MCMC),
97
autocorrelation, 100, 178, 237, 238
burn-in, 178
comparison to Monte Carlo, 99
convergence, 101
effective sample size, 103
mixing, 102
stationarity, 101
thinned chain, 181, 191
matrix inverse, 106
matrix trace, 110
matrix transpose, 106
matrix, positive definite, 109
maximum likelihood, 231
mean, 21
mean squared error (MSE), 81
median, 21
Metropolis algorithm, 175
acceptance ratio, 175
with Gibbs sampler, 187
Metropolis-Hastings algorithm, 181
acceptance ratio, 183
combining Gibbs and Metropolis, 187
stationary distribution, 186
missing data, 115
missing at random, 116, 221
mixed effects model, see hierarchical
model
mixture model, 234, 237
mode, 21
model, see distribution
model averaging, 167
model checking, 62, 232, 235
model selection
linear regression, 160, 243
logistic regression, 245
model, sampling, 2
Monte Carlo approximation, 54
Monte Carlo standard error, 56
multilevel data, 130
multivariate normal distribution, 106,
256

odds, 57
ordered probit regression, 211
ordinal variables, 210
ordinary least squares (OLS), 153
out-of-sample validation, 122, 161, 243

negative binomial distribution, 48
normal distribution, 20, 67, 255

random effect, 147, 197
random variable, 17

p-value, 126
parameter expansion, 219
parameter space, 2
partition, 14
point estimator, 79
Poisson distribution, 19, 43, 254
Poisson regression, 172
mixed effects model, 203
polynomial regression, 203
positive definite matrix, 109
posterior approximation, see Markov
chain Monte Carlo (MCMC)
discrete approximation, 90, 173
posterior distribution, 2
precision, 71
precision matrix, 110
predictive distribution, 40
posterior, 61
prior, 61, 239
sampling from, 60
prior distribution, 2
conjugate, 38, 51, 83
mixtures of, 228, 229, 233
improper, 78, 231
Jeffreys‚Äô, 231, 236, 238
unit information, 156, 200, 231, 236,
238, 248
weakly informative, 52, 84
probability
axioms of, 14
density, 18
distribution, 18
interpretations of, 226
probability density function (pdf), 18
probit regression, 237
ordered, 211
proposal distribution, 175
reflecting random walk, 190, 244
quantile, 22

270

Index

continuous, 19
discrete, 18
randomized block design, 246
rank likelihood, 214
reflecting random walk, 190, 244
sample autocorrelation function, 103
sample space, 2
sampling model, 2
sampling properties, 80
sensitivity analysis, 5, 227, 236, 242
shrinkage, 140
standard deviation, 22
sufficient statistic, 35, 83
binomial model, 35
exponential family, 51, 83
normal model, 70
Poisson model, 45
sum of squares matrix, 109
t-statistic

relationship to an improper prior
distribution, 79
two sample, 125
trace of a matrix, 110
training and test sets, 161
transformation model, 211, 214
transformation of variables, see change
of variables
transpose, of a matrix, 106
unbiased, 80
uniform distribution, 32
unit information prior, see prior
distribution
variable selection, see model selection
variance, 22
Wald interval, 7
weakly informative prior, see prior
distribution
Wishart distribution, 109, 257

